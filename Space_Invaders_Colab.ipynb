{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "name": "Space_Invaders_Walkthrough (1).ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QkcEYm3Nl9li"
      },
      "source": [
        "# 0. Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "87utp1v0l9lo",
        "outputId": "5a5d9327-ad7d-4bac-a98d-82f62b494e10"
      },
      "source": [
        "!pip install tensorflow==2.3.1 gym keras-rl2 gym[atari]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==2.3.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/eb/18/374af421dfbe74379a458e58ab40cf46b35c3206ce8e183e28c1c627494d/tensorflow-2.3.1-cp37-cp37m-manylinux2010_x86_64.whl (320.4MB)\n",
            "\u001b[K     |████████████████████████████████| 320.4MB 45kB/s \n",
            "\u001b[?25hRequirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "Collecting keras-rl2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b6/fc/143ee05aed804b3b9052d7b17b13832bc7f3c28e7b1bc50edd09c29d8525/keras_rl2-1.0.5-py3-none-any.whl (52kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 9.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.1) (3.3.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.1) (1.15.0)\n",
            "Collecting numpy<1.19.0,>=1.16.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d6/c6/58e517e8b1fb192725cfa23c01c2e60e4e6699314ee9684a1c5f5c9b27e1/numpy-1.18.5-cp37-cp37m-manylinux1_x86_64.whl (20.1MB)\n",
            "\u001b[K     |████████████████████████████████| 20.1MB 1.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.1) (0.36.2)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.1) (0.12.0)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.1) (1.6.3)\n",
            "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.1) (1.1.2)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.1) (0.2.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.1) (3.12.4)\n",
            "Collecting tensorflow-estimator<2.4.0,>=2.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e9/ed/5853ec0ae380cba4588eab1524e18ece1583b65f7ae0e97321f5ff9dfd60/tensorflow_estimator-2.3.0-py2.py3-none-any.whl (459kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 42.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.1) (1.12.1)\n",
            "Requirement already satisfied: tensorboard<3,>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.1) (2.5.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.1) (1.1.0)\n",
            "Collecting h5py<2.11.0,>=2.10.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3f/c0/abde58b837e066bca19a3f7332d9d0493521d7dd6b48248451a9e3fe2214/h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 27.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.1) (1.34.1)\n",
            "Collecting gast==0.3.3\n",
            "  Downloading https://files.pythonhosted.org/packages/d6/84/759f5dd23fec8ba71952d97bcc7e2c9d7d63bdc582421f3cd4be845f0c98/gast-0.3.3-py2.py3-none-any.whl\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.3.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym) (1.4.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.9.2->tensorflow==2.3.1) (57.0.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.1) (1.31.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.1) (3.3.4)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.1) (1.8.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.1) (1.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.1) (2.23.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.1) (0.4.4)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.1) (0.6.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (4.2.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (4.7.2)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (4.5.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (2.10)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (0.4.8)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (3.4.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (3.1.1)\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: numpy, tensorflow-estimator, h5py, gast, tensorflow, keras-rl2\n",
            "  Found existing installation: numpy 1.19.5\n",
            "    Uninstalling numpy-1.19.5:\n",
            "      Successfully uninstalled numpy-1.19.5\n",
            "  Found existing installation: tensorflow-estimator 2.5.0\n",
            "    Uninstalling tensorflow-estimator-2.5.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.5.0\n",
            "  Found existing installation: h5py 3.1.0\n",
            "    Uninstalling h5py-3.1.0:\n",
            "      Successfully uninstalled h5py-3.1.0\n",
            "  Found existing installation: gast 0.4.0\n",
            "    Uninstalling gast-0.4.0:\n",
            "      Successfully uninstalled gast-0.4.0\n",
            "  Found existing installation: tensorflow 2.5.0\n",
            "    Uninstalling tensorflow-2.5.0:\n",
            "      Successfully uninstalled tensorflow-2.5.0\n",
            "Successfully installed gast-0.3.3 h5py-2.10.0 keras-rl2-1.0.5 numpy-1.18.5 tensorflow-2.3.1 tensorflow-estimator-2.3.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kwa1Fn6zmHba",
        "outputId": "5481c59b-2055-4d69-ce8a-60889b1a9a34"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4BsBmZ6emJqr",
        "outputId": "6d34bc34-965c-4737-a8fb-3ded7e4cf361"
      },
      "source": [
        "!python -m atari_py.import_roms drive/MyDrive/ROMS/"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "copying mr_do.bin from drive/MyDrive/ROMS/Mr. Do! (1983) (CBS Electronics, Ed English) (4L4478) (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/mr_do.bin\n",
            "copying lost_luggage.bin from drive/MyDrive/ROMS/Lost Luggage (Airport Mayhem) (1982) (Apollo - Games by Apollo, Larry Minor, Ernie Runyon, Ed Salvo) (AP-2004) [no opening scene] ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/lost_luggage.bin\n",
            "copying elevator_action.bin from drive/MyDrive/ROMS/Elevator Action (1983) (Atari, Dan Hitchens) (CX26126) (Prototype) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/elevator_action.bin\n",
            "copying asterix.bin from drive/MyDrive/ROMS/Asterix (AKA Taz) (1984) (Atari, Jerome Domurat, Steve Woita) (CX2696).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/asterix.bin\n",
            "copying riverraid.bin from drive/MyDrive/ROMS/River Raid (1982) (Activision, Carol Shaw) (AX-020, AX-020-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/riverraid.bin\n",
            "copying video_pinball.bin from drive/MyDrive/ROMS/Pinball (AKA Video Pinball) (Zellers).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/video_pinball.bin\n",
            "copying road_runner.bin from patched version of drive/MyDrive/ROMS/Road Runner (1989) (Atari - Bobco, Robert C. Polaro) (CX2663) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/road_runner.bin\n",
            "copying qbert.bin from drive/MyDrive/ROMS/Q. Bert (1983) (CCE) (C-822).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/qbert.bin\n",
            "copying surround.bin from drive/MyDrive/ROMS/Surround (32 in 1) (Bit Corporation) (R320).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/surround.bin\n",
            "copying ms_pacman.bin from drive/MyDrive/ROMS/Ms. Pac-Man (1983) (Atari - GCC, Mark Ackerman, Glenn Parker) (CX2675) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/ms_pacman.bin\n",
            "copying up_n_down.bin from drive/MyDrive/ROMS/Up 'n Down (1984) (SEGA - Beck-Tech, Steve Beck, Phat Ho) (009-01) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/up_n_down.bin\n",
            "copying kung_fu_master.bin from drive/MyDrive/ROMS/Kung-Fu Master (1987) (Activision - Imagineering, Dan Kitchen, Garry Kitchen) (AG-039-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/kung_fu_master.bin\n",
            "copying atlantis.bin from drive/MyDrive/ROMS/Atlantis (Lost City of Atlantis) (1982) (Imagic, Dennis Koble) (720103-1A, 720103-1B, IA3203, IX-010-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/atlantis.bin\n",
            "copying pitfall.bin from drive/MyDrive/ROMS/Pitfall! - Pitfall Harry's Jungle Adventure (Jungle Runner) (1982) (Activision, David Crane) (AX-018, AX-018-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/pitfall.bin\n",
            "copying chopper_command.bin from drive/MyDrive/ROMS/Chopper Command (1982) (Activision, Bob Whitehead) (AX-015, AX-015-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/chopper_command.bin\n",
            "copying ice_hockey.bin from drive/MyDrive/ROMS/Ice Hockey - Le Hockey Sur Glace (1981) (Activision, Alan Miller) (AX-012, CAX-012, AX-012-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/ice_hockey.bin\n",
            "copying breakout.bin from drive/MyDrive/ROMS/Breakout - Breakaway IV (Paddle) (1978) (Atari, Brad Stewart - Sears) (CX2622 - 6-99813, 49-75107) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/breakout.bin\n",
            "copying boxing.bin from drive/MyDrive/ROMS/Boxing - La Boxe (1980) (Activision, Bob Whitehead) (AG-002, CAG-002, AG-002-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/boxing.bin\n",
            "copying freeway.bin from drive/MyDrive/ROMS/Freeway (1981) (Activision, David Crane) (AG-009, AG-009-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/freeway.bin\n",
            "copying kaboom.bin from drive/MyDrive/ROMS/Kaboom! (Paddle) (1981) (Activision, Larry Kaplan, David Crane) (AG-010, AG-010-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/kaboom.bin\n",
            "copying skiing.bin from drive/MyDrive/ROMS/Skiing - Le Ski (1980) (Activision, Bob Whitehead) (AG-005, CAG-005, AG-005-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/skiing.bin\n",
            "copying tennis.bin from drive/MyDrive/ROMS/Tennis - Le Tennis (1981) (Activision, Alan Miller) (AG-007, CAG-007) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/tennis.bin\n",
            "copying bank_heist.bin from drive/MyDrive/ROMS/Bank Heist (Bonnie & Clyde, Cops 'n' Robbers, Hold-Up, Roaring 20's) (1983) (20th Century Fox Video Games, Bill Aspromonte) (11012) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/bank_heist.bin\n",
            "copying demon_attack.bin from drive/MyDrive/ROMS/Demon Attack (Death from Above) (1982) (Imagic, Rob Fulop) (720000-200, 720101-1B, 720101-1C, IA3200, IA3200C, IX-006-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/demon_attack.bin\n",
            "copying koolaid.bin from drive/MyDrive/ROMS/Kool-Aid Man (Kool Aid Pitcher Man) (1983) (M Network, Stephen Tatsumi, Jane Terjung - Kool Aid) (MT4648) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/koolaid.bin\n",
            "copying star_gunner.bin from drive/MyDrive/ROMS/Stargunner (1983) (Telesys, Alex Leavens) (1005) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/star_gunner.bin\n",
            "copying trondead.bin from drive/MyDrive/ROMS/TRON - Deadly Discs (TRON Joystick) (1983) (M Network - INTV - APh Technological Consulting, Jeff Ronne, Brett Stutz) (MT5662) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/trondead.bin\n",
            "copying robotank.bin from drive/MyDrive/ROMS/Robot Tank (Robotank) (1983) (Activision, Alan Miller) (AZ-028, AG-028-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/robotank.bin\n",
            "copying space_invaders.bin from drive/MyDrive/ROMS/Space Invaders (1980) (Atari, Richard Maurer - Sears) (CX2632 - 49-75153) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/space_invaders.bin\n",
            "copying hero.bin from drive/MyDrive/ROMS/H.E.R.O. (1984) (Activision, John Van Ryzin) (AZ-036-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/hero.bin\n",
            "copying montezuma_revenge.bin from drive/MyDrive/ROMS/Montezuma's Revenge - Featuring Panama Joe (1984) (Parker Brothers - JWDA, Henry Will IV) (PB5760) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/montezuma_revenge.bin\n",
            "copying asteroids.bin from drive/MyDrive/ROMS/Asteroids (1981) (Atari, Brad Stewart - Sears) (CX2649 - 49-75163) [no copyright] ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/asteroids.bin\n",
            "copying air_raid.bin from drive/MyDrive/ROMS/Air Raid (Men-A-Vision) (PAL) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/air_raid.bin\n",
            "copying alien.bin from drive/MyDrive/ROMS/Alien (1982) (20th Century Fox Video Games, Douglas 'Dallas North' Neubauer) (11006) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/alien.bin\n",
            "copying amidar.bin from drive/MyDrive/ROMS/Amidar (1982) (Parker Brothers, Ed Temple) (PB5310) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/amidar.bin\n",
            "copying assault.bin from drive/MyDrive/ROMS/Assault (AKA Sky Alien) (1983) (Bomb - Onbase) (CA281).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/assault.bin\n",
            "copying bowling.bin from drive/MyDrive/ROMS/Bowling (1979) (Atari, Larry Kaplan - Sears) (CX2628 - 6-99842, 49-75117) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/bowling.bin\n",
            "copying beam_rider.bin from drive/MyDrive/ROMS/Beamrider (1984) (Activision - Cheshire Engineering, David Rolfe, Larry Zwick) (AZ-037-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/beam_rider.bin\n",
            "copying berzerk.bin from drive/MyDrive/ROMS/Berzerk (1982) (Atari, Dan Hitchens - Sears) (CX2650 - 49-75168) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/berzerk.bin\n",
            "copying battle_zone.bin from drive/MyDrive/ROMS/Battlezone (1983) (Atari - GCC, Mike Feinstein, Brad Rice) (CX2681) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/battle_zone.bin\n",
            "copying centipede.bin from drive/MyDrive/ROMS/Centipede (1983) (Atari - GCC) (CX2676) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/centipede.bin\n",
            "copying carnival.bin from drive/MyDrive/ROMS/Carnival (1982) (Coleco - Woodside Design Associates, Steve 'Jessica Stevens' Kitchen) (2468) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/carnival.bin\n",
            "copying crazy_climber.bin from drive/MyDrive/ROMS/Crazy Climber (1983) (Atari - Roklan, Joe Gaucher, Alex Leavens) (CX2683) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/crazy_climber.bin\n",
            "copying defender.bin from drive/MyDrive/ROMS/Defender (1982) (Atari, Robert C. Polaro, Alan J. Murphy - Sears) (CX2609 - 49-75186) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/defender.bin\n",
            "copying double_dunk.bin from drive/MyDrive/ROMS/Double Dunk (Super Basketball) (1989) (Atari, Matthew L. Hubbard) (CX26159) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/double_dunk.bin\n",
            "copying donkey_kong.bin from drive/MyDrive/ROMS/Donkey Kong (1982) (Coleco - Woodside Design Associates - Imaginative Systems Software, Garry Kitchen) (2451) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/donkey_kong.bin\n",
            "copying frostbite.bin from drive/MyDrive/ROMS/Frostbite (1983) (Activision, Steve Cartwright) (AX-031) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/frostbite.bin\n",
            "copying fishing_derby.bin from drive/MyDrive/ROMS/Fishing Derby (1980) (Activision, David Crane) (AG-004) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/fishing_derby.bin\n",
            "copying frogger.bin from drive/MyDrive/ROMS/Frogger (1982) (Parker Brothers, Ed English, David Lamkins) (PB5300) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/frogger.bin\n",
            "copying enduro.bin from drive/MyDrive/ROMS/Enduro (1983) (Activision, Larry Miller) (AX-026, AX-026-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/enduro.bin\n",
            "copying gravitar.bin from drive/MyDrive/ROMS/Gravitar (1983) (Atari, Dan Hitchens, Mimi Nyden) (CX2685) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/gravitar.bin\n",
            "copying gopher.bin from drive/MyDrive/ROMS/Gopher (Gopher Attack) (1982) (U.S. Games Corporation - JWDA, Sylvia Day, Todd Marshall, Robin McDaniel, Henry Will IV) (VC2001) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/gopher.bin\n",
            "copying galaxian.bin from drive/MyDrive/ROMS/Galaxian (1983) (Atari - GCC, Mark Ackerman, Tom Calderwood, Glenn Parker) (CX2684) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/galaxian.bin\n",
            "copying kangaroo.bin from drive/MyDrive/ROMS/Kangaroo (1983) (Atari - GCC, Kevin Osborn) (CX2689) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/kangaroo.bin\n",
            "copying journey_escape.bin from drive/MyDrive/ROMS/Journey Escape (1983) (Data Age, J. Ray Dettling) (112-006) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/journey_escape.bin\n",
            "copying krull.bin from drive/MyDrive/ROMS/Krull (1983) (Atari, Jerome Domurat, Dave Staugas) (CX2682) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/krull.bin\n",
            "copying name_this_game.bin from drive/MyDrive/ROMS/Name This Game (Guardians of Treasure) (1983) (U.S. Games Corporation - JWDA, Roger Booth, Sylvia Day, Ron Dubren, Todd Marshall, Robin McDaniel, Wes Trager, Henry Will IV) (VC1007) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/name_this_game.bin\n",
            "copying phoenix.bin from drive/MyDrive/ROMS/Phoenix (1983) (Atari - GCC, Mike Feinstein, John Mracek) (CX2673) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/phoenix.bin\n",
            "copying private_eye.bin from drive/MyDrive/ROMS/Private Eye (1984) (Activision, Bob Whitehead) (AG-034-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/private_eye.bin\n",
            "copying pooyan.bin from drive/MyDrive/ROMS/Pooyan (1983) (Konami) (RC 100-X 02) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/pooyan.bin\n",
            "copying solaris.bin from drive/MyDrive/ROMS/Solaris (The Last Starfighter, Star Raiders II, Universe) (1986) (Atari, Douglas Neubauer, Mimi Nyden) (CX26136) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/solaris.bin\n",
            "copying seaquest.bin from drive/MyDrive/ROMS/Seaquest (1983) (Activision, Steve Cartwright) (AX-022) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/seaquest.bin\n",
            "copying venture.bin from drive/MyDrive/ROMS/Venture (1982) (Coleco, Joseph Biel) (2457) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/venture.bin\n",
            "copying yars_revenge.bin from drive/MyDrive/ROMS/Yars' Revenge (Time Freeze) (1982) (Atari, Howard Scott Warshaw - Sears) (CX2655 - 49-75167) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/yars_revenge.bin\n",
            "copying zaxxon.bin from drive/MyDrive/ROMS/Zaxxon (1983) (Coleco) (2454) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/zaxxon.bin\n",
            "copying pong.bin from drive/MyDrive/ROMS/Video Olympics - Pong Sports (Paddle) (1977) (Atari, Joe Decuir - Sears) (CX2621 - 99806, 6-99806, 49-75104) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/pong.bin\n",
            "copying wizard_of_wor.bin from drive/MyDrive/ROMS/Wizard of Wor (1982) (CBS Electronics - Roklan, Joe Hellesen, Joe Wagner) (M8774, M8794) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/wizard_of_wor.bin\n",
            "copying time_pilot.bin from drive/MyDrive/ROMS/Time Pilot (1983) (Coleco - Woodside Design Associates, Harley H. Puthuff Jr.) (2663) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/time_pilot.bin\n",
            "copying tutankham.bin from drive/MyDrive/ROMS/Tutankham (1983) (Parker Brothers, Dave Engman, Dawn Stockbridge) (PB5340) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/tutankham.bin\n",
            "copying jamesbond.bin from drive/MyDrive/ROMS/James Bond 007 (James Bond Agent 007) (1984) (Parker Brothers - On-Time Software, Joe Gaucher, Louis Marbel) (PB5110) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/jamesbond.bin\n",
            "copying adventure.bin from drive/MyDrive/ROMS/Adventure (1980) (Atari, Warren Robinett) (CX2613, CX2613P) (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/adventure.bin\n",
            "copying pacman.bin from drive/MyDrive/ROMS/Pac-Man (1982) (Atari, Tod Frye) (CX2646) (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/pacman.bin\n",
            "copying keystone_kapers.bin from drive/MyDrive/ROMS/Keystone Kapers - Raueber und Gendarm (1983) (Activision, Garry Kitchen - Ariola) (EAX-025, EAX-025-04I - 711 025-725) (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/keystone_kapers.bin\n",
            "copying king_kong.bin from drive/MyDrive/ROMS/King Kong (1982) (Tigervision - Software Electronics Corporation, Karl T. Olinger - Teldec) (7-001 - 3.60001 VE) (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/king_kong.bin\n",
            "copying laser_gates.bin from drive/MyDrive/ROMS/Laser Gates (AKA Innerspace) (1983) (Imagic, Dan Oliver) (720118-2A, 13208, EIX-007-04I) (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/laser_gates.bin\n",
            "copying sir_lancelot.bin from drive/MyDrive/ROMS/Sir Lancelot (1983) (Xonox - K-Tel Software - Product Guild, Anthony R. Henderson) (99006, 6220) (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/sir_lancelot.bin\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ji2sKD5nl9lp"
      },
      "source": [
        "# 1. Test Random Environment with OpenAI Gym"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jAQ2osjLl9lp"
      },
      "source": [
        "import gym \n",
        "import random"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6h0GnxjMl9lp"
      },
      "source": [
        "env = gym.make('SpaceInvaders-ram-v0')\n",
        "# height, width, channels = env.observation_space.shape\n",
        "ram_obs = env.observation_space.shape[0]\n",
        "actions = env.action_space.n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9-T86b-gl9lq",
        "outputId": "5e337194-247d-4f69-c412-c7b3b8b9ea14"
      },
      "source": [
        "env.unwrapped.get_action_meanings()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WL-sjxITl9lq",
        "outputId": "8f0dbabd-c70f-41b2-9ae9-cbffb692b48e"
      },
      "source": [
        "ram_obs"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "128"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GCkwDM7ll9lq",
        "outputId": "08b55edf-7162-432a-c598-218c0f6d9eb7"
      },
      "source": [
        "episodes = 5\n",
        "for episode in range(1, episodes+1):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    score = 0 \n",
        "    \n",
        "    while not done:\n",
        "        #env.render()\n",
        "        action = random.choice([0,1,2,3,4,5])\n",
        "        n_state, reward, done, info = env.step(action)\n",
        "        score+=reward\n",
        "    print('Episode:{} Score:{}'.format(episode, score))\n",
        "env.close()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Episode:1 Score:120.0\n",
            "Episode:2 Score:185.0\n",
            "Episode:3 Score:105.0\n",
            "Episode:4 Score:155.0\n",
            "Episode:5 Score:155.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewCtk6pNl9lr"
      },
      "source": [
        "# 2. Create a Deep Learning Model with Keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qxuDMtoTl9lr"
      },
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Convolution2D\n",
        "from tensorflow.keras.optimizers import Adam"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzlHP0Ajl9lr"
      },
      "source": [
        "# def build_model(height, width, channels, actions):\n",
        "    \n",
        "def build_model(ram_obs, actions):\n",
        "    model = Sequential()\n",
        "    \n",
        "    #model.add(Convolution2D(32, (8,8), strides=(4,4), activation='relu', input_shape=(3,height, width, channels)))\n",
        "    #model.add(Convolution2D(64, (4,4), strides=(2,2), activation='relu'))\n",
        "    #model.add(Convolution2D(64, (3,3), activation='relu'))\n",
        "    #model.add(Flatten())\n",
        "    #model.add(Dense(512, activation='relu'))\n",
        "    #model.add(Dense(256, activation='relu'))\n",
        "    model.add(Flatten(input_shape=(1,128))),\n",
        "    model.add(Dense(128, input_dim=ram_obs, activation='relu'))\n",
        "    model.add(Dense(32, activation='relu'))\n",
        "    model.add(Dense(actions, activation='linear'))\n",
        "    return model"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSPATxQ3l9ls"
      },
      "source": [
        "del model"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdhLnY9Hl9ls"
      },
      "source": [
        "model = build_model(ram_obs, actions)\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dqsodqHWl9ls",
        "outputId": "ae1464b0-41b1-4435-c867-70dd2a447d0c"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_1 (Flatten)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 6)                 774       \n",
            "=================================================================\n",
            "Total params: 17,286\n",
            "Trainable params: 17,286\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fnl8ZdcZl9ls"
      },
      "source": [
        "# 3. Build Agent with Keras-RL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6RnVm44wl9ls"
      },
      "source": [
        "from rl.agents import DQNAgent\n",
        "from rl.memory import SequentialMemory\n",
        "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DsH00rmVl9ls"
      },
      "source": [
        "def build_agent(model, actions):\n",
        "    \n",
        "    # balance exploration and exploitation and decay agent \n",
        "    policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps', value_max=1., value_min=.1, value_test=.05, nb_steps=2000000)\n",
        "    \n",
        "    # provides a fast and efficient data structure that we can store the agent’s experiences in\n",
        "    memory = SequentialMemory(limit=1000, window_length=1)\n",
        "    dqn = DQNAgent(model=model, memory=memory, policy=policy,\n",
        "                  enable_dueling_network=True, dueling_type='avg', \n",
        "                   nb_actions=actions, nb_steps_warmup=1000\n",
        "                  )\n",
        "    return dqn"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i6RCgtc8l9lt"
      },
      "source": [
        "dqn = build_agent(model, actions)\n",
        "dqn.compile(Adam(lr=1e-4))"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GJ60TVqKl9lt",
        "outputId": "1f89b182-af27-41c9-aae8-c030e5d7fd9e"
      },
      "source": [
        "dqn.fit(env, nb_steps=2000000, visualize=False, verbose=3)\n",
        "dqn.save_weights('SavedWeights/10k-Fast/dqn2dense005_weights.h5f')"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training for 1000000 steps ...\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training_v1.py:2070: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
            "    394/1000000: episode: 1, duration: 0.550s, episode steps: 394, steps per second: 716, episode reward: 80.000, mean reward:  0.203 [ 0.000, 20.000], mean action: 2.487 [0.000, 5.000],  loss: --, mean_q: --, mean_eps: --\n",
            "    925/1000000: episode: 2, duration: 0.692s, episode steps: 531, steps per second: 767, episode reward: 25.000, mean reward:  0.047 [ 0.000, 10.000], mean action: 2.350 [0.000, 5.000],  loss: --, mean_q: --, mean_eps: --\n",
            "   1649/1000000: episode: 3, duration: 4.288s, episode steps: 724, steps per second: 169, episode reward: 510.000, mean reward:  0.704 [ 0.000, 200.000], mean action: 2.439 [0.000, 5.000],  loss: 985.240651, mean_q: 289.089040, mean_eps: 0.998808\n",
            "   2447/1000000: episode: 4, duration: 4.801s, episode steps: 798, steps per second: 166, episode reward: 155.000, mean reward:  0.194 [ 0.000, 30.000], mean action: 2.506 [0.000, 5.000],  loss: 354.423156, mean_q: 272.358266, mean_eps: 0.998157\n",
            "   3733/1000000: episode: 5, duration: 7.677s, episode steps: 1286, steps per second: 168, episode reward: 485.000, mean reward:  0.377 [ 0.000, 200.000], mean action: 2.450 [0.000, 5.000],  loss: 292.180184, mean_q: 271.521390, mean_eps: 0.997219\n",
            "   4209/1000000: episode: 6, duration: 2.837s, episode steps: 476, steps per second: 168, episode reward: 80.000, mean reward:  0.168 [ 0.000, 20.000], mean action: 2.521 [0.000, 5.000],  loss: 284.579588, mean_q: 262.672333, mean_eps: 0.996427\n",
            "   4686/1000000: episode: 7, duration: 2.837s, episode steps: 477, steps per second: 168, episode reward: 90.000, mean reward:  0.189 [ 0.000, 25.000], mean action: 2.451 [0.000, 5.000],  loss: 272.456175, mean_q: 260.737787, mean_eps: 0.995998\n",
            "   5449/1000000: episode: 8, duration: 4.569s, episode steps: 763, steps per second: 167, episode reward: 165.000, mean reward:  0.216 [ 0.000, 30.000], mean action: 2.474 [0.000, 5.000],  loss: 270.662473, mean_q: 263.267405, mean_eps: 0.995440\n",
            "   6111/1000000: episode: 9, duration: 3.959s, episode steps: 662, steps per second: 167, episode reward: 110.000, mean reward:  0.166 [ 0.000, 30.000], mean action: 2.562 [0.000, 5.000],  loss: 260.172299, mean_q: 262.079220, mean_eps: 0.994798\n",
            "   6767/1000000: episode: 10, duration: 3.965s, episode steps: 656, steps per second: 165, episode reward: 120.000, mean reward:  0.183 [ 0.000, 30.000], mean action: 2.460 [0.000, 5.000],  loss: 244.737702, mean_q: 266.657536, mean_eps: 0.994205\n",
            "   7384/1000000: episode: 11, duration: 3.720s, episode steps: 617, steps per second: 166, episode reward: 125.000, mean reward:  0.203 [ 0.000, 30.000], mean action: 2.535 [0.000, 5.000],  loss: 239.523936, mean_q: 268.179770, mean_eps: 0.993633\n",
            "   7818/1000000: episode: 12, duration: 2.592s, episode steps: 434, steps per second: 167, episode reward: 110.000, mean reward:  0.253 [ 0.000, 30.000], mean action: 2.583 [0.000, 5.000],  loss: 253.380277, mean_q: 269.708987, mean_eps: 0.993160\n",
            "   8518/1000000: episode: 13, duration: 4.165s, episode steps: 700, steps per second: 168, episode reward: 120.000, mean reward:  0.171 [ 0.000, 30.000], mean action: 2.576 [0.000, 5.000],  loss: 233.403630, mean_q: 269.203617, mean_eps: 0.992649\n",
            "   9341/1000000: episode: 14, duration: 4.841s, episode steps: 823, steps per second: 170, episode reward: 185.000, mean reward:  0.225 [ 0.000, 30.000], mean action: 2.445 [0.000, 5.000],  loss: 211.774381, mean_q: 267.945895, mean_eps: 0.991964\n",
            "   9844/1000000: episode: 15, duration: 3.005s, episode steps: 503, steps per second: 167, episode reward: 30.000, mean reward:  0.060 [ 0.000, 10.000], mean action: 2.628 [0.000, 5.000],  loss: 225.704083, mean_q: 266.129200, mean_eps: 0.991367\n",
            "  10546/1000000: episode: 16, duration: 4.178s, episode steps: 702, steps per second: 168, episode reward: 105.000, mean reward:  0.150 [ 0.000, 25.000], mean action: 2.487 [0.000, 5.000],  loss: 144.103868, mean_q: 274.567042, mean_eps: 0.990825\n",
            "  11167/1000000: episode: 17, duration: 3.725s, episode steps: 621, steps per second: 167, episode reward: 150.000, mean reward:  0.242 [ 0.000, 25.000], mean action: 2.522 [0.000, 5.000],  loss: 109.715628, mean_q: 277.968728, mean_eps: 0.990230\n",
            "  11963/1000000: episode: 18, duration: 4.813s, episode steps: 796, steps per second: 165, episode reward: 185.000, mean reward:  0.232 [ 0.000, 30.000], mean action: 2.472 [0.000, 5.000],  loss: 99.544149, mean_q: 274.083843, mean_eps: 0.989592\n",
            "  12594/1000000: episode: 19, duration: 3.774s, episode steps: 631, steps per second: 167, episode reward: 35.000, mean reward:  0.055 [ 0.000, 15.000], mean action: 2.418 [0.000, 5.000],  loss: 98.402635, mean_q: 278.442564, mean_eps: 0.988950\n",
            "  13218/1000000: episode: 20, duration: 3.706s, episode steps: 624, steps per second: 168, episode reward: 45.000, mean reward:  0.072 [ 0.000, 15.000], mean action: 2.385 [0.000, 5.000],  loss: 102.899785, mean_q: 277.536218, mean_eps: 0.988385\n",
            "  13978/1000000: episode: 21, duration: 4.526s, episode steps: 760, steps per second: 168, episode reward: 120.000, mean reward:  0.158 [ 0.000, 30.000], mean action: 2.462 [0.000, 5.000],  loss: 91.879063, mean_q: 275.744378, mean_eps: 0.987762\n",
            "  14967/1000000: episode: 22, duration: 5.934s, episode steps: 989, steps per second: 167, episode reward: 260.000, mean reward:  0.263 [ 0.000, 30.000], mean action: 2.542 [0.000, 5.000],  loss: 93.438582, mean_q: 271.721135, mean_eps: 0.986975\n",
            "  15659/1000000: episode: 23, duration: 4.139s, episode steps: 692, steps per second: 167, episode reward: 140.000, mean reward:  0.202 [ 0.000, 25.000], mean action: 2.500 [0.000, 5.000],  loss: 70.014592, mean_q: 266.990582, mean_eps: 0.986219\n",
            "  16137/1000000: episode: 24, duration: 2.917s, episode steps: 478, steps per second: 164, episode reward: 55.000, mean reward:  0.115 [ 0.000, 25.000], mean action: 2.559 [0.000, 5.000],  loss: 81.502142, mean_q: 276.093934, mean_eps: 0.985692\n",
            "  16758/1000000: episode: 25, duration: 3.741s, episode steps: 621, steps per second: 166, episode reward: 180.000, mean reward:  0.290 [ 0.000, 30.000], mean action: 2.562 [0.000, 5.000],  loss: 89.639110, mean_q: 275.566733, mean_eps: 0.985198\n",
            "  17711/1000000: episode: 26, duration: 5.731s, episode steps: 953, steps per second: 166, episode reward: 210.000, mean reward:  0.220 [ 0.000, 30.000], mean action: 2.526 [0.000, 5.000],  loss: 83.085643, mean_q: 269.410017, mean_eps: 0.984489\n",
            "  18062/1000000: episode: 27, duration: 2.134s, episode steps: 351, steps per second: 164, episode reward: 40.000, mean reward:  0.114 [ 0.000, 25.000], mean action: 2.370 [0.000, 5.000],  loss: 98.034674, mean_q: 282.428887, mean_eps: 0.983903\n",
            "  18444/1000000: episode: 28, duration: 2.316s, episode steps: 382, steps per second: 165, episode reward: 80.000, mean reward:  0.209 [ 0.000, 20.000], mean action: 2.529 [0.000, 5.000],  loss: 126.799197, mean_q: 282.162522, mean_eps: 0.983573\n",
            "  18956/1000000: episode: 29, duration: 3.107s, episode steps: 512, steps per second: 165, episode reward: 50.000, mean reward:  0.098 [ 0.000, 20.000], mean action: 2.633 [0.000, 5.000],  loss: 126.719192, mean_q: 276.448814, mean_eps: 0.983170\n",
            "  19490/1000000: episode: 30, duration: 3.249s, episode steps: 534, steps per second: 164, episode reward: 130.000, mean reward:  0.243 [ 0.000, 30.000], mean action: 2.506 [0.000, 5.000],  loss: 107.888015, mean_q: 274.673426, mean_eps: 0.982700\n",
            "  20004/1000000: episode: 31, duration: 3.123s, episode steps: 514, steps per second: 165, episode reward: 40.000, mean reward:  0.078 [ 0.000, 20.000], mean action: 2.514 [0.000, 5.000],  loss: 104.975162, mean_q: 273.989214, mean_eps: 0.982228\n",
            "  20766/1000000: episode: 32, duration: 4.566s, episode steps: 762, steps per second: 167, episode reward: 210.000, mean reward:  0.276 [ 0.000, 30.000], mean action: 2.455 [0.000, 5.000],  loss: 82.995785, mean_q: 276.788772, mean_eps: 0.981654\n",
            "  21147/1000000: episode: 33, duration: 2.301s, episode steps: 381, steps per second: 166, episode reward: 65.000, mean reward:  0.171 [ 0.000, 20.000], mean action: 2.682 [0.000, 5.000],  loss: 80.248298, mean_q: 280.986520, mean_eps: 0.981140\n",
            "  21778/1000000: episode: 34, duration: 3.763s, episode steps: 631, steps per second: 168, episode reward: 195.000, mean reward:  0.309 [ 0.000, 30.000], mean action: 2.528 [0.000, 5.000],  loss: 71.266875, mean_q: 278.171150, mean_eps: 0.980684\n",
            "  22576/1000000: episode: 35, duration: 4.715s, episode steps: 798, steps per second: 169, episode reward: 210.000, mean reward:  0.263 [ 0.000, 30.000], mean action: 2.461 [0.000, 5.000],  loss: 69.340104, mean_q: 278.369430, mean_eps: 0.980041\n",
            "  23062/1000000: episode: 36, duration: 2.865s, episode steps: 486, steps per second: 170, episode reward: 45.000, mean reward:  0.093 [ 0.000, 15.000], mean action: 2.494 [0.000, 5.000],  loss: 72.072138, mean_q: 277.540030, mean_eps: 0.979463\n",
            "  23738/1000000: episode: 37, duration: 4.053s, episode steps: 676, steps per second: 167, episode reward: 90.000, mean reward:  0.133 [ 0.000, 25.000], mean action: 2.543 [0.000, 5.000],  loss: 72.650317, mean_q: 274.886013, mean_eps: 0.978940\n",
            "  24176/1000000: episode: 38, duration: 2.649s, episode steps: 438, steps per second: 165, episode reward: 105.000, mean reward:  0.240 [ 0.000, 25.000], mean action: 2.614 [0.000, 5.000],  loss: 65.115077, mean_q: 272.884511, mean_eps: 0.978439\n",
            "  24736/1000000: episode: 39, duration: 3.318s, episode steps: 560, steps per second: 169, episode reward: 120.000, mean reward:  0.214 [ 0.000, 30.000], mean action: 2.421 [0.000, 5.000],  loss: 81.379657, mean_q: 274.086282, mean_eps: 0.977990\n",
            "  25160/1000000: episode: 40, duration: 2.542s, episode steps: 424, steps per second: 167, episode reward: 45.000, mean reward:  0.106 [ 0.000, 15.000], mean action: 2.392 [0.000, 5.000],  loss: 95.887887, mean_q: 275.366643, mean_eps: 0.977547\n",
            "  25674/1000000: episode: 41, duration: 3.072s, episode steps: 514, steps per second: 167, episode reward: 35.000, mean reward:  0.068 [ 0.000, 15.000], mean action: 2.486 [0.000, 5.000],  loss: 74.081620, mean_q: 276.032472, mean_eps: 0.977125\n",
            "  26839/1000000: episode: 42, duration: 6.844s, episode steps: 1165, steps per second: 170, episode reward: 275.000, mean reward:  0.236 [ 0.000, 30.000], mean action: 2.501 [0.000, 5.000],  loss: 65.875520, mean_q: 280.687592, mean_eps: 0.976370\n",
            "  27334/1000000: episode: 43, duration: 2.953s, episode steps: 495, steps per second: 168, episode reward: 65.000, mean reward:  0.131 [ 0.000, 20.000], mean action: 2.467 [0.000, 5.000],  loss: 67.691429, mean_q: 282.053519, mean_eps: 0.975623\n",
            "  27913/1000000: episode: 44, duration: 3.457s, episode steps: 579, steps per second: 167, episode reward: 135.000, mean reward:  0.233 [ 0.000, 30.000], mean action: 2.506 [0.000, 5.000],  loss: 66.817409, mean_q: 275.394988, mean_eps: 0.975139\n",
            "  28575/1000000: episode: 45, duration: 4.053s, episode steps: 662, steps per second: 163, episode reward: 155.000, mean reward:  0.234 [ 0.000, 30.000], mean action: 2.480 [0.000, 5.000],  loss: 66.950804, mean_q: 279.834042, mean_eps: 0.974581\n",
            "  29870/1000000: episode: 46, duration: 7.767s, episode steps: 1295, steps per second: 167, episode reward: 320.000, mean reward:  0.247 [ 0.000, 30.000], mean action: 2.553 [0.000, 5.000],  loss: 57.116707, mean_q: 271.606309, mean_eps: 0.973700\n",
            "  30766/1000000: episode: 47, duration: 5.401s, episode steps: 896, steps per second: 166, episode reward: 465.000, mean reward:  0.519 [ 0.000, 200.000], mean action: 2.589 [0.000, 5.000],  loss: 58.639024, mean_q: 277.396156, mean_eps: 0.972714\n",
            "  31173/1000000: episode: 48, duration: 2.446s, episode steps: 407, steps per second: 166, episode reward: 45.000, mean reward:  0.111 [ 0.000, 15.000], mean action: 2.521 [0.000, 5.000],  loss: 66.050619, mean_q: 280.927737, mean_eps: 0.972128\n",
            "  31575/1000000: episode: 49, duration: 2.418s, episode steps: 402, steps per second: 166, episode reward: 30.000, mean reward:  0.075 [ 0.000, 15.000], mean action: 2.430 [0.000, 5.000],  loss: 73.551478, mean_q: 271.501408, mean_eps: 0.971764\n",
            "  32175/1000000: episode: 50, duration: 3.565s, episode steps: 600, steps per second: 168, episode reward: 70.000, mean reward:  0.117 [ 0.000, 25.000], mean action: 2.558 [0.000, 5.000],  loss: 82.562587, mean_q: 270.427680, mean_eps: 0.971313\n",
            "  33247/1000000: episode: 51, duration: 6.383s, episode steps: 1072, steps per second: 168, episode reward: 305.000, mean reward:  0.285 [ 0.000, 30.000], mean action: 2.451 [0.000, 5.000],  loss: 45.845247, mean_q: 278.894704, mean_eps: 0.970561\n",
            "  33639/1000000: episode: 52, duration: 2.318s, episode steps: 392, steps per second: 169, episode reward: 60.000, mean reward:  0.153 [ 0.000, 25.000], mean action: 2.457 [0.000, 5.000],  loss: 58.956294, mean_q: 278.387891, mean_eps: 0.969902\n",
            "  34023/1000000: episode: 53, duration: 2.332s, episode steps: 384, steps per second: 165, episode reward: 125.000, mean reward:  0.326 [ 0.000, 25.000], mean action: 2.466 [0.000, 5.000],  loss: 66.079619, mean_q: 274.871269, mean_eps: 0.969553\n",
            "  34476/1000000: episode: 54, duration: 2.703s, episode steps: 453, steps per second: 168, episode reward: 35.000, mean reward:  0.077 [ 0.000, 15.000], mean action: 2.547 [0.000, 5.000],  loss: 85.748928, mean_q: 272.713611, mean_eps: 0.969176\n",
            "  35545/1000000: episode: 55, duration: 6.324s, episode steps: 1069, steps per second: 169, episode reward: 170.000, mean reward:  0.159 [ 0.000, 30.000], mean action: 2.582 [0.000, 5.000],  loss: 55.278644, mean_q: 279.526111, mean_eps: 0.968491\n",
            "  36183/1000000: episode: 56, duration: 3.855s, episode steps: 638, steps per second: 165, episode reward: 135.000, mean reward:  0.212 [ 0.000, 30.000], mean action: 2.472 [0.000, 5.000],  loss: 36.519523, mean_q: 283.053183, mean_eps: 0.967723\n",
            "  36715/1000000: episode: 57, duration: 3.204s, episode steps: 532, steps per second: 166, episode reward: 65.000, mean reward:  0.122 [ 0.000, 20.000], mean action: 2.549 [0.000, 5.000],  loss: 48.056699, mean_q: 274.507347, mean_eps: 0.967196\n",
            "  37527/1000000: episode: 58, duration: 4.792s, episode steps: 812, steps per second: 169, episode reward: 210.000, mean reward:  0.259 [ 0.000, 30.000], mean action: 2.549 [0.000, 5.000],  loss: 49.418886, mean_q: 270.349253, mean_eps: 0.966592\n",
            "  38026/1000000: episode: 59, duration: 2.935s, episode steps: 499, steps per second: 170, episode reward: 30.000, mean reward:  0.060 [ 0.000, 10.000], mean action: 2.453 [0.000, 5.000],  loss: 41.788467, mean_q: 273.665237, mean_eps: 0.966002\n",
            "  38783/1000000: episode: 60, duration: 4.471s, episode steps: 757, steps per second: 169, episode reward: 320.000, mean reward:  0.423 [ 0.000, 200.000], mean action: 2.560 [0.000, 5.000],  loss: 54.527300, mean_q: 271.369063, mean_eps: 0.965436\n",
            "  39399/1000000: episode: 61, duration: 3.670s, episode steps: 616, steps per second: 168, episode reward: 20.000, mean reward:  0.032 [ 0.000, 10.000], mean action: 2.623 [0.000, 5.000],  loss: 72.456445, mean_q: 281.945540, mean_eps: 0.964819\n",
            "  40029/1000000: episode: 62, duration: 3.755s, episode steps: 630, steps per second: 168, episode reward: 180.000, mean reward:  0.286 [ 0.000, 30.000], mean action: 2.424 [0.000, 5.000],  loss: 79.946974, mean_q: 281.449331, mean_eps: 0.964258\n",
            "  40520/1000000: episode: 63, duration: 2.926s, episode steps: 491, steps per second: 168, episode reward: 150.000, mean reward:  0.305 [ 0.000, 25.000], mean action: 2.574 [0.000, 5.000],  loss: 75.233586, mean_q: 283.523943, mean_eps: 0.963753\n",
            "  40928/1000000: episode: 64, duration: 2.446s, episode steps: 408, steps per second: 167, episode reward: 80.000, mean reward:  0.196 [ 0.000, 25.000], mean action: 2.605 [0.000, 5.000],  loss: 71.695576, mean_q: 280.517987, mean_eps: 0.963349\n",
            "  41843/1000000: episode: 65, duration: 5.462s, episode steps: 915, steps per second: 168, episode reward: 375.000, mean reward:  0.410 [ 0.000, 200.000], mean action: 2.375 [0.000, 5.000],  loss: 60.308999, mean_q: 276.642946, mean_eps: 0.962754\n",
            "  42813/1000000: episode: 66, duration: 5.730s, episode steps: 970, steps per second: 169, episode reward: 240.000, mean reward:  0.247 [ 0.000, 30.000], mean action: 2.529 [0.000, 5.000],  loss: 45.714973, mean_q: 276.985427, mean_eps: 0.961905\n",
            "  43568/1000000: episode: 67, duration: 4.462s, episode steps: 755, steps per second: 169, episode reward: 185.000, mean reward:  0.245 [ 0.000, 30.000], mean action: 2.501 [0.000, 5.000],  loss: 36.479396, mean_q: 264.852651, mean_eps: 0.961129\n",
            "  44130/1000000: episode: 68, duration: 3.316s, episode steps: 562, steps per second: 169, episode reward: 20.000, mean reward:  0.036 [ 0.000, 10.000], mean action: 2.560 [0.000, 5.000],  loss: 58.674611, mean_q: 273.274693, mean_eps: 0.960536\n",
            "  44489/1000000: episode: 69, duration: 2.157s, episode steps: 359, steps per second: 166, episode reward: 55.000, mean reward:  0.153 [ 0.000, 20.000], mean action: 2.568 [0.000, 5.000],  loss: 62.144039, mean_q: 275.633254, mean_eps: 0.960122\n",
            "  45045/1000000: episode: 70, duration: 3.310s, episode steps: 556, steps per second: 168, episode reward: 80.000, mean reward:  0.144 [ 0.000, 25.000], mean action: 2.406 [0.000, 5.000],  loss: 61.053581, mean_q: 274.753122, mean_eps: 0.959710\n",
            "  46095/1000000: episode: 71, duration: 6.178s, episode steps: 1050, steps per second: 170, episode reward: 420.000, mean reward:  0.400 [ 0.000, 200.000], mean action: 2.597 [0.000, 5.000],  loss: 58.099093, mean_q: 276.013989, mean_eps: 0.958987\n",
            "  46584/1000000: episode: 72, duration: 2.935s, episode steps: 489, steps per second: 167, episode reward: 65.000, mean reward:  0.133 [ 0.000, 30.000], mean action: 2.485 [0.000, 5.000],  loss: 55.385961, mean_q: 278.006659, mean_eps: 0.958295\n",
            "  47223/1000000: episode: 73, duration: 3.793s, episode steps: 639, steps per second: 168, episode reward: 130.000, mean reward:  0.203 [ 0.000, 25.000], mean action: 2.471 [0.000, 5.000],  loss: 62.394901, mean_q: 274.273502, mean_eps: 0.957787\n",
            "  47729/1000000: episode: 74, duration: 3.020s, episode steps: 506, steps per second: 168, episode reward: 80.000, mean reward:  0.158 [ 0.000, 25.000], mean action: 2.565 [0.000, 5.000],  loss: 70.053835, mean_q: 274.240785, mean_eps: 0.957272\n",
            "  48296/1000000: episode: 75, duration: 3.363s, episode steps: 567, steps per second: 169, episode reward: 80.000, mean reward:  0.141 [ 0.000, 25.000], mean action: 2.513 [0.000, 5.000],  loss: 57.601776, mean_q: 272.126007, mean_eps: 0.956789\n",
            "  49087/1000000: episode: 76, duration: 4.683s, episode steps: 791, steps per second: 169, episode reward: 220.000, mean reward:  0.278 [ 0.000, 30.000], mean action: 2.489 [0.000, 5.000],  loss: 48.466333, mean_q: 275.378314, mean_eps: 0.956178\n",
            "  49857/1000000: episode: 77, duration: 4.616s, episode steps: 770, steps per second: 167, episode reward: 210.000, mean reward:  0.273 [ 0.000, 30.000], mean action: 2.497 [0.000, 5.000],  loss: 44.001705, mean_q: 272.427033, mean_eps: 0.955476\n",
            "  50627/1000000: episode: 78, duration: 4.547s, episode steps: 770, steps per second: 169, episode reward: 120.000, mean reward:  0.156 [ 0.000, 30.000], mean action: 2.518 [0.000, 5.000],  loss: 40.866165, mean_q: 276.105078, mean_eps: 0.954783\n",
            "  51262/1000000: episode: 79, duration: 3.808s, episode steps: 635, steps per second: 167, episode reward: 110.000, mean reward:  0.173 [ 0.000, 30.000], mean action: 2.496 [0.000, 5.000],  loss: 46.144588, mean_q: 276.805666, mean_eps: 0.954150\n",
            "  52017/1000000: episode: 80, duration: 4.485s, episode steps: 755, steps per second: 168, episode reward: 165.000, mean reward:  0.219 [ 0.000, 30.000], mean action: 2.554 [0.000, 5.000],  loss: 46.818155, mean_q: 275.883206, mean_eps: 0.953525\n",
            "  52684/1000000: episode: 81, duration: 3.954s, episode steps: 667, steps per second: 169, episode reward: 210.000, mean reward:  0.315 [ 0.000, 30.000], mean action: 2.493 [0.000, 5.000],  loss: 50.220653, mean_q: 277.664879, mean_eps: 0.952885\n",
            "  53082/1000000: episode: 82, duration: 2.349s, episode steps: 398, steps per second: 169, episode reward: 50.000, mean reward:  0.126 [ 0.000, 20.000], mean action: 2.447 [0.000, 5.000],  loss: 57.288145, mean_q: 279.865671, mean_eps: 0.952406\n",
            "  53913/1000000: episode: 83, duration: 4.894s, episode steps: 831, steps per second: 170, episode reward: 270.000, mean reward:  0.325 [ 0.000, 30.000], mean action: 2.535 [0.000, 5.000],  loss: 62.580757, mean_q: 279.676248, mean_eps: 0.951853\n",
            "  54567/1000000: episode: 84, duration: 3.910s, episode steps: 654, steps per second: 167, episode reward: 135.000, mean reward:  0.206 [ 0.000, 30.000], mean action: 2.498 [0.000, 5.000],  loss: 57.917059, mean_q: 283.828447, mean_eps: 0.951184\n",
            "  55238/1000000: episode: 85, duration: 3.974s, episode steps: 671, steps per second: 169, episode reward: 120.000, mean reward:  0.179 [ 0.000, 30.000], mean action: 2.484 [0.000, 5.000],  loss: 45.494388, mean_q: 281.055757, mean_eps: 0.950588\n",
            "  55752/1000000: episode: 86, duration: 3.084s, episode steps: 514, steps per second: 167, episode reward: 80.000, mean reward:  0.156 [ 0.000, 20.000], mean action: 2.570 [0.000, 5.000],  loss: 48.415856, mean_q: 280.388345, mean_eps: 0.950055\n",
            "  56309/1000000: episode: 87, duration: 3.340s, episode steps: 557, steps per second: 167, episode reward: 130.000, mean reward:  0.233 [ 0.000, 25.000], mean action: 2.490 [0.000, 5.000],  loss: 56.952409, mean_q: 280.112371, mean_eps: 0.949573\n",
            "  57162/1000000: episode: 88, duration: 5.098s, episode steps: 853, steps per second: 167, episode reward: 135.000, mean reward:  0.158 [ 0.000, 30.000], mean action: 2.468 [0.000, 5.000],  loss: 51.679156, mean_q: 282.506394, mean_eps: 0.948938\n",
            "  57895/1000000: episode: 89, duration: 4.390s, episode steps: 733, steps per second: 167, episode reward: 155.000, mean reward:  0.211 [ 0.000, 30.000], mean action: 2.377 [0.000, 5.000],  loss: 42.437853, mean_q: 277.903428, mean_eps: 0.948225\n",
            "  58396/1000000: episode: 90, duration: 2.969s, episode steps: 501, steps per second: 169, episode reward:  5.000, mean reward:  0.010 [ 0.000,  5.000], mean action: 2.427 [0.000, 5.000],  loss: 33.102179, mean_q: 272.575062, mean_eps: 0.947670\n",
            "  59546/1000000: episode: 91, duration: 6.803s, episode steps: 1150, steps per second: 169, episode reward: 330.000, mean reward:  0.287 [ 0.000, 30.000], mean action: 2.576 [0.000, 5.000],  loss: 34.231849, mean_q: 276.474174, mean_eps: 0.946927\n",
            "  60106/1000000: episode: 92, duration: 3.347s, episode steps: 560, steps per second: 167, episode reward: 70.000, mean reward:  0.125 [ 0.000, 25.000], mean action: 2.352 [0.000, 5.000],  loss: 41.197001, mean_q: 278.677310, mean_eps: 0.946157\n",
            "  60812/1000000: episode: 93, duration: 4.171s, episode steps: 706, steps per second: 169, episode reward: 190.000, mean reward:  0.269 [ 0.000, 30.000], mean action: 2.542 [0.000, 5.000],  loss: 41.386431, mean_q: 278.651319, mean_eps: 0.945587\n",
            "  61499/1000000: episode: 94, duration: 4.097s, episode steps: 687, steps per second: 168, episode reward: 200.000, mean reward:  0.291 [ 0.000, 30.000], mean action: 2.486 [0.000, 5.000],  loss: 45.733613, mean_q: 281.236445, mean_eps: 0.944961\n",
            "  62228/1000000: episode: 95, duration: 4.406s, episode steps: 729, steps per second: 165, episode reward: 180.000, mean reward:  0.247 [ 0.000, 30.000], mean action: 2.399 [0.000, 5.000],  loss: 37.563182, mean_q: 285.257894, mean_eps: 0.944323\n",
            "  63055/1000000: episode: 96, duration: 4.940s, episode steps: 827, steps per second: 167, episode reward: 320.000, mean reward:  0.387 [ 0.000, 30.000], mean action: 2.434 [0.000, 5.000],  loss: 37.500386, mean_q: 285.369497, mean_eps: 0.943623\n",
            "  63978/1000000: episode: 97, duration: 5.482s, episode steps: 923, steps per second: 168, episode reward: 185.000, mean reward:  0.200 [ 0.000, 30.000], mean action: 2.652 [0.000, 5.000],  loss: 46.002931, mean_q: 285.160992, mean_eps: 0.942836\n",
            "  64705/1000000: episode: 98, duration: 4.366s, episode steps: 727, steps per second: 167, episode reward: 95.000, mean reward:  0.131 [ 0.000, 20.000], mean action: 2.404 [0.000, 5.000],  loss: 33.994054, mean_q: 275.102422, mean_eps: 0.942093\n",
            "  65144/1000000: episode: 99, duration: 2.616s, episode steps: 439, steps per second: 168, episode reward: 35.000, mean reward:  0.080 [ 0.000, 15.000], mean action: 2.638 [0.000, 5.000],  loss: 60.284485, mean_q: 283.535522, mean_eps: 0.941568\n",
            "  65721/1000000: episode: 100, duration: 3.444s, episode steps: 577, steps per second: 168, episode reward: 120.000, mean reward:  0.208 [ 0.000, 30.000], mean action: 2.426 [0.000, 5.000],  loss: 65.638602, mean_q: 282.983992, mean_eps: 0.941111\n",
            "  66362/1000000: episode: 101, duration: 3.850s, episode steps: 641, steps per second: 166, episode reward: 155.000, mean reward:  0.242 [ 0.000, 30.000], mean action: 2.527 [0.000, 5.000],  loss: 47.126046, mean_q: 277.636054, mean_eps: 0.940563\n",
            "  66738/1000000: episode: 102, duration: 2.268s, episode steps: 376, steps per second: 166, episode reward: 115.000, mean reward:  0.306 [ 0.000, 25.000], mean action: 2.551 [0.000, 5.000],  loss: 51.436682, mean_q: 275.083485, mean_eps: 0.940105\n",
            "  67181/1000000: episode: 103, duration: 2.675s, episode steps: 443, steps per second: 166, episode reward: 105.000, mean reward:  0.237 [ 0.000, 25.000], mean action: 2.379 [0.000, 5.000],  loss: 76.715747, mean_q: 276.710574, mean_eps: 0.939737\n",
            "  67850/1000000: episode: 104, duration: 4.001s, episode steps: 669, steps per second: 167, episode reward: 120.000, mean reward:  0.179 [ 0.000, 30.000], mean action: 2.555 [0.000, 5.000],  loss: 78.153227, mean_q: 278.730357, mean_eps: 0.939237\n",
            "  68802/1000000: episode: 105, duration: 5.684s, episode steps: 952, steps per second: 167, episode reward: 225.000, mean reward:  0.236 [ 0.000, 30.000], mean action: 2.517 [0.000, 5.000],  loss: 41.218660, mean_q: 282.485758, mean_eps: 0.938507\n",
            "  69216/1000000: episode: 106, duration: 2.490s, episode steps: 414, steps per second: 166, episode reward: 50.000, mean reward:  0.121 [ 0.000, 20.000], mean action: 2.365 [0.000, 5.000],  loss: 36.547169, mean_q: 284.159810, mean_eps: 0.937892\n",
            "  69968/1000000: episode: 107, duration: 4.478s, episode steps: 752, steps per second: 168, episode reward: 135.000, mean reward:  0.180 [ 0.000, 30.000], mean action: 2.515 [0.000, 5.000],  loss: 61.478416, mean_q: 283.248527, mean_eps: 0.937368\n",
            "  70569/1000000: episode: 108, duration: 3.688s, episode steps: 601, steps per second: 163, episode reward: 120.000, mean reward:  0.200 [ 0.000, 30.000], mean action: 2.438 [0.000, 5.000],  loss: 52.847546, mean_q: 285.734250, mean_eps: 0.936759\n",
            "  71134/1000000: episode: 109, duration: 3.430s, episode steps: 565, steps per second: 165, episode reward: 90.000, mean reward:  0.159 [ 0.000, 25.000], mean action: 2.598 [0.000, 5.000],  loss: 54.709180, mean_q: 280.853774, mean_eps: 0.936234\n",
            "  71947/1000000: episode: 110, duration: 4.839s, episode steps: 813, steps per second: 168, episode reward: 155.000, mean reward:  0.191 [ 0.000, 30.000], mean action: 2.571 [0.000, 5.000],  loss: 49.300732, mean_q: 280.743934, mean_eps: 0.935614\n",
            "  72941/1000000: episode: 111, duration: 5.862s, episode steps: 994, steps per second: 170, episode reward: 320.000, mean reward:  0.322 [ 0.000, 30.000], mean action: 2.378 [0.000, 5.000],  loss: 35.509300, mean_q: 280.422849, mean_eps: 0.934801\n",
            "  73581/1000000: episode: 112, duration: 3.831s, episode steps: 640, steps per second: 167, episode reward: 75.000, mean reward:  0.117 [ 0.000, 25.000], mean action: 2.422 [0.000, 5.000],  loss: 44.576824, mean_q: 283.729056, mean_eps: 0.934066\n",
            "  74097/1000000: episode: 113, duration: 3.076s, episode steps: 516, steps per second: 168, episode reward: 85.000, mean reward:  0.165 [ 0.000, 25.000], mean action: 2.572 [0.000, 5.000],  loss: 51.115402, mean_q: 283.382397, mean_eps: 0.933545\n",
            "  74712/1000000: episode: 114, duration: 3.651s, episode steps: 615, steps per second: 168, episode reward: 75.000, mean reward:  0.122 [ 0.000, 25.000], mean action: 2.525 [0.000, 5.000],  loss: 56.488480, mean_q: 285.273291, mean_eps: 0.933036\n",
            "  75368/1000000: episode: 115, duration: 3.934s, episode steps: 656, steps per second: 167, episode reward: 135.000, mean reward:  0.206 [ 0.000, 30.000], mean action: 2.488 [0.000, 5.000],  loss: 43.358534, mean_q: 283.861689, mean_eps: 0.932464\n",
            "  75769/1000000: episode: 116, duration: 2.417s, episode steps: 401, steps per second: 166, episode reward: 80.000, mean reward:  0.200 [ 0.000, 25.000], mean action: 2.486 [0.000, 5.000],  loss: 56.357861, mean_q: 281.012380, mean_eps: 0.931989\n",
            "  76552/1000000: episode: 117, duration: 4.716s, episode steps: 783, steps per second: 166, episode reward: 155.000, mean reward:  0.198 [ 0.000, 30.000], mean action: 2.534 [0.000, 5.000],  loss: 56.723346, mean_q: 278.405621, mean_eps: 0.931456\n",
            "  76935/1000000: episode: 118, duration: 2.332s, episode steps: 383, steps per second: 164, episode reward: 65.000, mean reward:  0.170 [ 0.000, 20.000], mean action: 2.564 [0.000, 5.000],  loss: 53.368942, mean_q: 280.485965, mean_eps: 0.930931\n",
            "  77569/1000000: episode: 119, duration: 3.800s, episode steps: 634, steps per second: 167, episode reward: 110.000, mean reward:  0.174 [ 0.000, 25.000], mean action: 2.386 [0.000, 5.000],  loss: 66.071115, mean_q: 281.737779, mean_eps: 0.930474\n",
            "  78470/1000000: episode: 120, duration: 5.359s, episode steps: 901, steps per second: 168, episode reward: 280.000, mean reward:  0.311 [ 0.000, 30.000], mean action: 2.412 [0.000, 5.000],  loss: 49.198271, mean_q: 283.940268, mean_eps: 0.929783\n",
            "  79136/1000000: episode: 121, duration: 3.955s, episode steps: 666, steps per second: 168, episode reward: 105.000, mean reward:  0.158 [ 0.000, 25.000], mean action: 2.377 [0.000, 5.000],  loss: 30.924292, mean_q: 285.448675, mean_eps: 0.929078\n",
            "  79922/1000000: episode: 122, duration: 4.689s, episode steps: 786, steps per second: 168, episode reward: 165.000, mean reward:  0.210 [ 0.000, 30.000], mean action: 2.536 [0.000, 5.000],  loss: 56.631836, mean_q: 289.271493, mean_eps: 0.928424\n",
            "  80588/1000000: episode: 123, duration: 4.020s, episode steps: 666, steps per second: 166, episode reward: 95.000, mean reward:  0.143 [ 0.000, 30.000], mean action: 2.556 [0.000, 5.000],  loss: 45.396622, mean_q: 293.694248, mean_eps: 0.927771\n",
            "  81088/1000000: episode: 124, duration: 3.043s, episode steps: 500, steps per second: 164, episode reward: 120.000, mean reward:  0.240 [ 0.000, 30.000], mean action: 2.576 [0.000, 5.000],  loss: 50.536505, mean_q: 287.220536, mean_eps: 0.927246\n",
            "  81676/1000000: episode: 125, duration: 3.544s, episode steps: 588, steps per second: 166, episode reward: 90.000, mean reward:  0.153 [ 0.000, 25.000], mean action: 2.571 [0.000, 5.000],  loss: 37.363357, mean_q: 283.280683, mean_eps: 0.926757\n",
            "  82259/1000000: episode: 126, duration: 3.532s, episode steps: 583, steps per second: 165, episode reward: 105.000, mean reward:  0.180 [ 0.000, 25.000], mean action: 2.487 [0.000, 5.000],  loss: 51.656540, mean_q: 282.565335, mean_eps: 0.926230\n",
            "  83497/1000000: episode: 127, duration: 7.362s, episode steps: 1238, steps per second: 168, episode reward: 280.000, mean reward:  0.226 [ 0.000, 30.000], mean action: 2.447 [0.000, 5.000],  loss: 46.843399, mean_q: 294.399008, mean_eps: 0.925410\n",
            "  84096/1000000: episode: 128, duration: 3.552s, episode steps: 599, steps per second: 169, episode reward: 80.000, mean reward:  0.134 [ 0.000, 20.000], mean action: 2.584 [0.000, 5.000],  loss: 51.456651, mean_q: 299.729194, mean_eps: 0.924584\n",
            "  84753/1000000: episode: 129, duration: 3.927s, episode steps: 657, steps per second: 167, episode reward: 85.000, mean reward:  0.129 [ 0.000, 20.000], mean action: 2.531 [0.000, 5.000],  loss: 61.983106, mean_q: 292.111147, mean_eps: 0.924018\n",
            "  85178/1000000: episode: 130, duration: 2.547s, episode steps: 425, steps per second: 167, episode reward: 65.000, mean reward:  0.153 [ 0.000, 20.000], mean action: 2.456 [0.000, 5.000],  loss: 52.835350, mean_q: 291.835131, mean_eps: 0.923532\n",
            "  85661/1000000: episode: 131, duration: 2.864s, episode steps: 483, steps per second: 169, episode reward: 80.000, mean reward:  0.166 [ 0.000, 20.000], mean action: 2.553 [0.000, 5.000],  loss: 66.075220, mean_q: 285.645847, mean_eps: 0.923123\n",
            "  86158/1000000: episode: 132, duration: 2.946s, episode steps: 497, steps per second: 169, episode reward: 105.000, mean reward:  0.211 [ 0.000, 25.000], mean action: 2.634 [0.000, 5.000],  loss: 75.044134, mean_q: 281.115769, mean_eps: 0.922682\n",
            "  86792/1000000: episode: 133, duration: 3.811s, episode steps: 634, steps per second: 166, episode reward: 130.000, mean reward:  0.205 [ 0.000, 30.000], mean action: 2.473 [0.000, 5.000],  loss: 51.669436, mean_q: 282.076710, mean_eps: 0.922173\n",
            "  87544/1000000: episode: 134, duration: 4.505s, episode steps: 752, steps per second: 167, episode reward: 120.000, mean reward:  0.160 [ 0.000, 30.000], mean action: 2.418 [0.000, 5.000],  loss: 42.000223, mean_q: 283.079690, mean_eps: 0.921549\n",
            "  88265/1000000: episode: 135, duration: 4.307s, episode steps: 721, steps per second: 167, episode reward: 150.000, mean reward:  0.208 [ 0.000, 30.000], mean action: 2.411 [0.000, 5.000],  loss: 42.821835, mean_q: 289.395113, mean_eps: 0.920886\n",
            "  89067/1000000: episode: 136, duration: 4.799s, episode steps: 802, steps per second: 167, episode reward: 240.000, mean reward:  0.299 [ 0.000, 30.000], mean action: 2.596 [0.000, 5.000],  loss: 50.614981, mean_q: 284.674619, mean_eps: 0.920201\n",
            "  89938/1000000: episode: 137, duration: 5.175s, episode steps: 871, steps per second: 168, episode reward: 235.000, mean reward:  0.270 [ 0.000, 30.000], mean action: 2.506 [0.000, 5.000],  loss: 35.990158, mean_q: 282.440404, mean_eps: 0.919448\n",
            "  90843/1000000: episode: 138, duration: 5.393s, episode steps: 905, steps per second: 168, episode reward: 180.000, mean reward:  0.199 [ 0.000, 30.000], mean action: 2.519 [0.000, 5.000],  loss: 28.038511, mean_q: 286.558929, mean_eps: 0.918649\n",
            "  91799/1000000: episode: 139, duration: 5.723s, episode steps: 956, steps per second: 167, episode reward: 230.000, mean reward:  0.241 [ 0.000, 30.000], mean action: 2.523 [0.000, 5.000],  loss: 41.805746, mean_q: 294.375976, mean_eps: 0.917812\n",
            "  92576/1000000: episode: 140, duration: 4.651s, episode steps: 777, steps per second: 167, episode reward: 225.000, mean reward:  0.290 [ 0.000, 30.000], mean action: 2.485 [0.000, 5.000],  loss: 41.362832, mean_q: 293.817134, mean_eps: 0.917032\n",
            "  93256/1000000: episode: 141, duration: 4.002s, episode steps: 680, steps per second: 170, episode reward: 120.000, mean reward:  0.176 [ 0.000, 30.000], mean action: 2.454 [0.000, 5.000],  loss: 33.110832, mean_q: 287.720994, mean_eps: 0.916376\n",
            "  93963/1000000: episode: 142, duration: 4.211s, episode steps: 707, steps per second: 168, episode reward: 105.000, mean reward:  0.149 [ 0.000, 30.000], mean action: 2.474 [0.000, 5.000],  loss: 25.234136, mean_q: 286.782461, mean_eps: 0.915752\n",
            "  94501/1000000: episode: 143, duration: 3.216s, episode steps: 538, steps per second: 167, episode reward: 135.000, mean reward:  0.251 [ 0.000, 30.000], mean action: 2.478 [0.000, 5.000],  loss: 43.300892, mean_q: 284.152730, mean_eps: 0.915192\n",
            "  95201/1000000: episode: 144, duration: 4.185s, episode steps: 700, steps per second: 167, episode reward: 110.000, mean reward:  0.157 [ 0.000, 30.000], mean action: 2.536 [0.000, 5.000],  loss: 44.301064, mean_q: 290.143999, mean_eps: 0.914635\n",
            "  95801/1000000: episode: 145, duration: 3.618s, episode steps: 600, steps per second: 166, episode reward: 135.000, mean reward:  0.225 [ 0.000, 30.000], mean action: 2.433 [0.000, 5.000],  loss: 38.558947, mean_q: 293.106761, mean_eps: 0.914050\n",
            "  96311/1000000: episode: 146, duration: 3.047s, episode steps: 510, steps per second: 167, episode reward: 105.000, mean reward:  0.206 [ 0.000, 30.000], mean action: 2.557 [0.000, 5.000],  loss: 37.615835, mean_q: 285.115057, mean_eps: 0.913550\n",
            "  96966/1000000: episode: 147, duration: 3.938s, episode steps: 655, steps per second: 166, episode reward: 120.000, mean reward:  0.183 [ 0.000, 30.000], mean action: 2.398 [0.000, 5.000],  loss: 48.767390, mean_q: 281.846148, mean_eps: 0.913026\n",
            "  97676/1000000: episode: 148, duration: 4.243s, episode steps: 710, steps per second: 167, episode reward: 155.000, mean reward:  0.218 [ 0.000, 30.000], mean action: 2.583 [0.000, 5.000],  loss: 41.183508, mean_q: 281.581016, mean_eps: 0.912412\n",
            "  98067/1000000: episode: 149, duration: 2.360s, episode steps: 391, steps per second: 166, episode reward: 55.000, mean reward:  0.141 [ 0.000, 20.000], mean action: 2.529 [0.000, 5.000],  loss: 36.259766, mean_q: 282.334708, mean_eps: 0.911916\n",
            "  98494/1000000: episode: 150, duration: 2.573s, episode steps: 427, steps per second: 166, episode reward: 95.000, mean reward:  0.222 [ 0.000, 30.000], mean action: 2.405 [0.000, 5.000],  loss: 50.121031, mean_q: 282.126305, mean_eps: 0.911548\n",
            "  99200/1000000: episode: 151, duration: 4.216s, episode steps: 706, steps per second: 167, episode reward: 180.000, mean reward:  0.255 [ 0.000, 30.000], mean action: 2.436 [0.000, 5.000],  loss: 73.917820, mean_q: 286.366507, mean_eps: 0.911038\n",
            "  99865/1000000: episode: 152, duration: 3.958s, episode steps: 665, steps per second: 168, episode reward: 210.000, mean reward:  0.316 [ 0.000, 30.000], mean action: 2.481 [0.000, 5.000],  loss: 56.210540, mean_q: 286.033140, mean_eps: 0.910421\n",
            " 100547/1000000: episode: 153, duration: 4.140s, episode steps: 682, steps per second: 165, episode reward: 120.000, mean reward:  0.176 [ 0.000, 30.000], mean action: 2.522 [0.000, 5.000],  loss: 29.465710, mean_q: 277.015205, mean_eps: 0.909815\n",
            " 101129/1000000: episode: 154, duration: 3.500s, episode steps: 582, steps per second: 166, episode reward: 65.000, mean reward:  0.112 [ 0.000, 30.000], mean action: 2.443 [0.000, 5.000],  loss: 47.923004, mean_q: 279.942463, mean_eps: 0.909246\n",
            " 101780/1000000: episode: 155, duration: 3.991s, episode steps: 651, steps per second: 163, episode reward: 155.000, mean reward:  0.238 [ 0.000, 30.000], mean action: 2.479 [0.000, 5.000],  loss: 58.905451, mean_q: 287.603063, mean_eps: 0.908691\n",
            " 102495/1000000: episode: 156, duration: 4.270s, episode steps: 715, steps per second: 167, episode reward: 120.000, mean reward:  0.168 [ 0.000, 30.000], mean action: 2.422 [0.000, 5.000],  loss: 42.271368, mean_q: 290.722576, mean_eps: 0.908077\n",
            " 103513/1000000: episode: 157, duration: 6.096s, episode steps: 1018, steps per second: 167, episode reward: 320.000, mean reward:  0.314 [ 0.000, 30.000], mean action: 2.641 [0.000, 5.000],  loss: 34.770340, mean_q: 288.071690, mean_eps: 0.907297\n",
            " 103915/1000000: episode: 158, duration: 2.434s, episode steps: 402, steps per second: 165, episode reward: 50.000, mean reward:  0.124 [ 0.000, 20.000], mean action: 2.296 [0.000, 5.000],  loss: 27.868197, mean_q: 295.627580, mean_eps: 0.906658\n",
            " 104694/1000000: episode: 159, duration: 4.675s, episode steps: 779, steps per second: 167, episode reward: 135.000, mean reward:  0.173 [ 0.000, 30.000], mean action: 2.462 [0.000, 5.000],  loss: 50.714944, mean_q: 287.823019, mean_eps: 0.906126\n",
            " 105296/1000000: episode: 160, duration: 3.631s, episode steps: 602, steps per second: 166, episode reward: 110.000, mean reward:  0.183 [ 0.000, 30.000], mean action: 2.485 [0.000, 5.000],  loss: 44.417624, mean_q: 285.968692, mean_eps: 0.905505\n",
            " 105939/1000000: episode: 161, duration: 3.871s, episode steps: 643, steps per second: 166, episode reward: 150.000, mean reward:  0.233 [ 0.000, 25.000], mean action: 2.451 [0.000, 5.000],  loss: 44.035839, mean_q: 289.210892, mean_eps: 0.904945\n",
            " 106491/1000000: episode: 162, duration: 3.282s, episode steps: 552, steps per second: 168, episode reward: 125.000, mean reward:  0.226 [ 0.000, 25.000], mean action: 2.482 [0.000, 5.000],  loss: 60.696198, mean_q: 292.789607, mean_eps: 0.904407\n",
            " 107154/1000000: episode: 163, duration: 3.987s, episode steps: 663, steps per second: 166, episode reward: 110.000, mean reward:  0.166 [ 0.000, 30.000], mean action: 2.351 [0.000, 5.000],  loss: 64.938610, mean_q: 290.290484, mean_eps: 0.903860\n",
            " 107726/1000000: episode: 164, duration: 3.400s, episode steps: 572, steps per second: 168, episode reward: 125.000, mean reward:  0.219 [ 0.000, 25.000], mean action: 2.435 [0.000, 5.000],  loss: 34.647609, mean_q: 281.535385, mean_eps: 0.903304\n",
            " 108092/1000000: episode: 165, duration: 2.192s, episode steps: 366, steps per second: 167, episode reward: 110.000, mean reward:  0.301 [ 0.000, 30.000], mean action: 2.440 [0.000, 5.000],  loss: 33.185198, mean_q: 282.676603, mean_eps: 0.902882\n",
            " 109122/1000000: episode: 166, duration: 6.134s, episode steps: 1030, steps per second: 168, episode reward: 145.000, mean reward:  0.141 [ 0.000, 30.000], mean action: 2.518 [0.000, 5.000],  loss: 59.372549, mean_q: 294.794186, mean_eps: 0.902254\n",
            " 109949/1000000: episode: 167, duration: 4.910s, episode steps: 827, steps per second: 168, episode reward: 285.000, mean reward:  0.345 [ 0.000, 30.000], mean action: 2.562 [0.000, 5.000],  loss: 33.889771, mean_q: 292.757648, mean_eps: 0.901419\n",
            " 110595/1000000: episode: 168, duration: 3.864s, episode steps: 646, steps per second: 167, episode reward: 125.000, mean reward:  0.193 [ 0.000, 25.000], mean action: 2.494 [0.000, 5.000],  loss: 30.126894, mean_q: 282.188558, mean_eps: 0.900756\n",
            " 111101/1000000: episode: 169, duration: 2.984s, episode steps: 506, steps per second: 170, episode reward: 80.000, mean reward:  0.158 [ 0.000, 20.000], mean action: 2.453 [0.000, 5.000],  loss: 35.847268, mean_q: 282.118268, mean_eps: 0.900237\n",
            " 111756/1000000: episode: 170, duration: 3.901s, episode steps: 655, steps per second: 168, episode reward: 105.000, mean reward:  0.160 [ 0.000, 30.000], mean action: 2.560 [0.000, 5.000],  loss: 53.123689, mean_q: 286.865978, mean_eps: 0.899715\n",
            " 112449/1000000: episode: 171, duration: 4.220s, episode steps: 693, steps per second: 164, episode reward: 105.000, mean reward:  0.152 [ 0.000, 30.000], mean action: 2.359 [0.000, 5.000],  loss: 40.480546, mean_q: 286.163810, mean_eps: 0.899108\n",
            " 113603/1000000: episode: 172, duration: 6.897s, episode steps: 1154, steps per second: 167, episode reward: 225.000, mean reward:  0.195 [ 0.000, 30.000], mean action: 2.407 [0.000, 5.000],  loss: 25.620267, mean_q: 285.360120, mean_eps: 0.898277\n",
            " 114190/1000000: episode: 173, duration: 3.541s, episode steps: 587, steps per second: 166, episode reward: 125.000, mean reward:  0.213 [ 0.000, 30.000], mean action: 2.595 [0.000, 5.000],  loss: 43.297665, mean_q: 292.686828, mean_eps: 0.897494\n",
            " 115707/1000000: episode: 174, duration: 8.978s, episode steps: 1517, steps per second: 169, episode reward: 290.000, mean reward:  0.191 [ 0.000, 30.000], mean action: 2.500 [0.000, 5.000],  loss: 28.294990, mean_q: 292.094808, mean_eps: 0.896547\n",
            " 116860/1000000: episode: 175, duration: 6.848s, episode steps: 1153, steps per second: 168, episode reward: 315.000, mean reward:  0.273 [ 0.000, 30.000], mean action: 2.628 [0.000, 5.000],  loss: 37.644484, mean_q: 296.318308, mean_eps: 0.895345\n",
            " 117257/1000000: episode: 176, duration: 2.408s, episode steps: 397, steps per second: 165, episode reward: 65.000, mean reward:  0.164 [ 0.000, 20.000], mean action: 2.592 [0.000, 5.000],  loss: 43.661250, mean_q: 292.239955, mean_eps: 0.894648\n",
            " 117938/1000000: episode: 177, duration: 4.028s, episode steps: 681, steps per second: 169, episode reward: 210.000, mean reward:  0.308 [ 0.000, 30.000], mean action: 2.565 [0.000, 5.000],  loss: 71.783448, mean_q: 284.230643, mean_eps: 0.894163\n",
            " 118912/1000000: episode: 178, duration: 5.803s, episode steps: 974, steps per second: 168, episode reward: 170.000, mean reward:  0.175 [ 0.000, 30.000], mean action: 2.536 [0.000, 5.000],  loss: 35.545713, mean_q: 290.938431, mean_eps: 0.893418\n",
            " 119699/1000000: episode: 179, duration: 4.696s, episode steps: 787, steps per second: 168, episode reward: 390.000, mean reward:  0.496 [ 0.000, 200.000], mean action: 2.429 [0.000, 5.000],  loss: 48.971173, mean_q: 300.430716, mean_eps: 0.892626\n",
            " 120330/1000000: episode: 180, duration: 3.759s, episode steps: 631, steps per second: 168, episode reward: 80.000, mean reward:  0.127 [ 0.000, 25.000], mean action: 2.536 [0.000, 5.000],  loss: 62.645564, mean_q: 278.465863, mean_eps: 0.891987\n",
            " 120994/1000000: episode: 181, duration: 3.949s, episode steps: 664, steps per second: 168, episode reward: 155.000, mean reward:  0.233 [ 0.000, 30.000], mean action: 2.554 [0.000, 5.000],  loss: 34.489625, mean_q: 281.652624, mean_eps: 0.891405\n",
            " 121908/1000000: episode: 182, duration: 5.481s, episode steps: 914, steps per second: 167, episode reward: 215.000, mean reward:  0.235 [ 0.000, 30.000], mean action: 2.625 [0.000, 5.000],  loss: 37.772653, mean_q: 288.638689, mean_eps: 0.890695\n",
            " 122370/1000000: episode: 183, duration: 2.778s, episode steps: 462, steps per second: 166, episode reward: 50.000, mean reward:  0.108 [ 0.000, 20.000], mean action: 2.478 [0.000, 5.000],  loss: 36.272438, mean_q: 294.891405, mean_eps: 0.890075\n",
            " 123078/1000000: episode: 184, duration: 4.181s, episode steps: 708, steps per second: 169, episode reward: 120.000, mean reward:  0.169 [ 0.000, 30.000], mean action: 2.566 [0.000, 5.000],  loss: 43.262706, mean_q: 287.638772, mean_eps: 0.889549\n",
            " 123669/1000000: episode: 185, duration: 3.511s, episode steps: 591, steps per second: 168, episode reward: 125.000, mean reward:  0.212 [ 0.000, 30.000], mean action: 2.528 [0.000, 5.000],  loss: 31.642824, mean_q: 287.235160, mean_eps: 0.888964\n",
            " 124418/1000000: episode: 186, duration: 4.475s, episode steps: 749, steps per second: 167, episode reward: 135.000, mean reward:  0.180 [ 0.000, 30.000], mean action: 2.443 [0.000, 5.000],  loss: 43.899604, mean_q: 287.196110, mean_eps: 0.888361\n",
            " 124787/1000000: episode: 187, duration: 2.217s, episode steps: 369, steps per second: 166, episode reward: 80.000, mean reward:  0.217 [ 0.000, 20.000], mean action: 2.390 [0.000, 5.000],  loss: 37.973517, mean_q: 282.692835, mean_eps: 0.887858\n",
            " 125576/1000000: episode: 188, duration: 4.700s, episode steps: 789, steps per second: 168, episode reward: 110.000, mean reward:  0.139 [ 0.000, 30.000], mean action: 2.504 [0.000, 5.000],  loss: 57.576123, mean_q: 284.863102, mean_eps: 0.887337\n",
            " 126265/1000000: episode: 189, duration: 4.099s, episode steps: 689, steps per second: 168, episode reward: 155.000, mean reward:  0.225 [ 0.000, 30.000], mean action: 2.582 [0.000, 5.000],  loss: 36.939496, mean_q: 290.888927, mean_eps: 0.886672\n",
            " 127189/1000000: episode: 190, duration: 5.536s, episode steps: 924, steps per second: 167, episode reward: 155.000, mean reward:  0.168 [ 0.000, 30.000], mean action: 2.590 [0.000, 5.000],  loss: 38.241923, mean_q: 291.860567, mean_eps: 0.885946\n",
            " 127838/1000000: episode: 191, duration: 3.874s, episode steps: 649, steps per second: 168, episode reward: 105.000, mean reward:  0.162 [ 0.000, 30.000], mean action: 2.544 [0.000, 5.000],  loss: 33.790787, mean_q: 294.756922, mean_eps: 0.885238\n",
            " 128468/1000000: episode: 192, duration: 3.729s, episode steps: 630, steps per second: 169, episode reward: 210.000, mean reward:  0.333 [ 0.000, 30.000], mean action: 2.638 [0.000, 5.000],  loss: 48.170984, mean_q: 287.161600, mean_eps: 0.884663\n",
            " 129097/1000000: episode: 193, duration: 3.749s, episode steps: 629, steps per second: 168, episode reward: 165.000, mean reward:  0.262 [ 0.000, 30.000], mean action: 2.397 [0.000, 5.000],  loss: 32.702830, mean_q: 286.807924, mean_eps: 0.884096\n",
            " 130301/1000000: episode: 194, duration: 7.151s, episode steps: 1204, steps per second: 168, episode reward: 360.000, mean reward:  0.299 [ 0.000, 30.000], mean action: 2.448 [0.000, 5.000],  loss: 25.405647, mean_q: 288.261004, mean_eps: 0.883271\n",
            " 131171/1000000: episode: 195, duration: 5.178s, episode steps: 870, steps per second: 168, episode reward: 210.000, mean reward:  0.241 [ 0.000, 30.000], mean action: 2.523 [0.000, 5.000],  loss: 69.130573, mean_q: 303.921599, mean_eps: 0.882338\n",
            " 131925/1000000: episode: 196, duration: 4.526s, episode steps: 754, steps per second: 167, episode reward: 210.000, mean reward:  0.279 [ 0.000, 30.000], mean action: 2.468 [0.000, 5.000],  loss: 42.697164, mean_q: 278.207710, mean_eps: 0.881607\n",
            " 132436/1000000: episode: 197, duration: 3.092s, episode steps: 511, steps per second: 165, episode reward: 60.000, mean reward:  0.117 [ 0.000, 15.000], mean action: 2.579 [0.000, 5.000],  loss: 34.318500, mean_q: 282.843532, mean_eps: 0.881038\n",
            " 133095/1000000: episode: 198, duration: 3.975s, episode steps: 659, steps per second: 166, episode reward: 120.000, mean reward:  0.182 [ 0.000, 30.000], mean action: 2.624 [0.000, 5.000],  loss: 56.163239, mean_q: 285.102408, mean_eps: 0.880512\n",
            " 133768/1000000: episode: 199, duration: 4.006s, episode steps: 673, steps per second: 168, episode reward: 155.000, mean reward:  0.230 [ 0.000, 30.000], mean action: 2.646 [0.000, 5.000],  loss: 45.600915, mean_q: 286.721259, mean_eps: 0.879912\n",
            " 134481/1000000: episode: 200, duration: 4.217s, episode steps: 713, steps per second: 169, episode reward: 210.000, mean reward:  0.295 [ 0.000, 30.000], mean action: 2.380 [0.000, 5.000],  loss: 41.919943, mean_q: 278.476048, mean_eps: 0.879288\n",
            " 135231/1000000: episode: 201, duration: 4.475s, episode steps: 750, steps per second: 168, episode reward: 155.000, mean reward:  0.207 [ 0.000, 30.000], mean action: 2.540 [0.000, 5.000],  loss: 33.718153, mean_q: 274.714670, mean_eps: 0.878630\n",
            " 135605/1000000: episode: 202, duration: 2.274s, episode steps: 374, steps per second: 164, episode reward: 65.000, mean reward:  0.174 [ 0.000, 20.000], mean action: 2.652 [0.000, 5.000],  loss: 33.242632, mean_q: 280.889356, mean_eps: 0.878124\n",
            " 136138/1000000: episode: 203, duration: 3.242s, episode steps: 533, steps per second: 164, episode reward: 55.000, mean reward:  0.103 [ 0.000, 20.000], mean action: 2.664 [0.000, 5.000],  loss: 67.452278, mean_q: 284.580154, mean_eps: 0.877716\n",
            " 137418/1000000: episode: 204, duration: 7.755s, episode steps: 1280, steps per second: 165, episode reward: 365.000, mean reward:  0.285 [ 0.000, 30.000], mean action: 2.409 [0.000, 5.000],  loss: 46.902438, mean_q: 296.811754, mean_eps: 0.876900\n",
            " 137909/1000000: episode: 205, duration: 2.942s, episode steps: 491, steps per second: 167, episode reward: 35.000, mean reward:  0.071 [ 0.000, 15.000], mean action: 2.255 [0.000, 5.000],  loss: 55.100960, mean_q: 294.916343, mean_eps: 0.876103\n",
            " 138735/1000000: episode: 206, duration: 4.945s, episode steps: 826, steps per second: 167, episode reward: 355.000, mean reward:  0.430 [ 0.000, 200.000], mean action: 2.609 [0.000, 5.000],  loss: 69.334710, mean_q: 291.883530, mean_eps: 0.875511\n",
            " 139227/1000000: episode: 207, duration: 2.953s, episode steps: 492, steps per second: 167, episode reward: 55.000, mean reward:  0.112 [ 0.000, 20.000], mean action: 2.494 [0.000, 5.000],  loss: 60.405302, mean_q: 290.715659, mean_eps: 0.874918\n",
            " 139671/1000000: episode: 208, duration: 2.657s, episode steps: 444, steps per second: 167, episode reward: 55.000, mean reward:  0.124 [ 0.000, 20.000], mean action: 2.556 [0.000, 5.000],  loss: 53.616163, mean_q: 285.637023, mean_eps: 0.874496\n",
            " 140330/1000000: episode: 209, duration: 3.944s, episode steps: 659, steps per second: 167, episode reward: 155.000, mean reward:  0.235 [ 0.000, 30.000], mean action: 2.551 [0.000, 5.000],  loss: 56.293724, mean_q: 289.154900, mean_eps: 0.874000\n",
            " 140979/1000000: episode: 210, duration: 3.906s, episode steps: 649, steps per second: 166, episode reward: 110.000, mean reward:  0.169 [ 0.000, 30.000], mean action: 2.492 [0.000, 5.000],  loss: 50.086104, mean_q: 294.388292, mean_eps: 0.873411\n",
            " 141382/1000000: episode: 211, duration: 2.440s, episode steps: 403, steps per second: 165, episode reward: 90.000, mean reward:  0.223 [ 0.000, 30.000], mean action: 2.533 [0.000, 5.000],  loss: 47.091916, mean_q: 290.518866, mean_eps: 0.872938\n",
            " 141990/1000000: episode: 212, duration: 3.660s, episode steps: 608, steps per second: 166, episode reward: 90.000, mean reward:  0.148 [ 0.000, 25.000], mean action: 2.456 [0.000, 5.000],  loss: 52.531544, mean_q: 289.361187, mean_eps: 0.872483\n",
            " 143095/1000000: episode: 213, duration: 6.582s, episode steps: 1105, steps per second: 168, episode reward: 320.000, mean reward:  0.290 [ 0.000, 30.000], mean action: 2.378 [0.000, 5.000],  loss: 34.091589, mean_q: 302.441610, mean_eps: 0.871712\n",
            " 143483/1000000: episode: 214, duration: 2.338s, episode steps: 388, steps per second: 166, episode reward: 45.000, mean reward:  0.116 [ 0.000, 15.000], mean action: 2.546 [0.000, 5.000],  loss: 56.993542, mean_q: 326.060781, mean_eps: 0.871040\n",
            " 144093/1000000: episode: 215, duration: 3.651s, episode steps: 610, steps per second: 167, episode reward: 110.000, mean reward:  0.180 [ 0.000, 30.000], mean action: 2.644 [0.000, 5.000],  loss: 59.579917, mean_q: 305.798237, mean_eps: 0.870591\n",
            " 144730/1000000: episode: 216, duration: 3.810s, episode steps: 637, steps per second: 167, episode reward: 125.000, mean reward:  0.196 [ 0.000, 30.000], mean action: 2.548 [0.000, 5.000],  loss: 44.273243, mean_q: 297.421241, mean_eps: 0.870030\n",
            " 145099/1000000: episode: 217, duration: 2.191s, episode steps: 369, steps per second: 168, episode reward: 70.000, mean reward:  0.190 [ 0.000, 25.000], mean action: 2.396 [0.000, 5.000],  loss: 78.724101, mean_q: 302.191963, mean_eps: 0.869577\n",
            " 145490/1000000: episode: 218, duration: 2.344s, episode steps: 391, steps per second: 167, episode reward: 100.000, mean reward:  0.256 [ 0.000, 20.000], mean action: 2.575 [0.000, 5.000],  loss: 73.440972, mean_q: 297.437524, mean_eps: 0.869235\n",
            " 146075/1000000: episode: 219, duration: 3.545s, episode steps: 585, steps per second: 165, episode reward: 130.000, mean reward:  0.222 [ 0.000, 20.000], mean action: 2.509 [0.000, 5.000],  loss: 81.870833, mean_q: 288.905489, mean_eps: 0.868796\n",
            " 146736/1000000: episode: 220, duration: 3.949s, episode steps: 661, steps per second: 167, episode reward: 135.000, mean reward:  0.204 [ 0.000, 30.000], mean action: 2.628 [0.000, 5.000],  loss: 52.802225, mean_q: 291.260107, mean_eps: 0.868235\n",
            " 147179/1000000: episode: 221, duration: 2.667s, episode steps: 443, steps per second: 166, episode reward: 50.000, mean reward:  0.113 [ 0.000, 15.000], mean action: 2.555 [0.000, 5.000],  loss: 43.473434, mean_q: 288.745651, mean_eps: 0.867739\n",
            " 147953/1000000: episode: 222, duration: 4.616s, episode steps: 774, steps per second: 168, episode reward: 195.000, mean reward:  0.252 [ 0.000, 30.000], mean action: 2.575 [0.000, 5.000],  loss: 35.805272, mean_q: 288.133681, mean_eps: 0.867191\n",
            " 148817/1000000: episode: 223, duration: 5.131s, episode steps: 864, steps per second: 168, episode reward: 210.000, mean reward:  0.243 [ 0.000, 30.000], mean action: 2.523 [0.000, 5.000],  loss: 37.538045, mean_q: 291.316852, mean_eps: 0.866454\n",
            " 149943/1000000: episode: 224, duration: 6.715s, episode steps: 1126, steps per second: 168, episode reward: 265.000, mean reward:  0.235 [ 0.000, 30.000], mean action: 2.544 [0.000, 5.000],  loss: 30.107262, mean_q: 294.559274, mean_eps: 0.865558\n",
            " 150579/1000000: episode: 225, duration: 3.840s, episode steps: 636, steps per second: 166, episode reward: 135.000, mean reward:  0.212 [ 0.000, 30.000], mean action: 2.497 [0.000, 5.000],  loss: 53.073302, mean_q: 310.403205, mean_eps: 0.864766\n",
            " 151350/1000000: episode: 226, duration: 4.630s, episode steps: 771, steps per second: 167, episode reward: 180.000, mean reward:  0.233 [ 0.000, 30.000], mean action: 2.414 [0.000, 5.000],  loss: 49.334760, mean_q: 294.485677, mean_eps: 0.864132\n",
            " 151934/1000000: episode: 227, duration: 3.494s, episode steps: 584, steps per second: 167, episode reward: 135.000, mean reward:  0.231 [ 0.000, 30.000], mean action: 2.471 [0.000, 5.000],  loss: 32.202754, mean_q: 283.927375, mean_eps: 0.863523\n",
            " 152348/1000000: episode: 228, duration: 2.514s, episode steps: 414, steps per second: 165, episode reward: 55.000, mean reward:  0.133 [ 0.000, 20.000], mean action: 2.440 [0.000, 5.000],  loss: 55.255710, mean_q: 286.541624, mean_eps: 0.863074\n",
            " 153752/1000000: episode: 229, duration: 8.376s, episode steps: 1404, steps per second: 168, episode reward: 485.000, mean reward:  0.345 [ 0.000, 200.000], mean action: 2.635 [0.000, 5.000],  loss: 39.522955, mean_q: 294.796853, mean_eps: 0.862255\n",
            " 154534/1000000: episode: 230, duration: 4.691s, episode steps: 782, steps per second: 167, episode reward: 225.000, mean reward:  0.288 [ 0.000, 30.000], mean action: 2.450 [0.000, 5.000],  loss: 35.285334, mean_q: 285.366412, mean_eps: 0.861272\n",
            " 155189/1000000: episode: 231, duration: 3.999s, episode steps: 655, steps per second: 164, episode reward: 140.000, mean reward:  0.214 [ 0.000, 25.000], mean action: 2.617 [0.000, 5.000],  loss: 34.099318, mean_q: 291.762231, mean_eps: 0.860625\n",
            " 155801/1000000: episode: 232, duration: 3.613s, episode steps: 612, steps per second: 169, episode reward: 100.000, mean reward:  0.163 [ 0.000, 20.000], mean action: 2.567 [0.000, 5.000],  loss: 72.555507, mean_q: 289.130090, mean_eps: 0.860055\n",
            " 156558/1000000: episode: 233, duration: 4.510s, episode steps: 757, steps per second: 168, episode reward: 135.000, mean reward:  0.178 [ 0.000, 30.000], mean action: 2.542 [0.000, 5.000],  loss: 37.303336, mean_q: 293.067035, mean_eps: 0.859439\n",
            " 157217/1000000: episode: 234, duration: 3.975s, episode steps: 659, steps per second: 166, episode reward: 75.000, mean reward:  0.114 [ 0.000, 25.000], mean action: 2.610 [0.000, 5.000],  loss: 30.320589, mean_q: 288.869474, mean_eps: 0.858802\n",
            " 157912/1000000: episode: 235, duration: 4.160s, episode steps: 695, steps per second: 167, episode reward: 110.000, mean reward:  0.158 [ 0.000, 30.000], mean action: 2.583 [0.000, 5.000],  loss: 51.204038, mean_q: 294.176885, mean_eps: 0.858192\n",
            " 158532/1000000: episode: 236, duration: 3.714s, episode steps: 620, steps per second: 167, episode reward: 135.000, mean reward:  0.218 [ 0.000, 30.000], mean action: 2.515 [0.000, 5.000],  loss: 42.253329, mean_q: 293.840186, mean_eps: 0.857601\n",
            " 159524/1000000: episode: 237, duration: 5.898s, episode steps: 992, steps per second: 168, episode reward: 290.000, mean reward:  0.292 [ 0.000, 30.000], mean action: 2.445 [0.000, 5.000],  loss: 37.389145, mean_q: 295.416466, mean_eps: 0.856875\n",
            " 160234/1000000: episode: 238, duration: 4.225s, episode steps: 710, steps per second: 168, episode reward: 210.000, mean reward:  0.296 [ 0.000, 30.000], mean action: 2.389 [0.000, 5.000],  loss: 47.296850, mean_q: 315.455053, mean_eps: 0.856109\n",
            " 160823/1000000: episode: 239, duration: 3.538s, episode steps: 589, steps per second: 166, episode reward: 80.000, mean reward:  0.136 [ 0.000, 25.000], mean action: 2.513 [0.000, 5.000],  loss: 42.341597, mean_q: 291.720128, mean_eps: 0.855525\n",
            " 161676/1000000: episode: 240, duration: 5.088s, episode steps: 853, steps per second: 168, episode reward: 255.000, mean reward:  0.299 [ 0.000, 30.000], mean action: 2.671 [0.000, 5.000],  loss: 49.022064, mean_q: 288.125956, mean_eps: 0.854876\n",
            " 162221/1000000: episode: 241, duration: 3.250s, episode steps: 545, steps per second: 168, episode reward: 95.000, mean reward:  0.174 [ 0.000, 30.000], mean action: 2.569 [0.000, 5.000],  loss: 50.130928, mean_q: 292.429270, mean_eps: 0.854247\n",
            " 163026/1000000: episode: 242, duration: 4.864s, episode steps: 805, steps per second: 165, episode reward: 210.000, mean reward:  0.261 [ 0.000, 30.000], mean action: 2.507 [0.000, 5.000],  loss: 51.573582, mean_q: 294.879441, mean_eps: 0.853639\n",
            " 163694/1000000: episode: 243, duration: 3.978s, episode steps: 668, steps per second: 168, episode reward: 120.000, mean reward:  0.180 [ 0.000, 30.000], mean action: 2.578 [0.000, 5.000],  loss: 29.878042, mean_q: 287.960420, mean_eps: 0.852976\n",
            " 164226/1000000: episode: 244, duration: 3.165s, episode steps: 532, steps per second: 168, episode reward:  5.000, mean reward:  0.009 [ 0.000,  5.000], mean action: 2.444 [0.000, 5.000],  loss: 35.977698, mean_q: 294.063912, mean_eps: 0.852436\n",
            " 164752/1000000: episode: 245, duration: 3.159s, episode steps: 526, steps per second: 167, episode reward: 155.000, mean reward:  0.295 [ 0.000, 30.000], mean action: 2.496 [0.000, 5.000],  loss: 46.114409, mean_q: 293.392633, mean_eps: 0.851960\n",
            " 165268/1000000: episode: 246, duration: 3.082s, episode steps: 516, steps per second: 167, episode reward: 120.000, mean reward:  0.233 [ 0.000, 30.000], mean action: 2.579 [0.000, 5.000],  loss: 63.212173, mean_q: 293.629079, mean_eps: 0.851491\n",
            " 166554/1000000: episode: 247, duration: 7.612s, episode steps: 1286, steps per second: 169, episode reward: 245.000, mean reward:  0.191 [ 0.000, 30.000], mean action: 2.400 [0.000, 5.000],  loss: 50.641569, mean_q: 306.622838, mean_eps: 0.850681\n",
            " 167585/1000000: episode: 248, duration: 6.157s, episode steps: 1031, steps per second: 167, episode reward: 205.000, mean reward:  0.199 [ 0.000, 30.000], mean action: 2.461 [0.000, 5.000],  loss: 51.580120, mean_q: 314.491087, mean_eps: 0.849638\n",
            " 168533/1000000: episode: 249, duration: 5.594s, episode steps: 948, steps per second: 169, episode reward: 390.000, mean reward:  0.411 [ 0.000, 30.000], mean action: 2.393 [0.000, 5.000],  loss: 31.327186, mean_q: 313.559812, mean_eps: 0.848747\n",
            " 169170/1000000: episode: 250, duration: 3.821s, episode steps: 637, steps per second: 167, episode reward: 180.000, mean reward:  0.283 [ 0.000, 30.000], mean action: 2.582 [0.000, 5.000],  loss: 42.484121, mean_q: 323.654585, mean_eps: 0.848034\n",
            " 169697/1000000: episode: 251, duration: 3.167s, episode steps: 527, steps per second: 166, episode reward: 80.000, mean reward:  0.152 [ 0.000, 25.000], mean action: 2.509 [0.000, 5.000],  loss: 51.319187, mean_q: 299.522799, mean_eps: 0.847510\n",
            " 170148/1000000: episode: 252, duration: 2.723s, episode steps: 451, steps per second: 166, episode reward: 75.000, mean reward:  0.166 [ 0.000, 25.000], mean action: 2.463 [0.000, 5.000],  loss: 39.965036, mean_q: 287.802421, mean_eps: 0.847070\n",
            " 171075/1000000: episode: 253, duration: 5.516s, episode steps: 927, steps per second: 168, episode reward: 250.000, mean reward:  0.270 [ 0.000, 30.000], mean action: 2.429 [0.000, 5.000],  loss: 47.571944, mean_q: 289.072317, mean_eps: 0.846450\n",
            " 172359/1000000: episode: 254, duration: 7.683s, episode steps: 1284, steps per second: 167, episode reward: 630.000, mean reward:  0.491 [ 0.000, 200.000], mean action: 2.437 [0.000, 5.000],  loss: 35.985543, mean_q: 281.248487, mean_eps: 0.845455\n",
            " 173131/1000000: episode: 255, duration: 4.648s, episode steps: 772, steps per second: 166, episode reward: 135.000, mean reward:  0.175 [ 0.000, 30.000], mean action: 2.490 [0.000, 5.000],  loss: 25.393779, mean_q: 289.385928, mean_eps: 0.844530\n",
            " 173768/1000000: episode: 256, duration: 3.795s, episode steps: 637, steps per second: 168, episode reward: 120.000, mean reward:  0.188 [ 0.000, 30.000], mean action: 2.317 [0.000, 5.000],  loss: 25.817930, mean_q: 287.118832, mean_eps: 0.843896\n",
            " 174587/1000000: episode: 257, duration: 4.869s, episode steps: 819, steps per second: 168, episode reward: 210.000, mean reward:  0.256 [ 0.000, 30.000], mean action: 2.595 [0.000, 5.000],  loss: 21.141097, mean_q: 294.338120, mean_eps: 0.843241\n",
            " 175127/1000000: episode: 258, duration: 3.271s, episode steps: 540, steps per second: 165, episode reward: 80.000, mean reward:  0.148 [ 0.000, 20.000], mean action: 2.531 [0.000, 5.000],  loss: 30.166574, mean_q: 300.287898, mean_eps: 0.842629\n",
            " 175628/1000000: episode: 259, duration: 3.013s, episode steps: 501, steps per second: 166, episode reward: 120.000, mean reward:  0.240 [ 0.000, 30.000], mean action: 2.553 [0.000, 5.000],  loss: 47.033518, mean_q: 296.761308, mean_eps: 0.842161\n",
            " 176098/1000000: episode: 260, duration: 2.825s, episode steps: 470, steps per second: 166, episode reward: 45.000, mean reward:  0.096 [ 0.000, 25.000], mean action: 2.362 [0.000, 5.000],  loss: 33.674452, mean_q: 294.327624, mean_eps: 0.841724\n",
            " 177345/1000000: episode: 261, duration: 7.431s, episode steps: 1247, steps per second: 168, episode reward: 360.000, mean reward:  0.289 [ 0.000, 30.000], mean action: 2.468 [0.000, 5.000],  loss: 26.157466, mean_q: 297.221468, mean_eps: 0.840951\n",
            " 177692/1000000: episode: 262, duration: 2.082s, episode steps: 347, steps per second: 167, episode reward: 50.000, mean reward:  0.144 [ 0.000, 20.000], mean action: 2.559 [0.000, 5.000],  loss: 33.488415, mean_q: 292.926070, mean_eps: 0.840234\n",
            " 178192/1000000: episode: 263, duration: 3.021s, episode steps: 500, steps per second: 165, episode reward: 30.000, mean reward:  0.060 [ 0.000, 15.000], mean action: 2.582 [0.000, 5.000],  loss: 67.002674, mean_q: 288.660293, mean_eps: 0.839853\n",
            " 178825/1000000: episode: 264, duration: 3.813s, episode steps: 633, steps per second: 166, episode reward: 155.000, mean reward:  0.245 [ 0.000, 30.000], mean action: 2.581 [0.000, 5.000],  loss: 82.884570, mean_q: 291.429561, mean_eps: 0.839343\n",
            " 179460/1000000: episode: 265, duration: 3.828s, episode steps: 635, steps per second: 166, episode reward: 110.000, mean reward:  0.173 [ 0.000, 20.000], mean action: 2.293 [0.000, 5.000],  loss: 35.570275, mean_q: 290.998209, mean_eps: 0.838772\n",
            " 180151/1000000: episode: 266, duration: 4.153s, episode steps: 691, steps per second: 166, episode reward: 135.000, mean reward:  0.195 [ 0.000, 30.000], mean action: 2.427 [0.000, 5.000],  loss: 29.852444, mean_q: 294.987707, mean_eps: 0.838176\n",
            " 180816/1000000: episode: 267, duration: 3.947s, episode steps: 665, steps per second: 168, episode reward: 150.000, mean reward:  0.226 [ 0.000, 25.000], mean action: 2.406 [0.000, 5.000],  loss: 46.437295, mean_q: 291.285248, mean_eps: 0.837565\n",
            " 181365/1000000: episode: 268, duration: 3.309s, episode steps: 549, steps per second: 166, episode reward: 30.000, mean reward:  0.055 [ 0.000, 10.000], mean action: 2.415 [0.000, 5.000],  loss: 49.647288, mean_q: 299.073181, mean_eps: 0.837019\n",
            " 182513/1000000: episode: 269, duration: 6.852s, episode steps: 1148, steps per second: 168, episode reward: 180.000, mean reward:  0.157 [ 0.000, 30.000], mean action: 2.432 [0.000, 5.000],  loss: 45.482159, mean_q: 295.620379, mean_eps: 0.836255\n",
            " 183058/1000000: episode: 270, duration: 3.261s, episode steps: 545, steps per second: 167, episode reward: 50.000, mean reward:  0.092 [ 0.000, 20.000], mean action: 2.484 [0.000, 5.000],  loss: 34.175525, mean_q: 303.865533, mean_eps: 0.835494\n",
            " 183673/1000000: episode: 271, duration: 3.703s, episode steps: 615, steps per second: 166, episode reward: 100.000, mean reward:  0.163 [ 0.000, 20.000], mean action: 2.382 [0.000, 5.000],  loss: 52.105606, mean_q: 299.759731, mean_eps: 0.834971\n",
            " 184931/1000000: episode: 272, duration: 7.581s, episode steps: 1258, steps per second: 166, episode reward: 195.000, mean reward:  0.155 [ 0.000, 30.000], mean action: 2.508 [0.000, 5.000],  loss: 45.565438, mean_q: 309.713371, mean_eps: 0.834129\n",
            " 185432/1000000: episode: 273, duration: 3.030s, episode steps: 501, steps per second: 165, episode reward: 30.000, mean reward:  0.060 [ 0.000, 10.000], mean action: 2.515 [0.000, 5.000],  loss: 39.216966, mean_q: 309.549308, mean_eps: 0.833337\n",
            " 185943/1000000: episode: 274, duration: 3.058s, episode steps: 511, steps per second: 167, episode reward: 105.000, mean reward:  0.205 [ 0.000, 30.000], mean action: 2.487 [0.000, 5.000],  loss: 63.299258, mean_q: 295.102312, mean_eps: 0.832882\n",
            " 186497/1000000: episode: 275, duration: 3.354s, episode steps: 554, steps per second: 165, episode reward: 55.000, mean reward:  0.099 [ 0.000, 20.000], mean action: 2.560 [0.000, 5.000],  loss: 34.375433, mean_q: 286.111144, mean_eps: 0.832402\n",
            " 187130/1000000: episode: 276, duration: 3.800s, episode steps: 633, steps per second: 167, episode reward: 120.000, mean reward:  0.190 [ 0.000, 30.000], mean action: 2.547 [0.000, 5.000],  loss: 34.995929, mean_q: 290.173505, mean_eps: 0.831868\n",
            " 187892/1000000: episode: 277, duration: 4.586s, episode steps: 762, steps per second: 166, episode reward: 180.000, mean reward:  0.236 [ 0.000, 30.000], mean action: 2.430 [0.000, 5.000],  loss: 63.503901, mean_q: 300.103525, mean_eps: 0.831241\n",
            " 188286/1000000: episode: 278, duration: 2.371s, episode steps: 394, steps per second: 166, episode reward: 65.000, mean reward:  0.165 [ 0.000, 20.000], mean action: 2.612 [0.000, 5.000],  loss: 45.773189, mean_q: 300.494401, mean_eps: 0.830720\n",
            " 188886/1000000: episode: 279, duration: 3.592s, episode steps: 600, steps per second: 167, episode reward: 120.000, mean reward:  0.200 [ 0.000, 30.000], mean action: 2.497 [0.000, 5.000],  loss: 59.692953, mean_q: 295.287135, mean_eps: 0.830273\n",
            " 189406/1000000: episode: 280, duration: 3.155s, episode steps: 520, steps per second: 165, episode reward: 30.000, mean reward:  0.058 [ 0.000, 10.000], mean action: 2.723 [0.000, 5.000],  loss: 55.443700, mean_q: 291.660721, mean_eps: 0.829769\n",
            " 189910/1000000: episode: 281, duration: 3.027s, episode steps: 504, steps per second: 167, episode reward: 80.000, mean reward:  0.159 [ 0.000, 20.000], mean action: 2.504 [0.000, 5.000],  loss: 38.002295, mean_q: 291.266265, mean_eps: 0.829308\n",
            " 190645/1000000: episode: 282, duration: 4.392s, episode steps: 735, steps per second: 167, episode reward: 135.000, mean reward:  0.184 [ 0.000, 30.000], mean action: 2.442 [0.000, 5.000],  loss: 37.259321, mean_q: 298.202496, mean_eps: 0.828751\n",
            " 191239/1000000: episode: 283, duration: 3.530s, episode steps: 594, steps per second: 168, episode reward: 55.000, mean reward:  0.093 [ 0.000, 20.000], mean action: 2.310 [0.000, 5.000],  loss: 41.600094, mean_q: 291.808891, mean_eps: 0.828153\n",
            " 191658/1000000: episode: 284, duration: 2.542s, episode steps: 419, steps per second: 165, episode reward: 105.000, mean reward:  0.251 [ 0.000, 30.000], mean action: 2.403 [0.000, 5.000],  loss: 41.100587, mean_q: 294.415346, mean_eps: 0.827697\n",
            " 192056/1000000: episode: 285, duration: 2.392s, episode steps: 398, steps per second: 166, episode reward: 30.000, mean reward:  0.075 [ 0.000, 15.000], mean action: 2.377 [0.000, 5.000],  loss: 58.220902, mean_q: 296.526990, mean_eps: 0.827329\n",
            " 192653/1000000: episode: 286, duration: 3.595s, episode steps: 597, steps per second: 166, episode reward: 120.000, mean reward:  0.201 [ 0.000, 30.000], mean action: 2.379 [0.000, 5.000],  loss: 57.319871, mean_q: 293.987218, mean_eps: 0.826881\n",
            " 193362/1000000: episode: 287, duration: 4.210s, episode steps: 709, steps per second: 168, episode reward: 170.000, mean reward:  0.240 [ 0.000, 30.000], mean action: 2.472 [0.000, 5.000],  loss: 40.760805, mean_q: 286.711514, mean_eps: 0.826294\n",
            " 194134/1000000: episode: 288, duration: 4.636s, episode steps: 772, steps per second: 167, episode reward: 165.000, mean reward:  0.214 [ 0.000, 30.000], mean action: 2.675 [0.000, 5.000],  loss: 24.108389, mean_q: 280.817322, mean_eps: 0.825627\n",
            " 194603/1000000: episode: 289, duration: 2.807s, episode steps: 469, steps per second: 167, episode reward: 55.000, mean reward:  0.117 [ 0.000, 20.000], mean action: 2.380 [0.000, 5.000],  loss: 28.712204, mean_q: 294.370230, mean_eps: 0.825069\n",
            " 195080/1000000: episode: 290, duration: 2.841s, episode steps: 477, steps per second: 168, episode reward: 90.000, mean reward:  0.189 [ 0.000, 30.000], mean action: 2.532 [0.000, 5.000],  loss: 33.310132, mean_q: 295.456304, mean_eps: 0.824643\n",
            " 195925/1000000: episode: 291, duration: 5.001s, episode steps: 845, steps per second: 169, episode reward: 370.000, mean reward:  0.438 [ 0.000, 200.000], mean action: 2.607 [0.000, 5.000],  loss: 71.900688, mean_q: 299.694208, mean_eps: 0.824048\n",
            " 196561/1000000: episode: 292, duration: 3.803s, episode steps: 636, steps per second: 167, episode reward: 155.000, mean reward:  0.244 [ 0.000, 30.000], mean action: 2.557 [0.000, 5.000],  loss: 75.661130, mean_q: 287.298831, mean_eps: 0.823382\n",
            " 197292/1000000: episode: 293, duration: 4.401s, episode steps: 731, steps per second: 166, episode reward: 210.000, mean reward:  0.287 [ 0.000, 30.000], mean action: 2.228 [0.000, 5.000],  loss: 37.446330, mean_q: 297.163442, mean_eps: 0.822767\n",
            " 197789/1000000: episode: 294, duration: 2.959s, episode steps: 497, steps per second: 168, episode reward: 105.000, mean reward:  0.211 [ 0.000, 25.000], mean action: 2.471 [0.000, 5.000],  loss: 41.593756, mean_q: 302.107107, mean_eps: 0.822214\n",
            " 198291/1000000: episode: 295, duration: 2.997s, episode steps: 502, steps per second: 168, episode reward: 90.000, mean reward:  0.179 [ 0.000, 25.000], mean action: 2.442 [0.000, 5.000],  loss: 60.868476, mean_q: 302.485992, mean_eps: 0.821764\n",
            " 198965/1000000: episode: 296, duration: 3.980s, episode steps: 674, steps per second: 169, episode reward: 95.000, mean reward:  0.141 [ 0.000, 20.000], mean action: 2.404 [0.000, 5.000],  loss: 74.124729, mean_q: 299.724691, mean_eps: 0.821235\n",
            " 199526/1000000: episode: 297, duration: 3.314s, episode steps: 561, steps per second: 169, episode reward: 120.000, mean reward:  0.214 [ 0.000, 30.000], mean action: 2.574 [0.000, 5.000],  loss: 74.478230, mean_q: 302.643921, mean_eps: 0.820680\n",
            " 199916/1000000: episode: 298, duration: 2.347s, episode steps: 390, steps per second: 166, episode reward: 80.000, mean reward:  0.205 [ 0.000, 20.000], mean action: 2.556 [0.000, 5.000],  loss: 63.092885, mean_q: 304.978950, mean_eps: 0.820252\n",
            " 200316/1000000: episode: 299, duration: 2.418s, episode steps: 400, steps per second: 165, episode reward: 75.000, mean reward:  0.188 [ 0.000, 25.000], mean action: 2.420 [0.000, 5.000],  loss: 82.794728, mean_q: 299.157989, mean_eps: 0.819896\n",
            " 200834/1000000: episode: 300, duration: 3.126s, episode steps: 518, steps per second: 166, episode reward: 65.000, mean reward:  0.125 [ 0.000, 20.000], mean action: 2.571 [0.000, 5.000],  loss: 59.283581, mean_q: 296.127590, mean_eps: 0.819483\n",
            " 201340/1000000: episode: 301, duration: 2.995s, episode steps: 506, steps per second: 169, episode reward: 110.000, mean reward:  0.217 [ 0.000, 30.000], mean action: 2.425 [0.000, 5.000],  loss: 53.047642, mean_q: 298.507515, mean_eps: 0.819022\n",
            " 201730/1000000: episode: 302, duration: 2.331s, episode steps: 390, steps per second: 167, episode reward: 120.000, mean reward:  0.308 [ 0.000, 30.000], mean action: 2.541 [0.000, 5.000],  loss: 28.482106, mean_q: 299.607067, mean_eps: 0.818619\n",
            " 203062/1000000: episode: 303, duration: 7.884s, episode steps: 1332, steps per second: 169, episode reward: 490.000, mean reward:  0.368 [ 0.000, 200.000], mean action: 2.422 [0.000, 5.000],  loss: 43.993024, mean_q: 299.414678, mean_eps: 0.817844\n",
            " 203880/1000000: episode: 304, duration: 4.899s, episode steps: 818, steps per second: 167, episode reward: 155.000, mean reward:  0.189 [ 0.000, 30.000], mean action: 2.421 [0.000, 5.000],  loss: 35.523608, mean_q: 299.678170, mean_eps: 0.816877\n",
            " 204822/1000000: episode: 305, duration: 5.620s, episode steps: 942, steps per second: 168, episode reward: 180.000, mean reward:  0.191 [ 0.000, 30.000], mean action: 2.340 [0.000, 5.000],  loss: 15.071687, mean_q: 295.918045, mean_eps: 0.816085\n",
            " 205426/1000000: episode: 306, duration: 3.599s, episode steps: 604, steps per second: 168, episode reward: 110.000, mean reward:  0.182 [ 0.000, 30.000], mean action: 2.391 [0.000, 5.000],  loss: 21.273831, mean_q: 284.041993, mean_eps: 0.815389\n",
            " 206297/1000000: episode: 307, duration: 5.202s, episode steps: 871, steps per second: 167, episode reward: 155.000, mean reward:  0.178 [ 0.000, 30.000], mean action: 2.518 [0.000, 5.000],  loss: 32.718759, mean_q: 289.083327, mean_eps: 0.814725\n",
            " 207079/1000000: episode: 308, duration: 4.691s, episode steps: 782, steps per second: 167, episode reward: 155.000, mean reward:  0.198 [ 0.000, 30.000], mean action: 2.499 [0.000, 5.000],  loss: 68.481132, mean_q: 285.840513, mean_eps: 0.813981\n",
            " 207763/1000000: episode: 309, duration: 4.087s, episode steps: 684, steps per second: 167, episode reward: 105.000, mean reward:  0.154 [ 0.000, 30.000], mean action: 2.389 [0.000, 5.000],  loss: 29.007790, mean_q: 281.051030, mean_eps: 0.813322\n",
            " 208334/1000000: episode: 310, duration: 3.418s, episode steps: 571, steps per second: 167, episode reward: 45.000, mean reward:  0.079 [ 0.000, 15.000], mean action: 2.371 [0.000, 5.000],  loss: 34.376581, mean_q: 280.132196, mean_eps: 0.812757\n",
            " 208983/1000000: episode: 311, duration: 3.868s, episode steps: 649, steps per second: 168, episode reward: 180.000, mean reward:  0.277 [ 0.000, 30.000], mean action: 2.505 [0.000, 5.000],  loss: 42.175117, mean_q: 289.828895, mean_eps: 0.812208\n",
            " 209764/1000000: episode: 312, duration: 4.678s, episode steps: 781, steps per second: 167, episode reward: 180.000, mean reward:  0.230 [ 0.000, 30.000], mean action: 2.545 [0.000, 5.000],  loss: 18.429426, mean_q: 291.656131, mean_eps: 0.811564\n",
            " 210169/1000000: episode: 313, duration: 2.423s, episode steps: 405, steps per second: 167, episode reward: 75.000, mean reward:  0.185 [ 0.000, 25.000], mean action: 2.474 [0.000, 5.000],  loss: 23.684128, mean_q: 288.401671, mean_eps: 0.811031\n",
            " 210786/1000000: episode: 314, duration: 3.700s, episode steps: 617, steps per second: 167, episode reward: 140.000, mean reward:  0.227 [ 0.000, 30.000], mean action: 2.379 [0.000, 5.000],  loss: 34.654453, mean_q: 295.132932, mean_eps: 0.810571\n",
            " 211542/1000000: episode: 315, duration: 4.515s, episode steps: 756, steps per second: 167, episode reward: 135.000, mean reward:  0.179 [ 0.000, 30.000], mean action: 2.446 [0.000, 5.000],  loss: 37.305143, mean_q: 298.192833, mean_eps: 0.809953\n",
            " 212055/1000000: episode: 316, duration: 3.061s, episode steps: 513, steps per second: 168, episode reward: 25.000, mean reward:  0.049 [ 0.000, 20.000], mean action: 2.546 [0.000, 5.000],  loss: 29.322966, mean_q: 286.618028, mean_eps: 0.809382\n",
            " 212831/1000000: episode: 317, duration: 4.641s, episode steps: 776, steps per second: 167, episode reward: 135.000, mean reward:  0.174 [ 0.000, 30.000], mean action: 2.445 [0.000, 5.000],  loss: 47.464173, mean_q: 290.781404, mean_eps: 0.808802\n",
            " 213792/1000000: episode: 318, duration: 5.698s, episode steps: 961, steps per second: 169, episode reward: 230.000, mean reward:  0.239 [ 0.000, 30.000], mean action: 2.587 [0.000, 5.000],  loss: 31.387345, mean_q: 293.161763, mean_eps: 0.808020\n",
            " 214419/1000000: episode: 319, duration: 3.697s, episode steps: 627, steps per second: 170, episode reward: 135.000, mean reward:  0.215 [ 0.000, 30.000], mean action: 2.443 [0.000, 5.000],  loss: 30.730883, mean_q: 288.452048, mean_eps: 0.807306\n",
            " 214837/1000000: episode: 320, duration: 2.547s, episode steps: 418, steps per second: 164, episode reward: 80.000, mean reward:  0.191 [ 0.000, 25.000], mean action: 2.706 [0.000, 5.000],  loss: 39.050466, mean_q: 292.935886, mean_eps: 0.806835\n",
            " 215602/1000000: episode: 321, duration: 4.590s, episode steps: 765, steps per second: 167, episode reward: 130.000, mean reward:  0.170 [ 0.000, 30.000], mean action: 2.503 [0.000, 5.000],  loss: 53.621426, mean_q: 297.645321, mean_eps: 0.806303\n",
            " 216280/1000000: episode: 322, duration: 4.098s, episode steps: 678, steps per second: 165, episode reward: 110.000, mean reward:  0.162 [ 0.000, 30.000], mean action: 2.385 [0.000, 5.000],  loss: 85.182550, mean_q: 288.617314, mean_eps: 0.805654\n",
            " 216760/1000000: episode: 323, duration: 2.881s, episode steps: 480, steps per second: 167, episode reward: 105.000, mean reward:  0.219 [ 0.000, 30.000], mean action: 2.469 [0.000, 5.000],  loss: 62.631020, mean_q: 283.750079, mean_eps: 0.805132\n",
            " 217427/1000000: episode: 324, duration: 4.035s, episode steps: 667, steps per second: 165, episode reward: 45.000, mean reward:  0.067 [ 0.000, 15.000], mean action: 2.301 [0.000, 5.000],  loss: 50.584789, mean_q: 290.979204, mean_eps: 0.804616\n",
            " 218373/1000000: episode: 325, duration: 5.686s, episode steps: 946, steps per second: 166, episode reward: 345.000, mean reward:  0.365 [ 0.000, 30.000], mean action: 2.543 [0.000, 5.000],  loss: 29.058365, mean_q: 300.275226, mean_eps: 0.803890\n",
            " 219438/1000000: episode: 326, duration: 6.318s, episode steps: 1065, steps per second: 169, episode reward: 300.000, mean reward:  0.282 [ 0.000, 30.000], mean action: 2.614 [0.000, 5.000],  loss: 42.501297, mean_q: 294.842659, mean_eps: 0.802986\n",
            " 220010/1000000: episode: 327, duration: 3.390s, episode steps: 572, steps per second: 169, episode reward: 135.000, mean reward:  0.236 [ 0.000, 30.000], mean action: 2.528 [0.000, 5.000],  loss: 57.548310, mean_q: 315.911088, mean_eps: 0.802249\n",
            " 221098/1000000: episode: 328, duration: 6.489s, episode steps: 1088, steps per second: 168, episode reward: 170.000, mean reward:  0.156 [ 0.000, 30.000], mean action: 2.529 [0.000, 5.000],  loss: 21.652463, mean_q: 303.845053, mean_eps: 0.801502\n",
            " 221718/1000000: episode: 329, duration: 3.733s, episode steps: 620, steps per second: 166, episode reward: 105.000, mean reward:  0.169 [ 0.000, 20.000], mean action: 2.650 [0.000, 5.000],  loss: 42.027993, mean_q: 312.022469, mean_eps: 0.800733\n",
            " 222888/1000000: episode: 330, duration: 6.927s, episode steps: 1170, steps per second: 169, episode reward: 355.000, mean reward:  0.303 [ 0.000, 200.000], mean action: 2.466 [0.000, 5.000],  loss: 40.176637, mean_q: 307.720933, mean_eps: 0.799928\n",
            " 223248/1000000: episode: 331, duration: 2.158s, episode steps: 360, steps per second: 167, episode reward: 45.000, mean reward:  0.125 [ 0.000, 15.000], mean action: 2.581 [0.000, 5.000],  loss: 43.821397, mean_q: 302.309220, mean_eps: 0.799239\n",
            " 223777/1000000: episode: 332, duration: 3.192s, episode steps: 529, steps per second: 166, episode reward: 30.000, mean reward:  0.057 [ 0.000, 15.000], mean action: 2.539 [0.000, 5.000],  loss: 91.150278, mean_q: 303.986558, mean_eps: 0.798839\n",
            " 224711/1000000: episode: 333, duration: 5.701s, episode steps: 934, steps per second: 164, episode reward: 140.000, mean reward:  0.150 [ 0.000, 30.000], mean action: 2.569 [0.000, 5.000],  loss: 42.645507, mean_q: 303.935275, mean_eps: 0.798181\n",
            " 225454/1000000: episode: 334, duration: 4.432s, episode steps: 743, steps per second: 168, episode reward: 110.000, mean reward:  0.148 [ 0.000, 30.000], mean action: 2.520 [0.000, 5.000],  loss: 43.561881, mean_q: 302.087740, mean_eps: 0.797426\n",
            " 226320/1000000: episode: 335, duration: 5.183s, episode steps: 866, steps per second: 167, episode reward: 485.000, mean reward:  0.560 [ 0.000, 200.000], mean action: 2.430 [0.000, 5.000],  loss: 48.205716, mean_q: 290.451374, mean_eps: 0.796702\n",
            " 226888/1000000: episode: 336, duration: 3.388s, episode steps: 568, steps per second: 168, episode reward: 135.000, mean reward:  0.238 [ 0.000, 30.000], mean action: 2.398 [0.000, 5.000],  loss: 48.481399, mean_q: 290.520164, mean_eps: 0.796057\n",
            " 227552/1000000: episode: 337, duration: 4.040s, episode steps: 664, steps per second: 164, episode reward: 110.000, mean reward:  0.166 [ 0.000, 30.000], mean action: 2.533 [0.000, 5.000],  loss: 38.876812, mean_q: 293.503221, mean_eps: 0.795502\n",
            " 228339/1000000: episode: 338, duration: 4.686s, episode steps: 787, steps per second: 168, episode reward: 210.000, mean reward:  0.267 [ 0.000, 30.000], mean action: 2.422 [0.000, 5.000],  loss: 50.834249, mean_q: 289.684313, mean_eps: 0.794849\n",
            " 229650/1000000: episode: 339, duration: 7.797s, episode steps: 1311, steps per second: 168, episode reward: 345.000, mean reward:  0.263 [ 0.000, 30.000], mean action: 2.416 [0.000, 5.000],  loss: 44.169381, mean_q: 289.600837, mean_eps: 0.793905\n",
            " 230159/1000000: episode: 340, duration: 3.049s, episode steps: 509, steps per second: 167, episode reward: 50.000, mean reward:  0.098 [ 0.000, 20.000], mean action: 2.483 [0.000, 5.000],  loss: 60.859931, mean_q: 292.854781, mean_eps: 0.793086\n",
            " 230982/1000000: episode: 341, duration: 4.946s, episode steps: 823, steps per second: 166, episode reward: 215.000, mean reward:  0.261 [ 0.000, 30.000], mean action: 2.564 [0.000, 5.000],  loss: 57.107013, mean_q: 297.563888, mean_eps: 0.792487\n",
            " 231623/1000000: episode: 342, duration: 3.828s, episode steps: 641, steps per second: 167, episode reward: 120.000, mean reward:  0.187 [ 0.000, 30.000], mean action: 2.585 [0.000, 5.000],  loss: 38.315132, mean_q: 290.539768, mean_eps: 0.791828\n",
            " 232115/1000000: episode: 343, duration: 2.924s, episode steps: 492, steps per second: 168, episode reward: 15.000, mean reward:  0.030 [ 0.000, 10.000], mean action: 2.514 [0.000, 5.000],  loss: 45.426391, mean_q: 289.351122, mean_eps: 0.791318\n",
            " 232779/1000000: episode: 344, duration: 4.014s, episode steps: 664, steps per second: 165, episode reward: 155.000, mean reward:  0.233 [ 0.000, 30.000], mean action: 2.684 [0.000, 5.000],  loss: 56.830667, mean_q: 295.808070, mean_eps: 0.790798\n",
            " 233690/1000000: episode: 345, duration: 5.428s, episode steps: 911, steps per second: 168, episode reward: 145.000, mean reward:  0.159 [ 0.000, 30.000], mean action: 2.542 [0.000, 5.000],  loss: 34.153535, mean_q: 299.976411, mean_eps: 0.790089\n",
            " 234127/1000000: episode: 346, duration: 2.685s, episode steps: 437, steps per second: 163, episode reward: 105.000, mean reward:  0.240 [ 0.000, 25.000], mean action: 2.435 [0.000, 5.000],  loss: 42.697901, mean_q: 300.868391, mean_eps: 0.789483\n",
            " 234726/1000000: episode: 347, duration: 3.615s, episode steps: 599, steps per second: 166, episode reward: 25.000, mean reward:  0.042 [ 0.000, 10.000], mean action: 2.511 [0.000, 5.000],  loss: 58.546922, mean_q: 295.893879, mean_eps: 0.789017\n",
            " 235541/1000000: episode: 348, duration: 4.861s, episode steps: 815, steps per second: 168, episode reward: 170.000, mean reward:  0.209 [ 0.000, 30.000], mean action: 2.494 [0.000, 5.000],  loss: 37.646733, mean_q: 296.603172, mean_eps: 0.788380\n",
            " 236502/1000000: episode: 349, duration: 5.819s, episode steps: 961, steps per second: 165, episode reward: 270.000, mean reward:  0.281 [ 0.000, 30.000], mean action: 2.545 [0.000, 5.000],  loss: 24.795171, mean_q: 281.906591, mean_eps: 0.787581\n",
            " 237235/1000000: episode: 350, duration: 4.468s, episode steps: 733, steps per second: 164, episode reward: 140.000, mean reward:  0.191 [ 0.000, 30.000], mean action: 2.409 [0.000, 5.000],  loss: 36.362224, mean_q: 283.287572, mean_eps: 0.786819\n",
            " 237918/1000000: episode: 351, duration: 4.092s, episode steps: 683, steps per second: 167, episode reward: 125.000, mean reward:  0.183 [ 0.000, 25.000], mean action: 2.613 [0.000, 5.000],  loss: 25.849906, mean_q: 295.505501, mean_eps: 0.786182\n",
            " 238620/1000000: episode: 352, duration: 4.224s, episode steps: 702, steps per second: 166, episode reward: 120.000, mean reward:  0.171 [ 0.000, 25.000], mean action: 2.501 [0.000, 5.000],  loss: 32.727246, mean_q: 301.295063, mean_eps: 0.785558\n",
            " 239060/1000000: episode: 353, duration: 2.633s, episode steps: 440, steps per second: 167, episode reward: 65.000, mean reward:  0.148 [ 0.000, 20.000], mean action: 2.570 [0.000, 5.000],  loss: 51.565566, mean_q: 304.045656, mean_eps: 0.785044\n",
            " 239793/1000000: episode: 354, duration: 4.359s, episode steps: 733, steps per second: 168, episode reward: 170.000, mean reward:  0.232 [ 0.000, 30.000], mean action: 2.452 [0.000, 5.000],  loss: 51.311995, mean_q: 301.356658, mean_eps: 0.784517\n",
            " 240746/1000000: episode: 355, duration: 5.617s, episode steps: 953, steps per second: 170, episode reward: 360.000, mean reward:  0.378 [ 0.000, 200.000], mean action: 2.475 [0.000, 5.000],  loss: 58.667605, mean_q: 309.018096, mean_eps: 0.783758\n",
            " 241594/1000000: episode: 356, duration: 5.079s, episode steps: 848, steps per second: 167, episode reward: 165.000, mean reward:  0.195 [ 0.000, 30.000], mean action: 2.612 [0.000, 5.000],  loss: 43.457657, mean_q: 301.778384, mean_eps: 0.782947\n",
            " 242187/1000000: episode: 357, duration: 3.555s, episode steps: 593, steps per second: 167, episode reward: 80.000, mean reward:  0.135 [ 0.000, 20.000], mean action: 2.428 [0.000, 5.000],  loss: 46.879708, mean_q: 308.268896, mean_eps: 0.782299\n",
            " 242841/1000000: episode: 358, duration: 3.943s, episode steps: 654, steps per second: 166, episode reward: 120.000, mean reward:  0.183 [ 0.000, 30.000], mean action: 2.453 [0.000, 5.000],  loss: 44.571789, mean_q: 302.896771, mean_eps: 0.781738\n",
            " 243561/1000000: episode: 359, duration: 4.314s, episode steps: 720, steps per second: 167, episode reward: 80.000, mean reward:  0.111 [ 0.000, 20.000], mean action: 2.308 [0.000, 5.000],  loss: 24.076060, mean_q: 297.550438, mean_eps: 0.781120\n",
            " 244713/1000000: episode: 360, duration: 6.812s, episode steps: 1152, steps per second: 169, episode reward: 350.000, mean reward:  0.304 [ 0.000, 200.000], mean action: 2.464 [0.000, 5.000],  loss: 29.609862, mean_q: 300.449496, mean_eps: 0.780277\n",
            " 245626/1000000: episode: 361, duration: 5.425s, episode steps: 913, steps per second: 168, episode reward: 145.000, mean reward:  0.159 [ 0.000, 20.000], mean action: 2.745 [0.000, 5.000],  loss: 42.234641, mean_q: 313.187003, mean_eps: 0.779348\n",
            " 246271/1000000: episode: 362, duration: 3.817s, episode steps: 645, steps per second: 169, episode reward: 170.000, mean reward:  0.264 [ 0.000, 25.000], mean action: 2.420 [0.000, 5.000],  loss: 34.025183, mean_q: 312.228972, mean_eps: 0.778647\n",
            " 247058/1000000: episode: 363, duration: 4.668s, episode steps: 787, steps per second: 169, episode reward: 150.000, mean reward:  0.191 [ 0.000, 30.000], mean action: 2.569 [0.000, 5.000],  loss: 72.305015, mean_q: 308.104820, mean_eps: 0.778002\n",
            " 247746/1000000: episode: 364, duration: 4.135s, episode steps: 688, steps per second: 166, episode reward: 135.000, mean reward:  0.196 [ 0.000, 30.000], mean action: 2.369 [0.000, 5.000],  loss: 44.962191, mean_q: 291.250207, mean_eps: 0.777339\n",
            " 248148/1000000: episode: 365, duration: 2.409s, episode steps: 402, steps per second: 167, episode reward: 75.000, mean reward:  0.187 [ 0.000, 25.000], mean action: 2.413 [0.000, 5.000],  loss: 47.017621, mean_q: 293.373685, mean_eps: 0.776848\n",
            " 248849/1000000: episode: 366, duration: 4.142s, episode steps: 701, steps per second: 169, episode reward: 110.000, mean reward:  0.157 [ 0.000, 30.000], mean action: 2.459 [0.000, 5.000],  loss: 77.184904, mean_q: 297.432248, mean_eps: 0.776352\n",
            " 249896/1000000: episode: 367, duration: 6.164s, episode steps: 1047, steps per second: 170, episode reward: 260.000, mean reward:  0.248 [ 0.000, 30.000], mean action: 2.468 [0.000, 5.000],  loss: 30.079354, mean_q: 307.019117, mean_eps: 0.775565\n",
            " 250543/1000000: episode: 368, duration: 3.890s, episode steps: 647, steps per second: 166, episode reward: 135.000, mean reward:  0.209 [ 0.000, 30.000], mean action: 2.382 [0.000, 5.000],  loss: 51.350265, mean_q: 329.242459, mean_eps: 0.774803\n",
            " 251298/1000000: episode: 369, duration: 4.496s, episode steps: 755, steps per second: 168, episode reward: 120.000, mean reward:  0.159 [ 0.000, 30.000], mean action: 2.506 [0.000, 5.000],  loss: 36.376469, mean_q: 299.815899, mean_eps: 0.774172\n",
            " 252467/1000000: episode: 370, duration: 6.891s, episode steps: 1169, steps per second: 170, episode reward: 295.000, mean reward:  0.252 [ 0.000, 30.000], mean action: 2.454 [0.000, 5.000],  loss: 29.559162, mean_q: 291.749129, mean_eps: 0.773306\n",
            " 253135/1000000: episode: 371, duration: 4.001s, episode steps: 668, steps per second: 167, episode reward: 90.000, mean reward:  0.135 [ 0.000, 25.000], mean action: 2.494 [0.000, 5.000],  loss: 53.309606, mean_q: 277.998192, mean_eps: 0.772480\n",
            " 253643/1000000: episode: 372, duration: 3.055s, episode steps: 508, steps per second: 166, episode reward: 45.000, mean reward:  0.089 [ 0.000, 15.000], mean action: 2.360 [0.000, 5.000],  loss: 37.726185, mean_q: 290.976944, mean_eps: 0.771950\n",
            " 254443/1000000: episode: 373, duration: 4.724s, episode steps: 800, steps per second: 169, episode reward: 165.000, mean reward:  0.206 [ 0.000, 30.000], mean action: 2.421 [0.000, 5.000],  loss: 39.243411, mean_q: 299.622682, mean_eps: 0.771362\n",
            " 255143/1000000: episode: 374, duration: 4.222s, episode steps: 700, steps per second: 166, episode reward: 130.000, mean reward:  0.186 [ 0.000, 30.000], mean action: 2.429 [0.000, 5.000],  loss: 30.473171, mean_q: 299.728732, mean_eps: 0.770687\n",
            " 255645/1000000: episode: 375, duration: 3.036s, episode steps: 502, steps per second: 165, episode reward: 130.000, mean reward:  0.259 [ 0.000, 25.000], mean action: 2.460 [0.000, 5.000],  loss: 35.502080, mean_q: 306.992178, mean_eps: 0.770146\n",
            " 256184/1000000: episode: 376, duration: 3.231s, episode steps: 539, steps per second: 167, episode reward: 105.000, mean reward:  0.195 [ 0.000, 30.000], mean action: 2.727 [0.000, 5.000],  loss: 38.039978, mean_q: 305.838598, mean_eps: 0.769677\n",
            " 256547/1000000: episode: 377, duration: 2.174s, episode steps: 363, steps per second: 167, episode reward: 70.000, mean reward:  0.193 [ 0.000, 20.000], mean action: 2.287 [0.000, 5.000],  loss: 28.645772, mean_q: 298.444201, mean_eps: 0.769272\n",
            " 257436/1000000: episode: 378, duration: 5.288s, episode steps: 889, steps per second: 168, episode reward: 175.000, mean reward:  0.197 [ 0.000, 30.000], mean action: 2.533 [0.000, 5.000],  loss: 46.461596, mean_q: 294.960336, mean_eps: 0.768708\n",
            " 258028/1000000: episode: 379, duration: 3.584s, episode steps: 592, steps per second: 165, episode reward: 125.000, mean reward:  0.211 [ 0.000, 30.000], mean action: 2.530 [0.000, 5.000],  loss: 51.237154, mean_q: 301.164694, mean_eps: 0.768042\n",
            " 258491/1000000: episode: 380, duration: 2.793s, episode steps: 463, steps per second: 166, episode reward: 120.000, mean reward:  0.259 [ 0.000, 30.000], mean action: 2.674 [0.000, 5.000],  loss: 49.542997, mean_q: 310.093554, mean_eps: 0.767567\n",
            " 259131/1000000: episode: 381, duration: 3.841s, episode steps: 640, steps per second: 167, episode reward: 110.000, mean reward:  0.172 [ 0.000, 30.000], mean action: 2.620 [0.000, 5.000],  loss: 51.858166, mean_q: 302.886873, mean_eps: 0.767071\n",
            " 259712/1000000: episode: 382, duration: 3.511s, episode steps: 581, steps per second: 165, episode reward: 90.000, mean reward:  0.155 [ 0.000, 25.000], mean action: 2.627 [0.000, 5.000],  loss: 42.484842, mean_q: 291.104685, mean_eps: 0.766521\n",
            " 260099/1000000: episode: 383, duration: 2.329s, episode steps: 387, steps per second: 166, episode reward: 50.000, mean reward:  0.129 [ 0.000, 20.000], mean action: 2.475 [0.000, 5.000],  loss: 37.139459, mean_q: 290.798595, mean_eps: 0.766085\n",
            " 260662/1000000: episode: 384, duration: 3.389s, episode steps: 563, steps per second: 166, episode reward: 105.000, mean reward:  0.187 [ 0.000, 30.000], mean action: 2.300 [0.000, 5.000],  loss: 47.088722, mean_q: 298.437873, mean_eps: 0.765658\n",
            " 261420/1000000: episode: 385, duration: 4.524s, episode steps: 758, steps per second: 168, episode reward: 160.000, mean reward:  0.211 [ 0.000, 30.000], mean action: 2.561 [0.000, 5.000],  loss: 33.442305, mean_q: 301.899090, mean_eps: 0.765064\n",
            " 262114/1000000: episode: 386, duration: 4.155s, episode steps: 694, steps per second: 167, episode reward: 105.000, mean reward:  0.151 [ 0.000, 30.000], mean action: 2.350 [0.000, 5.000],  loss: 59.206033, mean_q: 305.726854, mean_eps: 0.764410\n",
            " 263079/1000000: episode: 387, duration: 5.773s, episode steps: 965, steps per second: 167, episode reward: 240.000, mean reward:  0.249 [ 0.000, 30.000], mean action: 2.473 [0.000, 5.000],  loss: 40.102138, mean_q: 292.642677, mean_eps: 0.763664\n",
            " 263735/1000000: episode: 388, duration: 3.930s, episode steps: 656, steps per second: 167, episode reward: 45.000, mean reward:  0.069 [ 0.000, 15.000], mean action: 2.197 [0.000, 5.000],  loss: 36.637518, mean_q: 297.575103, mean_eps: 0.762934\n",
            " 264943/1000000: episode: 389, duration: 7.202s, episode steps: 1208, steps per second: 168, episode reward: 270.000, mean reward:  0.224 [ 0.000, 30.000], mean action: 2.514 [0.000, 5.000],  loss: 50.114422, mean_q: 296.482333, mean_eps: 0.762095\n",
            " 265544/1000000: episode: 390, duration: 3.615s, episode steps: 601, steps per second: 166, episode reward: 120.000, mean reward:  0.200 [ 0.000, 30.000], mean action: 2.549 [0.000, 5.000],  loss: 74.072204, mean_q: 282.621953, mean_eps: 0.761281\n",
            " 266126/1000000: episode: 391, duration: 3.482s, episode steps: 582, steps per second: 167, episode reward: 120.000, mean reward:  0.206 [ 0.000, 30.000], mean action: 2.514 [0.000, 5.000],  loss: 26.119018, mean_q: 293.839106, mean_eps: 0.760749\n",
            " 267113/1000000: episode: 392, duration: 5.831s, episode steps: 987, steps per second: 169, episode reward: 280.000, mean reward:  0.284 [ 0.000, 30.000], mean action: 2.306 [0.000, 5.000],  loss: 28.577434, mean_q: 297.576193, mean_eps: 0.760043\n",
            " 267496/1000000: episode: 393, duration: 2.294s, episode steps: 383, steps per second: 167, episode reward: 80.000, mean reward:  0.209 [ 0.000, 20.000], mean action: 2.436 [0.000, 5.000],  loss: 44.179659, mean_q: 322.829938, mean_eps: 0.759426\n",
            " 268317/1000000: episode: 394, duration: 4.930s, episode steps: 821, steps per second: 167, episode reward: 355.000, mean reward:  0.432 [ 0.000, 200.000], mean action: 2.622 [0.000, 5.000],  loss: 65.290672, mean_q: 305.297904, mean_eps: 0.758885\n",
            " 268927/1000000: episode: 395, duration: 3.651s, episode steps: 610, steps per second: 167, episode reward: 80.000, mean reward:  0.131 [ 0.000, 25.000], mean action: 2.285 [0.000, 5.000],  loss: 37.438168, mean_q: 290.148229, mean_eps: 0.758241\n",
            " 269368/1000000: episode: 396, duration: 2.635s, episode steps: 441, steps per second: 167, episode reward: 30.000, mean reward:  0.068 [ 0.000, 15.000], mean action: 2.451 [0.000, 5.000],  loss: 34.870475, mean_q: 294.182586, mean_eps: 0.757768\n",
            " 270122/1000000: episode: 397, duration: 4.497s, episode steps: 754, steps per second: 168, episode reward: 95.000, mean reward:  0.126 [ 0.000, 20.000], mean action: 2.706 [0.000, 5.000],  loss: 34.766052, mean_q: 304.567551, mean_eps: 0.757230\n",
            " 270809/1000000: episode: 398, duration: 4.134s, episode steps: 687, steps per second: 166, episode reward: 110.000, mean reward:  0.160 [ 0.000, 30.000], mean action: 2.381 [0.000, 5.000],  loss: 35.253987, mean_q: 315.083948, mean_eps: 0.756582\n",
            " 271509/1000000: episode: 399, duration: 4.151s, episode steps: 700, steps per second: 169, episode reward: 150.000, mean reward:  0.214 [ 0.000, 25.000], mean action: 2.324 [0.000, 5.000],  loss: 44.971954, mean_q: 298.564142, mean_eps: 0.755957\n",
            " 272132/1000000: episode: 400, duration: 3.738s, episode steps: 623, steps per second: 167, episode reward: 80.000, mean reward:  0.128 [ 0.000, 20.000], mean action: 2.398 [0.000, 5.000],  loss: 38.997615, mean_q: 300.585605, mean_eps: 0.755362\n",
            " 272704/1000000: episode: 401, duration: 3.420s, episode steps: 572, steps per second: 167, episode reward: 45.000, mean reward:  0.079 [ 0.000, 15.000], mean action: 2.437 [0.000, 5.000],  loss: 46.409564, mean_q: 302.713999, mean_eps: 0.754824\n",
            " 273405/1000000: episode: 402, duration: 4.187s, episode steps: 701, steps per second: 167, episode reward: 160.000, mean reward:  0.228 [ 0.000, 30.000], mean action: 2.629 [0.000, 5.000],  loss: 46.485527, mean_q: 305.144101, mean_eps: 0.754251\n",
            " 273936/1000000: episode: 403, duration: 3.203s, episode steps: 531, steps per second: 166, episode reward: 120.000, mean reward:  0.226 [ 0.000, 30.000], mean action: 2.661 [0.000, 5.000],  loss: 27.879573, mean_q: 316.883210, mean_eps: 0.753697\n",
            " 274576/1000000: episode: 404, duration: 3.844s, episode steps: 640, steps per second: 166, episode reward: 80.000, mean reward:  0.125 [ 0.000, 25.000], mean action: 2.439 [0.000, 5.000],  loss: 29.838939, mean_q: 303.052587, mean_eps: 0.753170\n",
            " 275365/1000000: episode: 405, duration: 4.779s, episode steps: 789, steps per second: 165, episode reward: 210.000, mean reward:  0.266 [ 0.000, 30.000], mean action: 2.471 [0.000, 5.000],  loss: 31.781789, mean_q: 297.887597, mean_eps: 0.752527\n",
            " 275758/1000000: episode: 406, duration: 2.335s, episode steps: 393, steps per second: 168, episode reward: 75.000, mean reward:  0.191 [ 0.000, 25.000], mean action: 2.486 [0.000, 5.000],  loss: 34.420178, mean_q: 293.440751, mean_eps: 0.751995\n",
            " 276257/1000000: episode: 407, duration: 2.991s, episode steps: 499, steps per second: 167, episode reward: 25.000, mean reward:  0.050 [ 0.000, 10.000], mean action: 2.255 [0.000, 5.000],  loss: 54.498329, mean_q: 301.026000, mean_eps: 0.751594\n",
            " 277459/1000000: episode: 408, duration: 7.076s, episode steps: 1202, steps per second: 170, episode reward: 275.000, mean reward:  0.229 [ 0.000, 30.000], mean action: 2.542 [0.000, 5.000],  loss: 47.325230, mean_q: 305.002259, mean_eps: 0.750828\n",
            " 278088/1000000: episode: 409, duration: 3.779s, episode steps: 629, steps per second: 166, episode reward: 180.000, mean reward:  0.286 [ 0.000, 30.000], mean action: 2.331 [0.000, 5.000],  loss: 27.229248, mean_q: 302.641882, mean_eps: 0.750004\n",
            " 278684/1000000: episode: 410, duration: 3.629s, episode steps: 596, steps per second: 164, episode reward: 75.000, mean reward:  0.126 [ 0.000, 25.000], mean action: 2.544 [0.000, 5.000],  loss: 36.060746, mean_q: 305.163559, mean_eps: 0.749453\n",
            " 279688/1000000: episode: 411, duration: 5.935s, episode steps: 1004, steps per second: 169, episode reward: 245.000, mean reward:  0.244 [ 0.000, 30.000], mean action: 2.763 [0.000, 5.000],  loss: 33.918228, mean_q: 310.592801, mean_eps: 0.748733\n",
            " 280343/1000000: episode: 412, duration: 3.895s, episode steps: 655, steps per second: 168, episode reward: 210.000, mean reward:  0.321 [ 0.000, 30.000], mean action: 2.434 [0.000, 5.000],  loss: 25.355301, mean_q: 319.157430, mean_eps: 0.747987\n",
            " 280896/1000000: episode: 413, duration: 3.267s, episode steps: 553, steps per second: 169, episode reward: 110.000, mean reward:  0.199 [ 0.000, 30.000], mean action: 2.322 [0.000, 5.000],  loss: 20.388983, mean_q: 301.949514, mean_eps: 0.747443\n",
            " 281485/1000000: episode: 414, duration: 3.484s, episode steps: 589, steps per second: 169, episode reward: 110.000, mean reward:  0.187 [ 0.000, 30.000], mean action: 2.447 [0.000, 5.000],  loss: 35.376033, mean_q: 297.456067, mean_eps: 0.746929\n",
            " 282072/1000000: episode: 415, duration: 3.481s, episode steps: 587, steps per second: 169, episode reward: 135.000, mean reward:  0.230 [ 0.000, 30.000], mean action: 2.482 [0.000, 5.000],  loss: 47.486072, mean_q: 297.999373, mean_eps: 0.746400\n",
            " 282777/1000000: episode: 416, duration: 4.189s, episode steps: 705, steps per second: 168, episode reward: 110.000, mean reward:  0.156 [ 0.000, 30.000], mean action: 2.458 [0.000, 5.000],  loss: 48.224152, mean_q: 306.389462, mean_eps: 0.745818\n",
            " 283586/1000000: episode: 417, duration: 4.807s, episode steps: 809, steps per second: 168, episode reward: 130.000, mean reward:  0.161 [ 0.000, 25.000], mean action: 2.466 [0.000, 5.000],  loss: 42.907213, mean_q: 300.694049, mean_eps: 0.745137\n",
            " 284277/1000000: episode: 418, duration: 4.123s, episode steps: 691, steps per second: 168, episode reward: 125.000, mean reward:  0.181 [ 0.000, 25.000], mean action: 2.527 [0.000, 5.000],  loss: 25.837346, mean_q: 303.561774, mean_eps: 0.744462\n",
            " 284661/1000000: episode: 419, duration: 2.289s, episode steps: 384, steps per second: 168, episode reward: 75.000, mean reward:  0.195 [ 0.000, 25.000], mean action: 2.469 [0.000, 5.000],  loss: 32.401872, mean_q: 302.703405, mean_eps: 0.743978\n",
            " 285076/1000000: episode: 420, duration: 2.458s, episode steps: 415, steps per second: 169, episode reward: 60.000, mean reward:  0.145 [ 0.000, 15.000], mean action: 2.487 [0.000, 5.000],  loss: 49.628692, mean_q: 301.021441, mean_eps: 0.743619\n",
            " 285793/1000000: episode: 421, duration: 4.245s, episode steps: 717, steps per second: 169, episode reward: 135.000, mean reward:  0.188 [ 0.000, 30.000], mean action: 2.608 [0.000, 5.000],  loss: 50.356949, mean_q: 303.845510, mean_eps: 0.743109\n",
            " 286737/1000000: episode: 422, duration: 5.642s, episode steps: 944, steps per second: 167, episode reward: 230.000, mean reward:  0.244 [ 0.000, 30.000], mean action: 2.541 [0.000, 5.000],  loss: 35.626877, mean_q: 305.095566, mean_eps: 0.742362\n",
            " 287837/1000000: episode: 423, duration: 6.598s, episode steps: 1100, steps per second: 167, episode reward: 225.000, mean reward:  0.205 [ 0.000, 30.000], mean action: 2.470 [0.000, 5.000],  loss: 23.329000, mean_q: 326.574180, mean_eps: 0.741442\n",
            " 288377/1000000: episode: 424, duration: 3.261s, episode steps: 540, steps per second: 166, episode reward: 35.000, mean reward:  0.065 [ 0.000, 15.000], mean action: 2.696 [0.000, 5.000],  loss: 37.392885, mean_q: 331.458604, mean_eps: 0.740704\n",
            " 288884/1000000: episode: 425, duration: 3.133s, episode steps: 507, steps per second: 162, episode reward: 80.000, mean reward:  0.158 [ 0.000, 25.000], mean action: 2.505 [0.000, 5.000],  loss: 63.619712, mean_q: 317.659366, mean_eps: 0.740233\n",
            " 289591/1000000: episode: 426, duration: 4.233s, episode steps: 707, steps per second: 167, episode reward: 160.000, mean reward:  0.226 [ 0.000, 30.000], mean action: 2.495 [0.000, 5.000],  loss: 47.718080, mean_q: 308.717731, mean_eps: 0.739687\n",
            " 290377/1000000: episode: 427, duration: 4.655s, episode steps: 786, steps per second: 169, episode reward: 135.000, mean reward:  0.172 [ 0.000, 30.000], mean action: 2.751 [0.000, 5.000],  loss: 31.004763, mean_q: 299.159713, mean_eps: 0.739015\n",
            " 290979/1000000: episode: 428, duration: 3.588s, episode steps: 602, steps per second: 168, episode reward: 80.000, mean reward:  0.133 [ 0.000, 30.000], mean action: 2.553 [0.000, 5.000],  loss: 27.980676, mean_q: 288.695094, mean_eps: 0.738390\n",
            " 291757/1000000: episode: 429, duration: 4.626s, episode steps: 778, steps per second: 168, episode reward: 160.000, mean reward:  0.206 [ 0.000, 25.000], mean action: 2.386 [0.000, 5.000],  loss: 43.051410, mean_q: 300.528457, mean_eps: 0.737769\n",
            " 292414/1000000: episode: 430, duration: 3.912s, episode steps: 657, steps per second: 168, episode reward: 155.000, mean reward:  0.236 [ 0.000, 30.000], mean action: 2.565 [0.000, 5.000],  loss: 26.823176, mean_q: 316.229973, mean_eps: 0.737124\n",
            " 293417/1000000: episode: 431, duration: 5.979s, episode steps: 1003, steps per second: 168, episode reward: 155.000, mean reward:  0.155 [ 0.000, 30.000], mean action: 2.541 [0.000, 5.000],  loss: 32.681816, mean_q: 301.512841, mean_eps: 0.736376\n",
            " 294168/1000000: episode: 432, duration: 4.472s, episode steps: 751, steps per second: 168, episode reward: 170.000, mean reward:  0.226 [ 0.000, 30.000], mean action: 2.547 [0.000, 5.000],  loss: 21.203712, mean_q: 297.542485, mean_eps: 0.735587\n",
            " 294680/1000000: episode: 433, duration: 3.072s, episode steps: 512, steps per second: 167, episode reward: 75.000, mean reward:  0.146 [ 0.000, 25.000], mean action: 2.557 [0.000, 5.000],  loss: 24.136903, mean_q: 296.369269, mean_eps: 0.735019\n",
            " 296160/1000000: episode: 434, duration: 8.807s, episode steps: 1480, steps per second: 168, episode reward: 415.000, mean reward:  0.280 [ 0.000, 200.000], mean action: 2.654 [0.000, 5.000],  loss: 41.648814, mean_q: 303.494421, mean_eps: 0.734122\n",
            " 296775/1000000: episode: 435, duration: 3.666s, episode steps: 615, steps per second: 168, episode reward: 140.000, mean reward:  0.228 [ 0.000, 30.000], mean action: 2.467 [0.000, 5.000],  loss: 44.571937, mean_q: 291.302736, mean_eps: 0.733180\n",
            " 297176/1000000: episode: 436, duration: 2.437s, episode steps: 401, steps per second: 165, episode reward: 20.000, mean reward:  0.050 [ 0.000, 10.000], mean action: 2.409 [0.000, 5.000],  loss: 38.541522, mean_q: 295.468969, mean_eps: 0.732722\n",
            " 297877/1000000: episode: 437, duration: 4.279s, episode steps: 701, steps per second: 164, episode reward: 115.000, mean reward:  0.164 [ 0.000, 20.000], mean action: 2.247 [0.000, 5.000],  loss: 44.312106, mean_q: 308.900149, mean_eps: 0.732227\n",
            " 298664/1000000: episode: 438, duration: 4.694s, episode steps: 787, steps per second: 168, episode reward: 230.000, mean reward:  0.292 [ 0.000, 30.000], mean action: 2.370 [0.000, 5.000],  loss: 49.863115, mean_q: 316.054118, mean_eps: 0.731557\n",
            " 299354/1000000: episode: 439, duration: 4.134s, episode steps: 690, steps per second: 167, episode reward: 165.000, mean reward:  0.239 [ 0.000, 30.000], mean action: 2.487 [0.000, 5.000],  loss: 27.815495, mean_q: 303.802308, mean_eps: 0.730892\n",
            " 299982/1000000: episode: 440, duration: 3.725s, episode steps: 628, steps per second: 169, episode reward: 45.000, mean reward:  0.072 [ 0.000, 15.000], mean action: 2.481 [0.000, 5.000],  loss: 32.458950, mean_q: 302.759722, mean_eps: 0.730299\n",
            " 301104/1000000: episode: 441, duration: 6.693s, episode steps: 1122, steps per second: 168, episode reward: 300.000, mean reward:  0.267 [ 0.000, 30.000], mean action: 2.365 [0.000, 5.000],  loss: 30.799265, mean_q: 302.873726, mean_eps: 0.729512\n",
            " 301874/1000000: episode: 442, duration: 4.578s, episode steps: 770, steps per second: 168, episode reward: 180.000, mean reward:  0.234 [ 0.000, 30.000], mean action: 2.365 [0.000, 5.000],  loss: 58.782369, mean_q: 317.380884, mean_eps: 0.728660\n",
            " 302868/1000000: episode: 443, duration: 5.873s, episode steps: 994, steps per second: 169, episode reward: 255.000, mean reward:  0.257 [ 0.000, 30.000], mean action: 2.485 [0.000, 5.000],  loss: 59.179890, mean_q: 288.981582, mean_eps: 0.727867\n",
            " 303538/1000000: episode: 444, duration: 4.015s, episode steps: 670, steps per second: 167, episode reward: 110.000, mean reward:  0.164 [ 0.000, 30.000], mean action: 2.604 [0.000, 5.000],  loss: 41.091987, mean_q: 289.368015, mean_eps: 0.727118\n",
            " 304034/1000000: episode: 445, duration: 2.979s, episode steps: 496, steps per second: 166, episode reward: 55.000, mean reward:  0.111 [ 0.000, 20.000], mean action: 2.607 [0.000, 5.000],  loss: 39.538002, mean_q: 290.975605, mean_eps: 0.726593\n",
            " 304573/1000000: episode: 446, duration: 3.222s, episode steps: 539, steps per second: 167, episode reward: 60.000, mean reward:  0.111 [ 0.000, 25.000], mean action: 2.436 [0.000, 5.000],  loss: 44.952823, mean_q: 296.459338, mean_eps: 0.726127\n",
            " 305252/1000000: episode: 447, duration: 4.056s, episode steps: 679, steps per second: 167, episode reward: 190.000, mean reward:  0.280 [ 0.000, 30.000], mean action: 2.564 [0.000, 5.000],  loss: 78.682228, mean_q: 313.650319, mean_eps: 0.725579\n",
            " 305840/1000000: episode: 448, duration: 3.522s, episode steps: 588, steps per second: 167, episode reward: 120.000, mean reward:  0.204 [ 0.000, 30.000], mean action: 2.493 [0.000, 5.000],  loss: 53.721756, mean_q: 316.645010, mean_eps: 0.725009\n",
            " 306734/1000000: episode: 449, duration: 5.301s, episode steps: 894, steps per second: 169, episode reward: 215.000, mean reward:  0.240 [ 0.000, 30.000], mean action: 2.512 [0.000, 5.000],  loss: 27.596168, mean_q: 297.642524, mean_eps: 0.724342\n",
            " 307390/1000000: episode: 450, duration: 3.947s, episode steps: 656, steps per second: 166, episode reward: 110.000, mean reward:  0.168 [ 0.000, 25.000], mean action: 2.312 [0.000, 5.000],  loss: 31.314766, mean_q: 286.060059, mean_eps: 0.723645\n",
            " 307801/1000000: episode: 451, duration: 2.447s, episode steps: 411, steps per second: 168, episode reward: 80.000, mean reward:  0.195 [ 0.000, 30.000], mean action: 2.530 [0.000, 5.000],  loss: 52.622402, mean_q: 292.434657, mean_eps: 0.723164\n",
            " 308547/1000000: episode: 452, duration: 4.477s, episode steps: 746, steps per second: 167, episode reward: 185.000, mean reward:  0.248 [ 0.000, 30.000], mean action: 2.720 [0.000, 5.000],  loss: 78.169410, mean_q: 299.648286, mean_eps: 0.722644\n",
            " 309242/1000000: episode: 453, duration: 4.168s, episode steps: 695, steps per second: 167, episode reward: 180.000, mean reward:  0.259 [ 0.000, 30.000], mean action: 2.473 [0.000, 5.000],  loss: 46.032675, mean_q: 294.098516, mean_eps: 0.721995\n",
            " 309744/1000000: episode: 454, duration: 3.004s, episode steps: 502, steps per second: 167, episode reward: 65.000, mean reward:  0.129 [ 0.000, 20.000], mean action: 2.502 [0.000, 5.000],  loss: 39.317087, mean_q: 293.775652, mean_eps: 0.721457\n",
            " 310739/1000000: episode: 455, duration: 5.872s, episode steps: 995, steps per second: 169, episode reward: 480.000, mean reward:  0.482 [ 0.000, 200.000], mean action: 2.608 [0.000, 5.000],  loss: 36.412099, mean_q: 303.293464, mean_eps: 0.720783\n",
            " 311268/1000000: episode: 456, duration: 3.158s, episode steps: 529, steps per second: 168, episode reward: 95.000, mean reward:  0.180 [ 0.000, 30.000], mean action: 2.439 [0.000, 5.000],  loss: 44.024186, mean_q: 312.615761, mean_eps: 0.720097\n",
            " 312342/1000000: episode: 457, duration: 6.373s, episode steps: 1074, steps per second: 169, episode reward: 280.000, mean reward:  0.261 [ 0.000, 30.000], mean action: 2.549 [0.000, 5.000],  loss: 52.082492, mean_q: 318.684638, mean_eps: 0.719376\n",
            " 312994/1000000: episode: 458, duration: 3.907s, episode steps: 652, steps per second: 167, episode reward: 185.000, mean reward:  0.284 [ 0.000, 30.000], mean action: 2.505 [0.000, 5.000],  loss: 34.357160, mean_q: 328.071639, mean_eps: 0.718599\n",
            " 313650/1000000: episode: 459, duration: 3.929s, episode steps: 656, steps per second: 167, episode reward: 120.000, mean reward:  0.183 [ 0.000, 30.000], mean action: 2.657 [0.000, 5.000],  loss: 50.017653, mean_q: 297.580011, mean_eps: 0.718011\n",
            " 314436/1000000: episode: 460, duration: 4.677s, episode steps: 786, steps per second: 168, episode reward: 210.000, mean reward:  0.267 [ 0.000, 30.000], mean action: 2.501 [0.000, 5.000],  loss: 39.151687, mean_q: 297.669339, mean_eps: 0.717362\n",
            " 315075/1000000: episode: 461, duration: 3.819s, episode steps: 639, steps per second: 167, episode reward: 105.000, mean reward:  0.164 [ 0.000, 30.000], mean action: 2.509 [0.000, 5.000],  loss: 59.069605, mean_q: 316.865629, mean_eps: 0.716721\n",
            " 315739/1000000: episode: 462, duration: 3.923s, episode steps: 664, steps per second: 169, episode reward: 90.000, mean reward:  0.136 [ 0.000, 25.000], mean action: 2.515 [0.000, 5.000],  loss: 25.143034, mean_q: 306.081806, mean_eps: 0.716134\n",
            " 316467/1000000: episode: 463, duration: 4.348s, episode steps: 728, steps per second: 167, episode reward: 105.000, mean reward:  0.144 [ 0.000, 30.000], mean action: 2.598 [0.000, 5.000],  loss: 26.883419, mean_q: 298.513736, mean_eps: 0.715508\n",
            " 317180/1000000: episode: 464, duration: 4.202s, episode steps: 713, steps per second: 170, episode reward: 120.000, mean reward:  0.168 [ 0.000, 30.000], mean action: 2.597 [0.000, 5.000],  loss: 31.893903, mean_q: 294.833382, mean_eps: 0.714859\n",
            " 317796/1000000: episode: 465, duration: 3.684s, episode steps: 616, steps per second: 167, episode reward: 135.000, mean reward:  0.219 [ 0.000, 30.000], mean action: 2.323 [0.000, 5.000],  loss: 33.609372, mean_q: 293.090394, mean_eps: 0.714261\n",
            " 318373/1000000: episode: 466, duration: 3.457s, episode steps: 577, steps per second: 167, episode reward: 75.000, mean reward:  0.130 [ 0.000, 25.000], mean action: 2.532 [0.000, 5.000],  loss: 24.593324, mean_q: 292.966299, mean_eps: 0.713724\n",
            " 318957/1000000: episode: 467, duration: 3.525s, episode steps: 584, steps per second: 166, episode reward: 110.000, mean reward:  0.188 [ 0.000, 30.000], mean action: 2.587 [0.000, 5.000],  loss: 41.354739, mean_q: 305.024576, mean_eps: 0.713202\n",
            " 319626/1000000: episode: 468, duration: 3.992s, episode steps: 669, steps per second: 168, episode reward: 105.000, mean reward:  0.157 [ 0.000, 20.000], mean action: 2.496 [0.000, 5.000],  loss: 79.078661, mean_q: 318.570283, mean_eps: 0.712638\n",
            " 320425/1000000: episode: 469, duration: 4.770s, episode steps: 799, steps per second: 168, episode reward: 195.000, mean reward:  0.244 [ 0.000, 30.000], mean action: 2.529 [0.000, 5.000],  loss: 48.065171, mean_q: 313.913947, mean_eps: 0.711978\n",
            " 320901/1000000: episode: 470, duration: 2.828s, episode steps: 476, steps per second: 168, episode reward: 115.000, mean reward:  0.242 [ 0.000, 25.000], mean action: 2.200 [0.000, 5.000],  loss: 57.154146, mean_q: 312.750099, mean_eps: 0.711404\n",
            " 321326/1000000: episode: 471, duration: 2.548s, episode steps: 425, steps per second: 167, episode reward: 115.000, mean reward:  0.271 [ 0.000, 30.000], mean action: 2.386 [0.000, 5.000],  loss: 75.245130, mean_q: 299.152244, mean_eps: 0.710998\n",
            " 321766/1000000: episode: 472, duration: 2.642s, episode steps: 440, steps per second: 167, episode reward: 100.000, mean reward:  0.227 [ 0.000, 20.000], mean action: 2.382 [0.000, 5.000],  loss: 74.315738, mean_q: 291.046522, mean_eps: 0.710609\n",
            " 322512/1000000: episode: 473, duration: 4.514s, episode steps: 746, steps per second: 165, episode reward: 135.000, mean reward:  0.181 [ 0.000, 30.000], mean action: 2.245 [0.000, 5.000],  loss: 43.429725, mean_q: 298.100422, mean_eps: 0.710075\n",
            " 323053/1000000: episode: 474, duration: 3.264s, episode steps: 541, steps per second: 166, episode reward: 110.000, mean reward:  0.203 [ 0.000, 30.000], mean action: 2.494 [0.000, 5.000],  loss: 30.534526, mean_q: 292.491007, mean_eps: 0.709496\n",
            " 323729/1000000: episode: 475, duration: 4.018s, episode steps: 676, steps per second: 168, episode reward: 80.000, mean reward:  0.118 [ 0.000, 25.000], mean action: 2.479 [0.000, 5.000],  loss: 33.501268, mean_q: 295.832339, mean_eps: 0.708949\n",
            " 324206/1000000: episode: 476, duration: 2.853s, episode steps: 477, steps per second: 167, episode reward: 55.000, mean reward:  0.115 [ 0.000, 25.000], mean action: 2.488 [0.000, 5.000],  loss: 53.766967, mean_q: 293.658527, mean_eps: 0.708430\n",
            " 324863/1000000: episode: 477, duration: 3.893s, episode steps: 657, steps per second: 169, episode reward: 180.000, mean reward:  0.274 [ 0.000, 30.000], mean action: 2.516 [0.000, 5.000],  loss: 47.104274, mean_q: 297.301883, mean_eps: 0.707919\n",
            " 325882/1000000: episode: 478, duration: 6.067s, episode steps: 1019, steps per second: 168, episode reward: 255.000, mean reward:  0.250 [ 0.000, 30.000], mean action: 2.524 [0.000, 5.000],  loss: 22.677991, mean_q: 310.283657, mean_eps: 0.707165\n",
            " 327141/1000000: episode: 479, duration: 7.412s, episode steps: 1259, steps per second: 170, episode reward: 460.000, mean reward:  0.365 [ 0.000, 200.000], mean action: 2.357 [0.000, 5.000],  loss: 38.221813, mean_q: 325.873605, mean_eps: 0.706140\n",
            " 328151/1000000: episode: 480, duration: 6.014s, episode steps: 1010, steps per second: 168, episode reward: 140.000, mean reward:  0.139 [ 0.000, 30.000], mean action: 2.432 [0.000, 5.000],  loss: 33.682636, mean_q: 306.984289, mean_eps: 0.705119\n",
            " 328786/1000000: episode: 481, duration: 3.780s, episode steps: 635, steps per second: 168, episode reward: 80.000, mean reward:  0.126 [ 0.000, 20.000], mean action: 2.304 [0.000, 5.000],  loss: 43.635067, mean_q: 300.189348, mean_eps: 0.704379\n",
            " 329464/1000000: episode: 482, duration: 4.004s, episode steps: 678, steps per second: 169, episode reward: 180.000, mean reward:  0.265 [ 0.000, 30.000], mean action: 2.367 [0.000, 5.000],  loss: 38.011016, mean_q: 299.396068, mean_eps: 0.703788\n",
            " 330537/1000000: episode: 483, duration: 6.366s, episode steps: 1073, steps per second: 169, episode reward: 335.000, mean reward:  0.312 [ 0.000, 30.000], mean action: 2.489 [0.000, 5.000],  loss: 36.724316, mean_q: 297.225550, mean_eps: 0.703000\n",
            " 331393/1000000: episode: 484, duration: 5.078s, episode steps: 856, steps per second: 169, episode reward: 190.000, mean reward:  0.222 [ 0.000, 30.000], mean action: 2.544 [0.000, 5.000],  loss: 30.720838, mean_q: 299.919506, mean_eps: 0.702132\n",
            " 331745/1000000: episode: 485, duration: 2.086s, episode steps: 352, steps per second: 169, episode reward: 20.000, mean reward:  0.057 [ 0.000, 10.000], mean action: 2.347 [0.000, 5.000],  loss: 27.988179, mean_q: 293.276452, mean_eps: 0.701588\n",
            " 332820/1000000: episode: 486, duration: 6.363s, episode steps: 1075, steps per second: 169, episode reward: 475.000, mean reward:  0.442 [ 0.000, 200.000], mean action: 2.569 [0.000, 5.000],  loss: 58.447601, mean_q: 302.954406, mean_eps: 0.700946\n",
            " 333523/1000000: episode: 487, duration: 4.230s, episode steps: 703, steps per second: 166, episode reward: 120.000, mean reward:  0.171 [ 0.000, 30.000], mean action: 2.259 [0.000, 5.000],  loss: 65.586083, mean_q: 303.251454, mean_eps: 0.700146\n",
            " 334335/1000000: episode: 488, duration: 4.846s, episode steps: 812, steps per second: 168, episode reward: 225.000, mean reward:  0.277 [ 0.000, 30.000], mean action: 2.436 [0.000, 5.000],  loss: 43.849502, mean_q: 298.232943, mean_eps: 0.699464\n",
            " 335000/1000000: episode: 489, duration: 3.948s, episode steps: 665, steps per second: 168, episode reward: 110.000, mean reward:  0.165 [ 0.000, 30.000], mean action: 2.478 [0.000, 5.000],  loss: 19.194603, mean_q: 293.451527, mean_eps: 0.698800\n",
            " 335606/1000000: episode: 490, duration: 3.601s, episode steps: 606, steps per second: 168, episode reward: 125.000, mean reward:  0.206 [ 0.000, 25.000], mean action: 2.399 [0.000, 5.000],  loss: 31.260160, mean_q: 300.053647, mean_eps: 0.698228\n",
            " 336471/1000000: episode: 491, duration: 5.144s, episode steps: 865, steps per second: 168, episode reward: 215.000, mean reward:  0.249 [ 0.000, 30.000], mean action: 2.356 [0.000, 5.000],  loss: 41.308548, mean_q: 298.741769, mean_eps: 0.697566\n",
            " 336894/1000000: episode: 492, duration: 2.531s, episode steps: 423, steps per second: 167, episode reward: 25.000, mean reward:  0.059 [ 0.000, 20.000], mean action: 2.683 [0.000, 5.000],  loss: 39.771433, mean_q: 285.343526, mean_eps: 0.696986\n",
            " 337505/1000000: episode: 493, duration: 3.667s, episode steps: 611, steps per second: 167, episode reward: 90.000, mean reward:  0.147 [ 0.000, 25.000], mean action: 2.481 [0.000, 5.000],  loss: 54.483187, mean_q: 295.727862, mean_eps: 0.696521\n",
            " 338149/1000000: episode: 494, duration: 3.843s, episode steps: 644, steps per second: 168, episode reward: 105.000, mean reward:  0.163 [ 0.000, 30.000], mean action: 2.643 [0.000, 5.000],  loss: 43.688666, mean_q: 308.047632, mean_eps: 0.695956\n",
            " 338920/1000000: episode: 495, duration: 4.631s, episode steps: 771, steps per second: 166, episode reward: 165.000, mean reward:  0.214 [ 0.000, 30.000], mean action: 2.466 [0.000, 5.000],  loss: 40.316393, mean_q: 299.678149, mean_eps: 0.695319\n",
            " 339416/1000000: episode: 496, duration: 2.989s, episode steps: 496, steps per second: 166, episode reward: 60.000, mean reward:  0.121 [ 0.000, 15.000], mean action: 2.569 [0.000, 5.000],  loss: 39.464235, mean_q: 290.567873, mean_eps: 0.694749\n",
            " 339992/1000000: episode: 497, duration: 3.448s, episode steps: 576, steps per second: 167, episode reward: 135.000, mean reward:  0.234 [ 0.000, 30.000], mean action: 2.502 [0.000, 5.000],  loss: 51.165676, mean_q: 296.235262, mean_eps: 0.694267\n",
            " 340482/1000000: episode: 498, duration: 2.982s, episode steps: 490, steps per second: 164, episode reward: 135.000, mean reward:  0.276 [ 0.000, 30.000], mean action: 2.202 [0.000, 5.000],  loss: 41.273926, mean_q: 294.405308, mean_eps: 0.693787\n",
            " 341463/1000000: episode: 499, duration: 5.921s, episode steps: 981, steps per second: 166, episode reward: 255.000, mean reward:  0.260 [ 0.000, 30.000], mean action: 2.666 [0.000, 5.000],  loss: 57.724066, mean_q: 298.005113, mean_eps: 0.693125\n",
            " 342605/1000000: episode: 500, duration: 6.763s, episode steps: 1142, steps per second: 169, episode reward: 320.000, mean reward:  0.280 [ 0.000, 30.000], mean action: 2.606 [0.000, 5.000],  loss: 43.976906, mean_q: 291.211960, mean_eps: 0.692170\n",
            " 343183/1000000: episode: 501, duration: 3.434s, episode steps: 578, steps per second: 168, episode reward: 45.000, mean reward:  0.078 [ 0.000, 15.000], mean action: 2.419 [0.000, 5.000],  loss: 39.277103, mean_q: 287.633424, mean_eps: 0.691396\n",
            " 343912/1000000: episode: 502, duration: 4.393s, episode steps: 729, steps per second: 166, episode reward: 165.000, mean reward:  0.226 [ 0.000, 25.000], mean action: 2.394 [0.000, 5.000],  loss: 42.318116, mean_q: 294.574844, mean_eps: 0.690808\n",
            " 344520/1000000: episode: 503, duration: 3.628s, episode steps: 608, steps per second: 168, episode reward: 155.000, mean reward:  0.255 [ 0.000, 30.000], mean action: 2.650 [0.000, 5.000],  loss: 39.703772, mean_q: 301.633264, mean_eps: 0.690206\n",
            " 345352/1000000: episode: 504, duration: 4.929s, episode steps: 832, steps per second: 169, episode reward: 265.000, mean reward:  0.319 [ 0.000, 30.000], mean action: 2.359 [0.000, 5.000],  loss: 25.960007, mean_q: 306.710837, mean_eps: 0.689558\n",
            " 345995/1000000: episode: 505, duration: 3.838s, episode steps: 643, steps per second: 168, episode reward: 125.000, mean reward:  0.194 [ 0.000, 25.000], mean action: 2.722 [0.000, 5.000],  loss: 21.836185, mean_q: 302.526253, mean_eps: 0.688894\n",
            " 346671/1000000: episode: 506, duration: 4.048s, episode steps: 676, steps per second: 167, episode reward: 390.000, mean reward:  0.577 [ 0.000, 200.000], mean action: 2.536 [0.000, 5.000],  loss: 29.699102, mean_q: 307.954434, mean_eps: 0.688301\n",
            " 347709/1000000: episode: 507, duration: 6.166s, episode steps: 1038, steps per second: 168, episode reward: 445.000, mean reward:  0.429 [ 0.000, 200.000], mean action: 2.491 [0.000, 5.000],  loss: 55.185101, mean_q: 308.271330, mean_eps: 0.687529\n",
            " 348581/1000000: episode: 508, duration: 5.175s, episode steps: 872, steps per second: 168, episode reward: 130.000, mean reward:  0.149 [ 0.000, 20.000], mean action: 2.615 [0.000, 5.000],  loss: 42.438459, mean_q: 283.700753, mean_eps: 0.686670\n",
            " 349242/1000000: episode: 509, duration: 3.954s, episode steps: 661, steps per second: 167, episode reward: 60.000, mean reward:  0.091 [ 0.000, 25.000], mean action: 2.437 [0.000, 5.000],  loss: 23.414885, mean_q: 300.453179, mean_eps: 0.685980\n",
            " 350009/1000000: episode: 510, duration: 4.585s, episode steps: 767, steps per second: 167, episode reward: 165.000, mean reward:  0.215 [ 0.000, 30.000], mean action: 2.437 [0.000, 5.000],  loss: 40.784342, mean_q: 299.784328, mean_eps: 0.685338\n",
            " 350953/1000000: episode: 511, duration: 5.675s, episode steps: 944, steps per second: 166, episode reward: 265.000, mean reward:  0.281 [ 0.000, 30.000], mean action: 2.635 [0.000, 5.000],  loss: 39.431426, mean_q: 291.421235, mean_eps: 0.684568\n",
            " 351470/1000000: episode: 512, duration: 3.131s, episode steps: 517, steps per second: 165, episode reward: 35.000, mean reward:  0.068 [ 0.000, 15.000], mean action: 2.683 [0.000, 5.000],  loss: 39.964846, mean_q: 281.746499, mean_eps: 0.683910\n",
            " 351973/1000000: episode: 513, duration: 3.017s, episode steps: 503, steps per second: 167, episode reward: 35.000, mean reward:  0.070 [ 0.000, 15.000], mean action: 2.342 [0.000, 5.000],  loss: 43.823523, mean_q: 289.041019, mean_eps: 0.683451\n",
            " 352966/1000000: episode: 514, duration: 5.950s, episode steps: 993, steps per second: 167, episode reward: 250.000, mean reward:  0.252 [ 0.000, 30.000], mean action: 2.686 [0.000, 5.000],  loss: 28.938423, mean_q: 301.456148, mean_eps: 0.682778\n",
            " 353607/1000000: episode: 515, duration: 3.872s, episode steps: 641, steps per second: 166, episode reward: 135.000, mean reward:  0.211 [ 0.000, 30.000], mean action: 2.590 [0.000, 5.000],  loss: 17.763210, mean_q: 294.847377, mean_eps: 0.682043\n",
            " 354136/1000000: episode: 516, duration: 3.183s, episode steps: 529, steps per second: 166, episode reward: 180.000, mean reward:  0.340 [ 0.000, 30.000], mean action: 2.302 [0.000, 5.000],  loss: 31.179026, mean_q: 297.749290, mean_eps: 0.681516\n",
            " 354759/1000000: episode: 517, duration: 3.727s, episode steps: 623, steps per second: 167, episode reward: 90.000, mean reward:  0.144 [ 0.000, 25.000], mean action: 2.408 [0.000, 5.000],  loss: 42.807385, mean_q: 304.187043, mean_eps: 0.680998\n",
            " 355521/1000000: episode: 518, duration: 4.557s, episode steps: 762, steps per second: 167, episode reward: 135.000, mean reward:  0.177 [ 0.000, 30.000], mean action: 2.493 [0.000, 5.000],  loss: 40.892801, mean_q: 299.205219, mean_eps: 0.680374\n",
            " 356108/1000000: episode: 519, duration: 3.530s, episode steps: 587, steps per second: 166, episode reward: 110.000, mean reward:  0.187 [ 0.000, 30.000], mean action: 2.603 [0.000, 5.000],  loss: 30.759332, mean_q: 291.224543, mean_eps: 0.679767\n",
            " 357014/1000000: episode: 520, duration: 5.391s, episode steps: 906, steps per second: 168, episode reward: 520.000, mean reward:  0.574 [ 0.000, 200.000], mean action: 2.662 [0.000, 5.000],  loss: 43.062732, mean_q: 300.175877, mean_eps: 0.679096\n",
            " 357628/1000000: episode: 521, duration: 3.701s, episode steps: 614, steps per second: 166, episode reward: 180.000, mean reward:  0.293 [ 0.000, 30.000], mean action: 2.565 [0.000, 5.000],  loss: 31.391008, mean_q: 308.524195, mean_eps: 0.678412\n",
            " 358493/1000000: episode: 522, duration: 5.184s, episode steps: 865, steps per second: 167, episode reward: 245.000, mean reward:  0.283 [ 0.000, 30.000], mean action: 2.253 [0.000, 5.000],  loss: 22.998547, mean_q: 296.261165, mean_eps: 0.677746\n",
            " 359362/1000000: episode: 523, duration: 5.228s, episode steps: 869, steps per second: 166, episode reward: 135.000, mean reward:  0.155 [ 0.000, 30.000], mean action: 2.595 [0.000, 5.000],  loss: 27.304322, mean_q: 283.957497, mean_eps: 0.676966\n",
            " 359774/1000000: episode: 524, duration: 2.530s, episode steps: 412, steps per second: 163, episode reward: 60.000, mean reward:  0.146 [ 0.000, 30.000], mean action: 2.653 [0.000, 5.000],  loss: 43.049235, mean_q: 291.329935, mean_eps: 0.676389\n",
            " 360890/1000000: episode: 525, duration: 6.674s, episode steps: 1116, steps per second: 167, episode reward: 300.000, mean reward:  0.269 [ 0.000, 30.000], mean action: 2.496 [0.000, 5.000],  loss: 53.701174, mean_q: 299.203668, mean_eps: 0.675702\n",
            " 361542/1000000: episode: 526, duration: 3.906s, episode steps: 652, steps per second: 167, episode reward: 110.000, mean reward:  0.169 [ 0.000, 30.000], mean action: 2.420 [0.000, 5.000],  loss: 54.584076, mean_q: 305.741656, mean_eps: 0.674906\n",
            " 362192/1000000: episode: 527, duration: 3.862s, episode steps: 650, steps per second: 168, episode reward: 80.000, mean reward:  0.123 [ 0.000, 20.000], mean action: 2.462 [0.000, 5.000],  loss: 32.242208, mean_q: 301.080839, mean_eps: 0.674320\n",
            " 362936/1000000: episode: 528, duration: 4.483s, episode steps: 744, steps per second: 166, episode reward: 195.000, mean reward:  0.262 [ 0.000, 30.000], mean action: 2.687 [0.000, 5.000],  loss: 20.016579, mean_q: 297.219282, mean_eps: 0.673693\n",
            " 363419/1000000: episode: 529, duration: 2.969s, episode steps: 483, steps per second: 163, episode reward: 130.000, mean reward:  0.269 [ 0.000, 30.000], mean action: 2.569 [0.000, 5.000],  loss: 35.685376, mean_q: 300.968035, mean_eps: 0.673141\n",
            " 363819/1000000: episode: 530, duration: 2.380s, episode steps: 400, steps per second: 168, episode reward: 25.000, mean reward:  0.062 [ 0.000, 15.000], mean action: 2.362 [0.000, 5.000],  loss: 57.503883, mean_q: 304.180542, mean_eps: 0.672743\n",
            " 364456/1000000: episode: 531, duration: 3.810s, episode steps: 637, steps per second: 167, episode reward: 150.000, mean reward:  0.235 [ 0.000, 30.000], mean action: 2.568 [0.000, 5.000],  loss: 55.569062, mean_q: 307.269143, mean_eps: 0.672277\n",
            " 364898/1000000: episode: 532, duration: 2.631s, episode steps: 442, steps per second: 168, episode reward: 40.000, mean reward:  0.090 [ 0.000, 30.000], mean action: 2.480 [0.000, 5.000],  loss: 62.961970, mean_q: 308.917109, mean_eps: 0.671791\n",
            " 365323/1000000: episode: 533, duration: 2.518s, episode steps: 425, steps per second: 169, episode reward: 90.000, mean reward:  0.212 [ 0.000, 25.000], mean action: 2.534 [0.000, 5.000],  loss: 65.847320, mean_q: 309.711100, mean_eps: 0.671401\n",
            " 365880/1000000: episode: 534, duration: 3.334s, episode steps: 557, steps per second: 167, episode reward: 105.000, mean reward:  0.189 [ 0.000, 25.000], mean action: 2.217 [0.000, 5.000],  loss: 87.737461, mean_q: 308.819525, mean_eps: 0.670959\n",
            " 366698/1000000: episode: 535, duration: 4.865s, episode steps: 818, steps per second: 168, episode reward: 200.000, mean reward:  0.244 [ 0.000, 20.000], mean action: 2.460 [0.000, 5.000],  loss: 49.526733, mean_q: 315.331816, mean_eps: 0.670340\n",
            " 367264/1000000: episode: 536, duration: 3.385s, episode steps: 566, steps per second: 167, episode reward: 55.000, mean reward:  0.097 [ 0.000, 20.000], mean action: 2.666 [0.000, 5.000],  loss: 48.072924, mean_q: 321.593475, mean_eps: 0.669718\n",
            " 367968/1000000: episode: 537, duration: 4.222s, episode steps: 704, steps per second: 167, episode reward: 120.000, mean reward:  0.170 [ 0.000, 30.000], mean action: 2.703 [0.000, 5.000],  loss: 52.489211, mean_q: 307.330105, mean_eps: 0.669146\n",
            " 368519/1000000: episode: 538, duration: 3.309s, episode steps: 551, steps per second: 167, episode reward: 145.000, mean reward:  0.263 [ 0.000, 30.000], mean action: 2.452 [0.000, 5.000],  loss: 48.799018, mean_q: 298.300724, mean_eps: 0.668581\n",
            " 369196/1000000: episode: 539, duration: 4.044s, episode steps: 677, steps per second: 167, episode reward: 110.000, mean reward:  0.162 [ 0.000, 30.000], mean action: 2.433 [0.000, 5.000],  loss: 37.264572, mean_q: 291.662281, mean_eps: 0.668029\n",
            " 369764/1000000: episode: 540, duration: 3.466s, episode steps: 568, steps per second: 164, episode reward: 90.000, mean reward:  0.158 [ 0.000, 25.000], mean action: 2.475 [0.000, 5.000],  loss: 30.596137, mean_q: 295.027341, mean_eps: 0.667468\n",
            " 370306/1000000: episode: 541, duration: 3.239s, episode steps: 542, steps per second: 167, episode reward: 75.000, mean reward:  0.138 [ 0.000, 25.000], mean action: 2.653 [0.000, 5.000],  loss: 31.672939, mean_q: 297.884443, mean_eps: 0.666969\n",
            " 370696/1000000: episode: 542, duration: 2.347s, episode steps: 390, steps per second: 166, episode reward: 65.000, mean reward:  0.167 [ 0.000, 20.000], mean action: 2.395 [0.000, 5.000],  loss: 36.441241, mean_q: 302.703392, mean_eps: 0.666550\n",
            " 371335/1000000: episode: 543, duration: 3.878s, episode steps: 639, steps per second: 165, episode reward: 105.000, mean reward:  0.164 [ 0.000, 25.000], mean action: 2.443 [0.000, 5.000],  loss: 84.423288, mean_q: 308.633023, mean_eps: 0.666086\n",
            " 371791/1000000: episode: 544, duration: 2.758s, episode steps: 456, steps per second: 165, episode reward: 110.000, mean reward:  0.241 [ 0.000, 30.000], mean action: 2.373 [0.000, 5.000],  loss: 70.854498, mean_q: 304.069499, mean_eps: 0.665594\n",
            " 372425/1000000: episode: 545, duration: 3.806s, episode steps: 634, steps per second: 167, episode reward: 100.000, mean reward:  0.158 [ 0.000, 20.000], mean action: 2.550 [0.000, 5.000],  loss: 46.476107, mean_q: 299.943423, mean_eps: 0.665103\n",
            " 373859/1000000: episode: 546, duration: 8.502s, episode steps: 1434, steps per second: 169, episode reward: 325.000, mean reward:  0.227 [ 0.000, 30.000], mean action: 2.416 [0.000, 5.000],  loss: 37.181178, mean_q: 316.633907, mean_eps: 0.664173\n",
            " 374219/1000000: episode: 547, duration: 2.145s, episode steps: 360, steps per second: 168, episode reward: 90.000, mean reward:  0.250 [ 0.000, 25.000], mean action: 2.408 [0.000, 5.000],  loss: 107.125698, mean_q: 314.378025, mean_eps: 0.663365\n",
            " 375034/1000000: episode: 548, duration: 4.839s, episode steps: 815, steps per second: 168, episode reward: 225.000, mean reward:  0.276 [ 0.000, 30.000], mean action: 2.464 [0.000, 5.000],  loss: 101.689667, mean_q: 300.254667, mean_eps: 0.662837\n",
            " 375454/1000000: episode: 549, duration: 2.563s, episode steps: 420, steps per second: 164, episode reward: 50.000, mean reward:  0.119 [ 0.000, 20.000], mean action: 2.583 [0.000, 5.000],  loss: 32.111604, mean_q: 289.336945, mean_eps: 0.662281\n",
            " 375928/1000000: episode: 550, duration: 2.896s, episode steps: 474, steps per second: 164, episode reward: 50.000, mean reward:  0.105 [ 0.000, 20.000], mean action: 2.340 [0.000, 5.000],  loss: 63.106590, mean_q: 292.834658, mean_eps: 0.661879\n",
            " 376320/1000000: episode: 551, duration: 2.370s, episode steps: 392, steps per second: 165, episode reward: 20.000, mean reward:  0.051 [ 0.000, 10.000], mean action: 2.566 [0.000, 5.000],  loss: 71.839910, mean_q: 301.602639, mean_eps: 0.661489\n",
            " 377079/1000000: episode: 552, duration: 4.651s, episode steps: 759, steps per second: 163, episode reward: 155.000, mean reward:  0.204 [ 0.000, 30.000], mean action: 2.482 [0.000, 5.000],  loss: 61.601281, mean_q: 303.286590, mean_eps: 0.660971\n",
            " 378175/1000000: episode: 553, duration: 6.592s, episode steps: 1096, steps per second: 166, episode reward: 550.000, mean reward:  0.502 [ 0.000, 200.000], mean action: 2.328 [0.000, 5.000],  loss: 27.188875, mean_q: 302.638345, mean_eps: 0.660136\n",
            " 378824/1000000: episode: 554, duration: 3.915s, episode steps: 649, steps per second: 166, episode reward: 45.000, mean reward:  0.069 [ 0.000, 15.000], mean action: 2.418 [0.000, 5.000],  loss: 54.728325, mean_q: 317.157520, mean_eps: 0.659351\n",
            " 379219/1000000: episode: 555, duration: 2.423s, episode steps: 395, steps per second: 163, episode reward: 80.000, mean reward:  0.203 [ 0.000, 20.000], mean action: 2.466 [0.000, 5.000],  loss: 56.345708, mean_q: 305.009012, mean_eps: 0.658881\n",
            " 379848/1000000: episode: 556, duration: 3.833s, episode steps: 629, steps per second: 164, episode reward: 85.000, mean reward:  0.135 [ 0.000, 20.000], mean action: 2.523 [0.000, 5.000],  loss: 62.942022, mean_q: 303.934178, mean_eps: 0.658420\n",
            " 380572/1000000: episode: 557, duration: 4.383s, episode steps: 724, steps per second: 165, episode reward: 140.000, mean reward:  0.193 [ 0.000, 30.000], mean action: 2.122 [0.000, 5.000],  loss: 40.676513, mean_q: 308.037105, mean_eps: 0.657811\n",
            " 381278/1000000: episode: 558, duration: 4.276s, episode steps: 706, steps per second: 165, episode reward: 240.000, mean reward:  0.340 [ 0.000, 30.000], mean action: 2.497 [0.000, 5.000],  loss: 58.046383, mean_q: 310.461327, mean_eps: 0.657168\n",
            " 381665/1000000: episode: 559, duration: 2.353s, episode steps: 387, steps per second: 164, episode reward: 125.000, mean reward:  0.323 [ 0.000, 25.000], mean action: 2.633 [0.000, 5.000],  loss: 71.345913, mean_q: 319.591925, mean_eps: 0.656676\n",
            " 382298/1000000: episode: 560, duration: 3.849s, episode steps: 633, steps per second: 164, episode reward: 170.000, mean reward:  0.269 [ 0.000, 30.000], mean action: 2.442 [0.000, 5.000],  loss: 77.809967, mean_q: 311.916211, mean_eps: 0.656217\n",
            " 383000/1000000: episode: 561, duration: 4.236s, episode steps: 702, steps per second: 166, episode reward: 120.000, mean reward:  0.171 [ 0.000, 30.000], mean action: 2.407 [0.000, 5.000],  loss: 56.926833, mean_q: 305.142310, mean_eps: 0.655616\n",
            " 383654/1000000: episode: 562, duration: 3.957s, episode steps: 654, steps per second: 165, episode reward: 135.000, mean reward:  0.206 [ 0.000, 30.000], mean action: 2.401 [0.000, 5.000],  loss: 30.921013, mean_q: 302.247268, mean_eps: 0.655006\n",
            " 384531/1000000: episode: 563, duration: 5.280s, episode steps: 877, steps per second: 166, episode reward: 165.000, mean reward:  0.188 [ 0.000, 25.000], mean action: 2.403 [0.000, 5.000],  loss: 45.076790, mean_q: 312.699035, mean_eps: 0.654317\n",
            " 385211/1000000: episode: 564, duration: 4.112s, episode steps: 680, steps per second: 165, episode reward: 155.000, mean reward:  0.228 [ 0.000, 30.000], mean action: 2.575 [0.000, 5.000],  loss: 39.149336, mean_q: 317.739616, mean_eps: 0.653617\n",
            " 385846/1000000: episode: 565, duration: 3.805s, episode steps: 635, steps per second: 167, episode reward: 95.000, mean reward:  0.150 [ 0.000, 25.000], mean action: 2.326 [0.000, 5.000],  loss: 27.217997, mean_q: 300.014485, mean_eps: 0.653025\n",
            " 387359/1000000: episode: 566, duration: 8.949s, episode steps: 1513, steps per second: 169, episode reward: 445.000, mean reward:  0.294 [ 0.000, 30.000], mean action: 2.602 [0.000, 5.000],  loss: 32.779260, mean_q: 315.558593, mean_eps: 0.652058\n",
            " 388287/1000000: episode: 567, duration: 5.511s, episode steps: 928, steps per second: 168, episode reward: 225.000, mean reward:  0.242 [ 0.000, 30.000], mean action: 2.431 [0.000, 5.000],  loss: 34.651982, mean_q: 314.232109, mean_eps: 0.650960\n",
            " 388996/1000000: episode: 568, duration: 4.208s, episode steps: 709, steps per second: 169, episode reward: 180.000, mean reward:  0.254 [ 0.000, 30.000], mean action: 2.508 [0.000, 5.000],  loss: 42.516578, mean_q: 290.292769, mean_eps: 0.650223\n",
            " 389521/1000000: episode: 569, duration: 3.160s, episode steps: 525, steps per second: 166, episode reward: 75.000, mean reward:  0.143 [ 0.000, 25.000], mean action: 2.373 [0.000, 5.000],  loss: 30.332912, mean_q: 287.952359, mean_eps: 0.649668\n",
            " 390048/1000000: episode: 570, duration: 3.192s, episode steps: 527, steps per second: 165, episode reward: 45.000, mean reward:  0.085 [ 0.000, 15.000], mean action: 2.537 [0.000, 5.000],  loss: 29.502053, mean_q: 297.178824, mean_eps: 0.649194\n",
            " 390653/1000000: episode: 571, duration: 3.595s, episode steps: 605, steps per second: 168, episode reward: 135.000, mean reward:  0.223 [ 0.000, 30.000], mean action: 2.412 [0.000, 5.000],  loss: 37.117476, mean_q: 305.773146, mean_eps: 0.648685\n",
            " 391274/1000000: episode: 572, duration: 3.785s, episode steps: 621, steps per second: 164, episode reward: 170.000, mean reward:  0.274 [ 0.000, 30.000], mean action: 2.717 [0.000, 5.000],  loss: 29.754132, mean_q: 300.251753, mean_eps: 0.648133\n",
            " 391602/1000000: episode: 573, duration: 2.075s, episode steps: 328, steps per second: 158, episode reward: 30.000, mean reward:  0.091 [ 0.000, 10.000], mean action: 2.848 [0.000, 5.000],  loss: 35.962689, mean_q: 294.720744, mean_eps: 0.647706\n",
            " 392007/1000000: episode: 574, duration: 2.480s, episode steps: 405, steps per second: 163, episode reward: 50.000, mean reward:  0.123 [ 0.000, 30.000], mean action: 2.526 [0.000, 5.000],  loss: 65.860142, mean_q: 299.822091, mean_eps: 0.647376\n",
            " 392919/1000000: episode: 575, duration: 5.511s, episode steps: 912, steps per second: 165, episode reward: 210.000, mean reward:  0.230 [ 0.000, 30.000], mean action: 2.671 [0.000, 5.000],  loss: 89.278503, mean_q: 303.959813, mean_eps: 0.646784\n",
            " 393613/1000000: episode: 576, duration: 4.189s, episode steps: 694, steps per second: 166, episode reward: 110.000, mean reward:  0.159 [ 0.000, 30.000], mean action: 2.542 [0.000, 5.000],  loss: 46.579481, mean_q: 280.174282, mean_eps: 0.646061\n",
            " 394558/1000000: episode: 577, duration: 5.628s, episode steps: 945, steps per second: 168, episode reward: 195.000, mean reward:  0.206 [ 0.000, 30.000], mean action: 2.558 [0.000, 5.000],  loss: 34.688256, mean_q: 288.527629, mean_eps: 0.645324\n",
            " 395504/1000000: episode: 578, duration: 5.632s, episode steps: 946, steps per second: 168, episode reward: 185.000, mean reward:  0.196 [ 0.000, 30.000], mean action: 2.468 [0.000, 5.000],  loss: 40.784628, mean_q: 295.308786, mean_eps: 0.644473\n",
            " 396828/1000000: episode: 579, duration: 7.942s, episode steps: 1324, steps per second: 167, episode reward: 460.000, mean reward:  0.347 [ 0.000, 200.000], mean action: 2.604 [0.000, 5.000],  loss: 34.477857, mean_q: 313.425896, mean_eps: 0.643451\n",
            " 397679/1000000: episode: 580, duration: 5.036s, episode steps: 851, steps per second: 169, episode reward: 155.000, mean reward:  0.182 [ 0.000, 30.000], mean action: 2.429 [0.000, 5.000],  loss: 51.966381, mean_q: 292.173411, mean_eps: 0.642472\n",
            " 398393/1000000: episode: 581, duration: 4.240s, episode steps: 714, steps per second: 168, episode reward: 135.000, mean reward:  0.189 [ 0.000, 30.000], mean action: 2.538 [0.000, 5.000],  loss: 28.864388, mean_q: 283.338981, mean_eps: 0.641768\n",
            " 399045/1000000: episode: 582, duration: 3.913s, episode steps: 652, steps per second: 167, episode reward: 135.000, mean reward:  0.207 [ 0.000, 30.000], mean action: 2.698 [0.000, 5.000],  loss: 24.425228, mean_q: 292.684430, mean_eps: 0.641153\n",
            " 399720/1000000: episode: 583, duration: 4.014s, episode steps: 675, steps per second: 168, episode reward: 130.000, mean reward:  0.193 [ 0.000, 20.000], mean action: 2.418 [0.000, 5.000],  loss: 19.009444, mean_q: 305.349973, mean_eps: 0.640556\n",
            " 400541/1000000: episode: 584, duration: 4.862s, episode steps: 821, steps per second: 169, episode reward: 210.000, mean reward:  0.256 [ 0.000, 30.000], mean action: 2.440 [0.000, 5.000],  loss: 33.613275, mean_q: 304.276816, mean_eps: 0.639883\n",
            " 401129/1000000: episode: 585, duration: 3.488s, episode steps: 588, steps per second: 169, episode reward: 85.000, mean reward:  0.145 [ 0.000, 25.000], mean action: 2.313 [0.000, 5.000],  loss: 23.395403, mean_q: 293.951128, mean_eps: 0.639249\n",
            " 401650/1000000: episode: 586, duration: 3.139s, episode steps: 521, steps per second: 166, episode reward: 80.000, mean reward:  0.154 [ 0.000, 30.000], mean action: 2.599 [0.000, 5.000],  loss: 35.726248, mean_q: 301.135746, mean_eps: 0.638750\n",
            " 402438/1000000: episode: 587, duration: 4.688s, episode steps: 788, steps per second: 168, episode reward: 155.000, mean reward:  0.197 [ 0.000, 30.000], mean action: 2.580 [0.000, 5.000],  loss: 41.009796, mean_q: 301.046906, mean_eps: 0.638161\n",
            " 402979/1000000: episode: 588, duration: 3.210s, episode steps: 541, steps per second: 169, episode reward: 80.000, mean reward:  0.148 [ 0.000, 25.000], mean action: 2.471 [0.000, 5.000],  loss: 28.077078, mean_q: 301.383730, mean_eps: 0.637563\n",
            " 403700/1000000: episode: 589, duration: 4.311s, episode steps: 721, steps per second: 167, episode reward: 80.000, mean reward:  0.111 [ 0.000, 25.000], mean action: 2.248 [0.000, 5.000],  loss: 41.843826, mean_q: 302.004710, mean_eps: 0.636995\n",
            " 404376/1000000: episode: 590, duration: 4.055s, episode steps: 676, steps per second: 167, episode reward: 105.000, mean reward:  0.155 [ 0.000, 30.000], mean action: 2.374 [0.000, 5.000],  loss: 40.421087, mean_q: 299.550678, mean_eps: 0.636366\n",
            " 404826/1000000: episode: 591, duration: 2.658s, episode steps: 450, steps per second: 169, episode reward: 65.000, mean reward:  0.144 [ 0.000, 20.000], mean action: 2.251 [0.000, 5.000],  loss: 30.040656, mean_q: 293.389878, mean_eps: 0.635860\n",
            " 405943/1000000: episode: 592, duration: 6.597s, episode steps: 1117, steps per second: 169, episode reward: 360.000, mean reward:  0.322 [ 0.000, 30.000], mean action: 2.603 [0.000, 5.000],  loss: 35.711178, mean_q: 307.086243, mean_eps: 0.635154\n",
            " 406754/1000000: episode: 593, duration: 4.788s, episode steps: 811, steps per second: 169, episode reward: 380.000, mean reward:  0.469 [ 0.000, 200.000], mean action: 2.349 [0.000, 5.000],  loss: 45.692192, mean_q: 308.019785, mean_eps: 0.634287\n",
            " 407151/1000000: episode: 594, duration: 2.395s, episode steps: 397, steps per second: 166, episode reward: 125.000, mean reward:  0.315 [ 0.000, 25.000], mean action: 2.290 [0.000, 5.000],  loss: 67.686904, mean_q: 279.735312, mean_eps: 0.633743\n",
            " 407719/1000000: episode: 595, duration: 3.455s, episode steps: 568, steps per second: 164, episode reward: 80.000, mean reward:  0.141 [ 0.000, 25.000], mean action: 2.553 [0.000, 5.000],  loss: 64.433584, mean_q: 288.690198, mean_eps: 0.633309\n",
            " 408504/1000000: episode: 596, duration: 4.651s, episode steps: 785, steps per second: 169, episode reward: 180.000, mean reward:  0.229 [ 0.000, 30.000], mean action: 2.345 [0.000, 5.000],  loss: 40.253891, mean_q: 299.070246, mean_eps: 0.632700\n",
            " 409326/1000000: episode: 597, duration: 4.908s, episode steps: 822, steps per second: 167, episode reward: 210.000, mean reward:  0.255 [ 0.000, 30.000], mean action: 2.528 [0.000, 5.000],  loss: 21.360436, mean_q: 287.898561, mean_eps: 0.631977\n",
            " 410002/1000000: episode: 598, duration: 4.035s, episode steps: 676, steps per second: 168, episode reward: 105.000, mean reward:  0.155 [ 0.000, 30.000], mean action: 2.512 [0.000, 5.000],  loss: 35.931626, mean_q: 295.753060, mean_eps: 0.631303\n",
            " 410982/1000000: episode: 599, duration: 5.799s, episode steps: 980, steps per second: 169, episode reward: 265.000, mean reward:  0.270 [ 0.000, 30.000], mean action: 2.412 [0.000, 5.000],  loss: 31.919132, mean_q: 302.480878, mean_eps: 0.630558\n",
            " 411394/1000000: episode: 600, duration: 2.463s, episode steps: 412, steps per second: 167, episode reward: 45.000, mean reward:  0.109 [ 0.000, 15.000], mean action: 2.483 [0.000, 5.000],  loss: 28.014992, mean_q: 289.451459, mean_eps: 0.629931\n",
            " 411914/1000000: episode: 601, duration: 3.140s, episode steps: 520, steps per second: 166, episode reward: 75.000, mean reward:  0.144 [ 0.000, 25.000], mean action: 2.544 [0.000, 5.000],  loss: 53.911341, mean_q: 287.547866, mean_eps: 0.629512\n",
            " 412419/1000000: episode: 602, duration: 3.035s, episode steps: 505, steps per second: 166, episode reward: 55.000, mean reward:  0.109 [ 0.000, 20.000], mean action: 2.606 [0.000, 5.000],  loss: 44.225629, mean_q: 300.797790, mean_eps: 0.629051\n",
            " 413179/1000000: episode: 603, duration: 4.509s, episode steps: 760, steps per second: 169, episode reward: 210.000, mean reward:  0.276 [ 0.000, 30.000], mean action: 2.424 [0.000, 5.000],  loss: 42.545428, mean_q: 302.588198, mean_eps: 0.628481\n",
            " 413725/1000000: episode: 604, duration: 3.279s, episode steps: 546, steps per second: 166, episode reward: 150.000, mean reward:  0.275 [ 0.000, 25.000], mean action: 2.460 [0.000, 5.000],  loss: 55.825595, mean_q: 291.790856, mean_eps: 0.627894\n",
            " 414328/1000000: episode: 605, duration: 3.631s, episode steps: 603, steps per second: 166, episode reward: 70.000, mean reward:  0.116 [ 0.000, 20.000], mean action: 2.458 [0.000, 5.000],  loss: 74.178347, mean_q: 297.717036, mean_eps: 0.627377\n",
            " 414810/1000000: episode: 606, duration: 2.865s, episode steps: 482, steps per second: 168, episode reward: 65.000, mean reward:  0.135 [ 0.000, 20.000], mean action: 2.398 [0.000, 5.000],  loss: 62.585129, mean_q: 312.609291, mean_eps: 0.626888\n",
            " 415315/1000000: episode: 607, duration: 3.034s, episode steps: 505, steps per second: 166, episode reward: 115.000, mean reward:  0.228 [ 0.000, 30.000], mean action: 2.455 [0.000, 5.000],  loss: 67.149529, mean_q: 308.566660, mean_eps: 0.626444\n",
            " 416452/1000000: episode: 608, duration: 6.787s, episode steps: 1137, steps per second: 168, episode reward: 560.000, mean reward:  0.493 [ 0.000, 200.000], mean action: 2.734 [0.000, 5.000],  loss: 42.751787, mean_q: 304.401638, mean_eps: 0.625705\n",
            " 417065/1000000: episode: 609, duration: 3.728s, episode steps: 613, steps per second: 164, episode reward: 75.000, mean reward:  0.122 [ 0.000, 30.000], mean action: 2.493 [0.000, 5.000],  loss: 64.537881, mean_q: 286.233961, mean_eps: 0.624918\n",
            " 417529/1000000: episode: 610, duration: 2.820s, episode steps: 464, steps per second: 165, episode reward: 80.000, mean reward:  0.172 [ 0.000, 20.000], mean action: 2.578 [0.000, 5.000],  loss: 43.004263, mean_q: 294.298110, mean_eps: 0.624433\n",
            " 418033/1000000: episode: 611, duration: 3.057s, episode steps: 504, steps per second: 165, episode reward: 55.000, mean reward:  0.109 [ 0.000, 20.000], mean action: 2.585 [0.000, 5.000],  loss: 47.204684, mean_q: 301.289821, mean_eps: 0.623998\n",
            " 418797/1000000: episode: 612, duration: 4.611s, episode steps: 764, steps per second: 166, episode reward: 300.000, mean reward:  0.393 [ 0.000, 30.000], mean action: 2.339 [0.000, 5.000],  loss: 44.370420, mean_q: 303.287542, mean_eps: 0.623427\n",
            " 419627/1000000: episode: 613, duration: 5.012s, episode steps: 830, steps per second: 166, episode reward: 145.000, mean reward:  0.175 [ 0.000, 30.000], mean action: 2.472 [0.000, 5.000],  loss: 35.298315, mean_q: 303.902298, mean_eps: 0.622710\n",
            " 420641/1000000: episode: 614, duration: 6.109s, episode steps: 1014, steps per second: 166, episode reward: 310.000, mean reward:  0.306 [ 0.000, 30.000], mean action: 2.653 [0.000, 5.000],  loss: 25.597348, mean_q: 308.395077, mean_eps: 0.621880\n",
            " 421032/1000000: episode: 615, duration: 2.379s, episode steps: 391, steps per second: 164, episode reward: 65.000, mean reward:  0.166 [ 0.000, 20.000], mean action: 2.637 [0.000, 5.000],  loss: 39.317908, mean_q: 333.173636, mean_eps: 0.621248\n",
            " 421559/1000000: episode: 616, duration: 3.219s, episode steps: 527, steps per second: 164, episode reward: 85.000, mean reward:  0.161 [ 0.000, 25.000], mean action: 2.342 [0.000, 5.000],  loss: 63.721817, mean_q: 314.173531, mean_eps: 0.620834\n",
            " 422374/1000000: episode: 617, duration: 4.879s, episode steps: 815, steps per second: 167, episode reward: 160.000, mean reward:  0.196 [ 0.000, 30.000], mean action: 2.600 [0.000, 5.000],  loss: 46.901720, mean_q: 304.104295, mean_eps: 0.620231\n",
            " 423114/1000000: episode: 618, duration: 4.463s, episode steps: 740, steps per second: 166, episode reward: 110.000, mean reward:  0.149 [ 0.000, 30.000], mean action: 3.000 [0.000, 5.000],  loss: 26.359722, mean_q: 302.223663, mean_eps: 0.619531\n",
            " 423745/1000000: episode: 619, duration: 3.848s, episode steps: 631, steps per second: 164, episode reward: 135.000, mean reward:  0.214 [ 0.000, 30.000], mean action: 2.658 [0.000, 5.000],  loss: 28.664608, mean_q: 289.014431, mean_eps: 0.618914\n",
            " 425135/1000000: episode: 620, duration: 8.298s, episode steps: 1390, steps per second: 168, episode reward: 260.000, mean reward:  0.187 [ 0.000, 30.000], mean action: 2.591 [0.000, 5.000],  loss: 31.910679, mean_q: 289.110339, mean_eps: 0.618004\n",
            " 425781/1000000: episode: 621, duration: 3.884s, episode steps: 646, steps per second: 166, episode reward: 155.000, mean reward:  0.240 [ 0.000, 30.000], mean action: 2.704 [0.000, 5.000],  loss: 30.878734, mean_q: 280.663349, mean_eps: 0.617088\n",
            " 426555/1000000: episode: 622, duration: 4.617s, episode steps: 774, steps per second: 168, episode reward: 125.000, mean reward:  0.161 [ 0.000, 25.000], mean action: 2.539 [0.000, 5.000],  loss: 26.193558, mean_q: 296.653519, mean_eps: 0.616449\n",
            " 427107/1000000: episode: 623, duration: 3.309s, episode steps: 552, steps per second: 167, episode reward: 165.000, mean reward:  0.299 [ 0.000, 25.000], mean action: 2.317 [0.000, 5.000],  loss: 43.805646, mean_q: 305.163997, mean_eps: 0.615853\n",
            " 427927/1000000: episode: 624, duration: 4.973s, episode steps: 820, steps per second: 165, episode reward: 135.000, mean reward:  0.165 [ 0.000, 30.000], mean action: 2.605 [0.000, 5.000],  loss: 45.884127, mean_q: 293.031532, mean_eps: 0.615235\n",
            " 428309/1000000: episode: 625, duration: 2.357s, episode steps: 382, steps per second: 162, episode reward: 30.000, mean reward:  0.079 [ 0.000, 10.000], mean action: 2.453 [0.000, 5.000],  loss: 31.321414, mean_q: 287.301470, mean_eps: 0.614694\n",
            " 429042/1000000: episode: 626, duration: 4.452s, episode steps: 733, steps per second: 165, episode reward: 120.000, mean reward:  0.164 [ 0.000, 30.000], mean action: 2.387 [0.000, 5.000],  loss: 43.864638, mean_q: 293.065070, mean_eps: 0.614193\n",
            " 429739/1000000: episode: 627, duration: 4.203s, episode steps: 697, steps per second: 166, episode reward: 115.000, mean reward:  0.165 [ 0.000, 25.000], mean action: 2.525 [0.000, 5.000],  loss: 41.669400, mean_q: 295.422298, mean_eps: 0.613549\n",
            " 430521/1000000: episode: 628, duration: 4.699s, episode steps: 782, steps per second: 166, episode reward: 140.000, mean reward:  0.179 [ 0.000, 25.000], mean action: 2.766 [0.000, 5.000],  loss: 33.603825, mean_q: 298.617006, mean_eps: 0.612883\n",
            " 431541/1000000: episode: 629, duration: 6.173s, episode steps: 1020, steps per second: 165, episode reward: 190.000, mean reward:  0.186 [ 0.000, 30.000], mean action: 2.675 [0.000, 5.000],  loss: 25.311178, mean_q: 309.341544, mean_eps: 0.612073\n",
            " 432387/1000000: episode: 630, duration: 5.073s, episode steps: 846, steps per second: 167, episode reward: 145.000, mean reward:  0.171 [ 0.000, 30.000], mean action: 2.629 [0.000, 5.000],  loss: 31.128687, mean_q: 311.648284, mean_eps: 0.611233\n",
            " 433008/1000000: episode: 631, duration: 3.793s, episode steps: 621, steps per second: 164, episode reward: 155.000, mean reward:  0.250 [ 0.000, 30.000], mean action: 2.551 [0.000, 5.000],  loss: 43.498313, mean_q: 312.114437, mean_eps: 0.610573\n",
            " 433522/1000000: episode: 632, duration: 3.143s, episode steps: 514, steps per second: 164, episode reward: 210.000, mean reward:  0.409 [ 0.000, 30.000], mean action: 2.683 [0.000, 5.000],  loss: 45.656989, mean_q: 295.151179, mean_eps: 0.610062\n",
            " 434304/1000000: episode: 633, duration: 4.687s, episode steps: 782, steps per second: 167, episode reward: 130.000, mean reward:  0.166 [ 0.000, 30.000], mean action: 2.446 [0.000, 5.000],  loss: 37.651295, mean_q: 288.470199, mean_eps: 0.609479\n",
            " 434910/1000000: episode: 634, duration: 3.626s, episode steps: 606, steps per second: 167, episode reward: 65.000, mean reward:  0.107 [ 0.000, 20.000], mean action: 2.485 [0.000, 5.000],  loss: 26.113044, mean_q: 287.443948, mean_eps: 0.608854\n",
            " 435857/1000000: episode: 635, duration: 5.662s, episode steps: 947, steps per second: 167, episode reward: 210.000, mean reward:  0.222 [ 0.000, 30.000], mean action: 2.532 [0.000, 5.000],  loss: 21.860818, mean_q: 301.282463, mean_eps: 0.608155\n",
            " 436750/1000000: episode: 636, duration: 5.286s, episode steps: 893, steps per second: 169, episode reward: 240.000, mean reward:  0.269 [ 0.000, 30.000], mean action: 2.522 [0.000, 5.000],  loss: 16.515161, mean_q: 296.000412, mean_eps: 0.607327\n",
            " 437272/1000000: episode: 637, duration: 3.116s, episode steps: 522, steps per second: 168, episode reward: 180.000, mean reward:  0.345 [ 0.000, 30.000], mean action: 2.575 [0.000, 5.000],  loss: 25.342533, mean_q: 289.435624, mean_eps: 0.606691\n",
            " 438729/1000000: episode: 638, duration: 8.587s, episode steps: 1457, steps per second: 170, episode reward: 515.000, mean reward:  0.353 [ 0.000, 30.000], mean action: 2.768 [0.000, 5.000],  loss: 21.256046, mean_q: 299.559014, mean_eps: 0.605800\n",
            " 439116/1000000: episode: 639, duration: 2.299s, episode steps: 387, steps per second: 168, episode reward: 30.000, mean reward:  0.078 [ 0.000, 15.000], mean action: 2.455 [0.000, 5.000],  loss: 30.265994, mean_q: 296.336475, mean_eps: 0.604970\n",
            " 439563/1000000: episode: 640, duration: 2.720s, episode steps: 447, steps per second: 164, episode reward: 90.000, mean reward:  0.201 [ 0.000, 25.000], mean action: 2.427 [0.000, 5.000],  loss: 40.308338, mean_q: 296.573772, mean_eps: 0.604595\n",
            " 440193/1000000: episode: 641, duration: 3.788s, episode steps: 630, steps per second: 166, episode reward: 105.000, mean reward:  0.167 [ 0.000, 30.000], mean action: 2.410 [0.000, 5.000],  loss: 39.899341, mean_q: 301.748317, mean_eps: 0.604110\n",
            " 440872/1000000: episode: 642, duration: 4.090s, episode steps: 679, steps per second: 166, episode reward: 220.000, mean reward:  0.324 [ 0.000, 30.000], mean action: 2.689 [0.000, 5.000],  loss: 38.073606, mean_q: 299.828271, mean_eps: 0.603521\n",
            " 441426/1000000: episode: 643, duration: 3.369s, episode steps: 554, steps per second: 164, episode reward: 140.000, mean reward:  0.253 [ 0.000, 25.000], mean action: 2.495 [0.000, 5.000],  loss: 40.143885, mean_q: 298.861045, mean_eps: 0.602966\n",
            " 441904/1000000: episode: 644, duration: 2.883s, episode steps: 478, steps per second: 166, episode reward: 30.000, mean reward:  0.063 [ 0.000, 15.000], mean action: 2.527 [0.000, 5.000],  loss: 57.991411, mean_q: 295.332268, mean_eps: 0.602502\n",
            " 442535/1000000: episode: 645, duration: 3.771s, episode steps: 631, steps per second: 167, episode reward: 120.000, mean reward:  0.190 [ 0.000, 30.000], mean action: 2.525 [0.000, 5.000],  loss: 35.001361, mean_q: 296.970872, mean_eps: 0.602003\n",
            " 443175/1000000: episode: 646, duration: 3.860s, episode steps: 640, steps per second: 166, episode reward: 85.000, mean reward:  0.133 [ 0.000, 25.000], mean action: 2.406 [0.000, 5.000],  loss: 33.504207, mean_q: 301.943117, mean_eps: 0.601431\n",
            " 443798/1000000: episode: 647, duration: 3.789s, episode steps: 623, steps per second: 164, episode reward: 105.000, mean reward:  0.169 [ 0.000, 25.000], mean action: 2.401 [0.000, 5.000],  loss: 26.281013, mean_q: 299.762895, mean_eps: 0.600863\n",
            " 444600/1000000: episode: 648, duration: 4.883s, episode steps: 802, steps per second: 164, episode reward: 155.000, mean reward:  0.193 [ 0.000, 30.000], mean action: 2.461 [0.000, 5.000],  loss: 36.788743, mean_q: 301.532143, mean_eps: 0.600221\n",
            " 444986/1000000: episode: 649, duration: 2.298s, episode steps: 386, steps per second: 168, episode reward: 45.000, mean reward:  0.117 [ 0.000, 15.000], mean action: 2.251 [0.000, 5.000],  loss: 55.533019, mean_q: 286.139041, mean_eps: 0.599687\n",
            " 445747/1000000: episode: 650, duration: 4.529s, episode steps: 761, steps per second: 168, episode reward: 210.000, mean reward:  0.276 [ 0.000, 30.000], mean action: 2.311 [0.000, 5.000],  loss: 56.798455, mean_q: 286.456407, mean_eps: 0.599171\n",
            " 446184/1000000: episode: 651, duration: 2.600s, episode steps: 437, steps per second: 168, episode reward: 25.000, mean reward:  0.057 [ 0.000, 10.000], mean action: 2.563 [0.000, 5.000],  loss: 31.871610, mean_q: 282.861698, mean_eps: 0.598631\n",
            " 446619/1000000: episode: 652, duration: 2.579s, episode steps: 435, steps per second: 169, episode reward: 75.000, mean reward:  0.172 [ 0.000, 25.000], mean action: 2.402 [0.000, 5.000],  loss: 45.838437, mean_q: 280.034687, mean_eps: 0.598239\n",
            " 447242/1000000: episode: 653, duration: 3.692s, episode steps: 623, steps per second: 169, episode reward: 180.000, mean reward:  0.289 [ 0.000, 30.000], mean action: 2.287 [0.000, 5.000],  loss: 32.961841, mean_q: 294.443161, mean_eps: 0.597763\n",
            " 447895/1000000: episode: 654, duration: 3.895s, episode steps: 653, steps per second: 168, episode reward: 105.000, mean reward:  0.161 [ 0.000, 30.000], mean action: 2.389 [0.000, 5.000],  loss: 25.362503, mean_q: 300.456864, mean_eps: 0.597189\n",
            " 448821/1000000: episode: 655, duration: 5.518s, episode steps: 926, steps per second: 168, episode reward: 155.000, mean reward:  0.167 [ 0.000, 30.000], mean action: 2.662 [0.000, 5.000],  loss: 21.559102, mean_q: 300.331888, mean_eps: 0.596478\n",
            " 449542/1000000: episode: 656, duration: 4.291s, episode steps: 721, steps per second: 168, episode reward: 225.000, mean reward:  0.312 [ 0.000, 30.000], mean action: 2.537 [0.000, 5.000],  loss: 29.678053, mean_q: 301.446754, mean_eps: 0.595737\n",
            " 450428/1000000: episode: 657, duration: 5.252s, episode steps: 886, steps per second: 169, episode reward: 295.000, mean reward:  0.333 [ 0.000, 30.000], mean action: 2.448 [0.000, 5.000],  loss: 24.180748, mean_q: 295.176539, mean_eps: 0.595014\n",
            " 451156/1000000: episode: 658, duration: 4.349s, episode steps: 728, steps per second: 167, episode reward: 110.000, mean reward:  0.151 [ 0.000, 30.000], mean action: 2.771 [0.000, 5.000],  loss: 16.389543, mean_q: 294.447090, mean_eps: 0.594288\n",
            " 451735/1000000: episode: 659, duration: 3.443s, episode steps: 579, steps per second: 168, episode reward: 105.000, mean reward:  0.181 [ 0.000, 30.000], mean action: 2.509 [0.000, 5.000],  loss: 22.737865, mean_q: 294.564674, mean_eps: 0.593699\n",
            " 452419/1000000: episode: 660, duration: 4.064s, episode steps: 684, steps per second: 168, episode reward: 95.000, mean reward:  0.139 [ 0.000, 30.000], mean action: 2.044 [0.000, 5.000],  loss: 23.388733, mean_q: 294.082015, mean_eps: 0.593131\n",
            " 452821/1000000: episode: 661, duration: 2.384s, episode steps: 402, steps per second: 169, episode reward: 30.000, mean reward:  0.075 [ 0.000, 15.000], mean action: 2.475 [0.000, 5.000],  loss: 33.459148, mean_q: 291.545652, mean_eps: 0.592642\n",
            " 453564/1000000: episode: 662, duration: 4.429s, episode steps: 743, steps per second: 168, episode reward: 110.000, mean reward:  0.148 [ 0.000, 20.000], mean action: 2.420 [0.000, 5.000],  loss: 47.528050, mean_q: 296.579935, mean_eps: 0.592127\n",
            " 454669/1000000: episode: 663, duration: 6.598s, episode steps: 1105, steps per second: 167, episode reward: 475.000, mean reward:  0.430 [ 0.000, 200.000], mean action: 2.356 [0.000, 5.000],  loss: 47.234597, mean_q: 300.227478, mean_eps: 0.591296\n",
            " 455209/1000000: episode: 664, duration: 3.241s, episode steps: 540, steps per second: 167, episode reward: 40.000, mean reward:  0.074 [ 0.000, 15.000], mean action: 2.452 [0.000, 5.000],  loss: 46.135005, mean_q: 299.126699, mean_eps: 0.590555\n",
            " 455942/1000000: episode: 665, duration: 4.399s, episode steps: 733, steps per second: 167, episode reward: 135.000, mean reward:  0.184 [ 0.000, 30.000], mean action: 2.513 [0.000, 5.000],  loss: 31.762225, mean_q: 297.960750, mean_eps: 0.589982\n",
            " 457548/1000000: episode: 666, duration: 9.464s, episode steps: 1606, steps per second: 170, episode reward: 775.000, mean reward:  0.483 [ 0.000, 200.000], mean action: 2.151 [0.000, 5.000],  loss: 38.678094, mean_q: 300.626330, mean_eps: 0.588930\n",
            " 458199/1000000: episode: 667, duration: 3.850s, episode steps: 651, steps per second: 169, episode reward: 230.000, mean reward:  0.353 [ 0.000, 30.000], mean action: 2.518 [0.000, 5.000],  loss: 64.201924, mean_q: 287.307823, mean_eps: 0.587914\n",
            " 459109/1000000: episode: 668, duration: 5.441s, episode steps: 910, steps per second: 167, episode reward: 225.000, mean reward:  0.247 [ 0.000, 30.000], mean action: 2.566 [0.000, 5.000],  loss: 24.130961, mean_q: 302.287402, mean_eps: 0.587212\n",
            " 459465/1000000: episode: 669, duration: 2.133s, episode steps: 356, steps per second: 167, episode reward: 35.000, mean reward:  0.098 [ 0.000, 15.000], mean action: 2.685 [0.000, 5.000],  loss: 21.119541, mean_q: 318.157085, mean_eps: 0.586642\n",
            " 460449/1000000: episode: 670, duration: 5.871s, episode steps: 984, steps per second: 168, episode reward: 275.000, mean reward:  0.279 [ 0.000, 30.000], mean action: 2.225 [0.000, 5.000],  loss: 60.912502, mean_q: 308.151123, mean_eps: 0.586039\n",
            " 461093/1000000: episode: 671, duration: 3.856s, episode steps: 644, steps per second: 167, episode reward: 185.000, mean reward:  0.287 [ 0.000, 30.000], mean action: 2.658 [0.000, 5.000],  loss: 40.402099, mean_q: 286.335491, mean_eps: 0.585307\n",
            " 461619/1000000: episode: 672, duration: 3.202s, episode steps: 526, steps per second: 164, episode reward: 130.000, mean reward:  0.247 [ 0.000, 30.000], mean action: 2.681 [0.000, 5.000],  loss: 35.516610, mean_q: 295.748555, mean_eps: 0.584780\n",
            " 462519/1000000: episode: 673, duration: 5.356s, episode steps: 900, steps per second: 168, episode reward: 315.000, mean reward:  0.350 [ 0.000, 30.000], mean action: 2.496 [0.000, 5.000],  loss: 32.318735, mean_q: 292.915255, mean_eps: 0.584138\n",
            " 463139/1000000: episode: 674, duration: 3.730s, episode steps: 620, steps per second: 166, episode reward: 105.000, mean reward:  0.169 [ 0.000, 25.000], mean action: 2.515 [0.000, 5.000],  loss: 31.819661, mean_q: 273.782727, mean_eps: 0.583454\n",
            " 463841/1000000: episode: 675, duration: 4.223s, episode steps: 702, steps per second: 166, episode reward: 50.000, mean reward:  0.071 [ 0.000, 15.000], mean action: 2.472 [0.000, 5.000],  loss: 42.752936, mean_q: 284.242226, mean_eps: 0.582859\n",
            " 465160/1000000: episode: 676, duration: 7.954s, episode steps: 1319, steps per second: 166, episode reward: 515.000, mean reward:  0.390 [ 0.000, 200.000], mean action: 2.609 [0.000, 5.000],  loss: 39.953450, mean_q: 306.465810, mean_eps: 0.581950\n",
            " 465744/1000000: episode: 677, duration: 3.557s, episode steps: 584, steps per second: 164, episode reward: 150.000, mean reward:  0.257 [ 0.000, 25.000], mean action: 2.529 [0.000, 5.000],  loss: 103.615940, mean_q: 281.578767, mean_eps: 0.581094\n",
            " 466237/1000000: episode: 678, duration: 2.993s, episode steps: 493, steps per second: 165, episode reward: 150.000, mean reward:  0.304 [ 0.000, 25.000], mean action: 2.379 [0.000, 5.000],  loss: 78.756891, mean_q: 288.114400, mean_eps: 0.580609\n",
            " 467108/1000000: episode: 679, duration: 5.270s, episode steps: 871, steps per second: 165, episode reward: 185.000, mean reward:  0.212 [ 0.000, 30.000], mean action: 2.509 [0.000, 5.000],  loss: 46.983992, mean_q: 293.238245, mean_eps: 0.579995\n",
            " 467759/1000000: episode: 680, duration: 3.912s, episode steps: 651, steps per second: 166, episode reward: 140.000, mean reward:  0.215 [ 0.000, 30.000], mean action: 2.510 [0.000, 5.000],  loss: 49.896888, mean_q: 276.930047, mean_eps: 0.579310\n",
            " 468104/1000000: episode: 681, duration: 2.119s, episode steps: 345, steps per second: 163, episode reward: 35.000, mean reward:  0.101 [ 0.000, 15.000], mean action: 2.357 [0.000, 5.000],  loss: 36.494393, mean_q: 282.858786, mean_eps: 0.578862\n",
            " 468986/1000000: episode: 682, duration: 5.315s, episode steps: 882, steps per second: 166, episode reward: 100.000, mean reward:  0.113 [ 0.000, 20.000], mean action: 2.357 [0.000, 5.000],  loss: 67.461366, mean_q: 299.854865, mean_eps: 0.578310\n",
            " 469685/1000000: episode: 683, duration: 4.215s, episode steps: 699, steps per second: 166, episode reward: 105.000, mean reward:  0.150 [ 0.000, 30.000], mean action: 2.565 [0.000, 5.000],  loss: 43.184866, mean_q: 306.694238, mean_eps: 0.577598\n",
            " 470257/1000000: episode: 684, duration: 3.417s, episode steps: 572, steps per second: 167, episode reward: 100.000, mean reward:  0.175 [ 0.000, 20.000], mean action: 2.238 [0.000, 5.000],  loss: 26.572324, mean_q: 294.870993, mean_eps: 0.577027\n",
            " 470697/1000000: episode: 685, duration: 2.711s, episode steps: 440, steps per second: 162, episode reward: 70.000, mean reward:  0.159 [ 0.000, 20.000], mean action: 2.520 [0.000, 5.000],  loss: 40.057982, mean_q: 290.420923, mean_eps: 0.576571\n",
            " 471097/1000000: episode: 686, duration: 2.400s, episode steps: 400, steps per second: 167, episode reward: 90.000, mean reward:  0.225 [ 0.000, 25.000], mean action: 2.362 [0.000, 5.000],  loss: 52.559583, mean_q: 296.663374, mean_eps: 0.576193\n",
            " 471808/1000000: episode: 687, duration: 4.271s, episode steps: 711, steps per second: 166, episode reward: 175.000, mean reward:  0.246 [ 0.000, 30.000], mean action: 2.624 [0.000, 5.000],  loss: 46.885118, mean_q: 298.681015, mean_eps: 0.575693\n",
            " 472624/1000000: episode: 688, duration: 4.864s, episode steps: 816, steps per second: 168, episode reward: 145.000, mean reward:  0.178 [ 0.000, 30.000], mean action: 3.011 [0.000, 5.000],  loss: 32.210517, mean_q: 287.931268, mean_eps: 0.575006\n",
            " 473408/1000000: episode: 689, duration: 4.664s, episode steps: 784, steps per second: 168, episode reward: 180.000, mean reward:  0.230 [ 0.000, 30.000], mean action: 2.190 [0.000, 5.000],  loss: 19.598071, mean_q: 280.998513, mean_eps: 0.574286\n",
            " 473997/1000000: episode: 690, duration: 3.576s, episode steps: 589, steps per second: 165, episode reward: 120.000, mean reward:  0.204 [ 0.000, 30.000], mean action: 2.081 [0.000, 5.000],  loss: 24.306844, mean_q: 276.957315, mean_eps: 0.573668\n",
            " 475008/1000000: episode: 691, duration: 6.049s, episode steps: 1011, steps per second: 167, episode reward: 210.000, mean reward:  0.208 [ 0.000, 30.000], mean action: 2.546 [0.000, 5.000],  loss: 29.728354, mean_q: 282.612207, mean_eps: 0.572948\n",
            " 476047/1000000: episode: 692, duration: 6.198s, episode steps: 1039, steps per second: 168, episode reward: 455.000, mean reward:  0.438 [ 0.000, 30.000], mean action: 2.418 [0.000, 5.000],  loss: 38.424275, mean_q: 291.611886, mean_eps: 0.572026\n",
            " 476439/1000000: episode: 693, duration: 2.365s, episode steps: 392, steps per second: 166, episode reward: 55.000, mean reward:  0.140 [ 0.000, 20.000], mean action: 2.495 [0.000, 5.000],  loss: 28.103399, mean_q: 315.377917, mean_eps: 0.571382\n",
            " 477299/1000000: episode: 694, duration: 5.113s, episode steps: 860, steps per second: 168, episode reward: 455.000, mean reward:  0.529 [ 0.000, 200.000], mean action: 2.471 [0.000, 5.000],  loss: 52.956130, mean_q: 298.715715, mean_eps: 0.570818\n",
            " 477973/1000000: episode: 695, duration: 4.025s, episode steps: 674, steps per second: 167, episode reward: 60.000, mean reward:  0.089 [ 0.000, 20.000], mean action: 2.638 [0.000, 5.000],  loss: 51.234006, mean_q: 294.366687, mean_eps: 0.570128\n",
            " 478983/1000000: episode: 696, duration: 6.054s, episode steps: 1010, steps per second: 167, episode reward: 245.000, mean reward:  0.243 [ 0.000, 30.000], mean action: 2.796 [0.000, 5.000],  loss: 49.922431, mean_q: 279.649302, mean_eps: 0.569370\n",
            " 479388/1000000: episode: 697, duration: 2.453s, episode steps: 405, steps per second: 165, episode reward: 100.000, mean reward:  0.247 [ 0.000, 20.000], mean action: 2.301 [0.000, 5.000],  loss: 46.793182, mean_q: 273.444745, mean_eps: 0.568734\n",
            " 480216/1000000: episode: 698, duration: 4.939s, episode steps: 828, steps per second: 168, episode reward: 210.000, mean reward:  0.254 [ 0.000, 30.000], mean action: 2.620 [0.000, 5.000],  loss: 57.304054, mean_q: 282.938406, mean_eps: 0.568179\n",
            " 480894/1000000: episode: 699, duration: 4.028s, episode steps: 678, steps per second: 168, episode reward: 105.000, mean reward:  0.155 [ 0.000, 30.000], mean action: 2.504 [0.000, 5.000],  loss: 47.505511, mean_q: 269.718826, mean_eps: 0.567501\n",
            " 481432/1000000: episode: 700, duration: 3.257s, episode steps: 538, steps per second: 165, episode reward: 80.000, mean reward:  0.149 [ 0.000, 25.000], mean action: 2.266 [0.000, 5.000],  loss: 33.743093, mean_q: 280.759880, mean_eps: 0.566954\n",
            " 481820/1000000: episode: 701, duration: 2.387s, episode steps: 388, steps per second: 163, episode reward: 60.000, mean reward:  0.155 [ 0.000, 30.000], mean action: 2.338 [0.000, 5.000],  loss: 42.393664, mean_q: 285.951697, mean_eps: 0.566537\n",
            " 482222/1000000: episode: 702, duration: 2.405s, episode steps: 402, steps per second: 167, episode reward: 55.000, mean reward:  0.137 [ 0.000, 20.000], mean action: 2.356 [0.000, 5.000],  loss: 103.531109, mean_q: 294.762526, mean_eps: 0.566182\n",
            " 482914/1000000: episode: 703, duration: 4.144s, episode steps: 692, steps per second: 167, episode reward: 110.000, mean reward:  0.159 [ 0.000, 30.000], mean action: 2.684 [0.000, 5.000],  loss: 62.634981, mean_q: 298.113172, mean_eps: 0.565689\n",
            " 483264/1000000: episode: 704, duration: 2.085s, episode steps: 350, steps per second: 168, episode reward: 65.000, mean reward:  0.186 [ 0.000, 30.000], mean action: 2.526 [0.000, 5.000],  loss: 42.166860, mean_q: 296.377930, mean_eps: 0.565220\n",
            " 483956/1000000: episode: 705, duration: 4.126s, episode steps: 692, steps per second: 168, episode reward: 320.000, mean reward:  0.462 [ 0.000, 200.000], mean action: 2.523 [0.000, 5.000],  loss: 45.390956, mean_q: 294.973031, mean_eps: 0.564751\n",
            " 485084/1000000: episode: 706, duration: 6.684s, episode steps: 1128, steps per second: 169, episode reward: 455.000, mean reward:  0.403 [ 0.000, 200.000], mean action: 2.676 [0.000, 5.000],  loss: 42.136858, mean_q: 297.050856, mean_eps: 0.563932\n",
            " 485592/1000000: episode: 707, duration: 3.013s, episode steps: 508, steps per second: 169, episode reward: 30.000, mean reward:  0.059 [ 0.000, 10.000], mean action: 2.506 [0.000, 5.000],  loss: 92.899949, mean_q: 294.631261, mean_eps: 0.563196\n",
            " 486070/1000000: episode: 708, duration: 2.874s, episode steps: 478, steps per second: 166, episode reward: 30.000, mean reward:  0.063 [ 0.000, 20.000], mean action: 1.969 [0.000, 5.000],  loss: 85.482545, mean_q: 293.398215, mean_eps: 0.562753\n",
            " 487222/1000000: episode: 709, duration: 6.841s, episode steps: 1152, steps per second: 168, episode reward: 260.000, mean reward:  0.226 [ 0.000, 30.000], mean action: 2.455 [0.000, 5.000],  loss: 35.368918, mean_q: 293.451996, mean_eps: 0.562019\n",
            " 487671/1000000: episode: 710, duration: 2.665s, episode steps: 449, steps per second: 168, episode reward: 35.000, mean reward:  0.078 [ 0.000, 15.000], mean action: 2.125 [0.000, 5.000],  loss: 39.320806, mean_q: 309.575052, mean_eps: 0.561299\n",
            " 488360/1000000: episode: 711, duration: 4.094s, episode steps: 689, steps per second: 168, episode reward: 135.000, mean reward:  0.196 [ 0.000, 30.000], mean action: 2.472 [0.000, 5.000],  loss: 54.194842, mean_q: 299.152353, mean_eps: 0.560786\n",
            " 489061/1000000: episode: 712, duration: 4.166s, episode steps: 701, steps per second: 168, episode reward: 45.000, mean reward:  0.064 [ 0.000, 15.000], mean action: 2.772 [0.000, 5.000],  loss: 39.778741, mean_q: 281.881619, mean_eps: 0.560161\n",
            " 489962/1000000: episode: 713, duration: 5.301s, episode steps: 901, steps per second: 170, episode reward: 140.000, mean reward:  0.155 [ 0.000, 20.000], mean action: 2.932 [0.000, 5.000],  loss: 19.683115, mean_q: 292.038666, mean_eps: 0.559440\n",
            " 490804/1000000: episode: 714, duration: 5.052s, episode steps: 842, steps per second: 167, episode reward: 225.000, mean reward:  0.267 [ 0.000, 30.000], mean action: 2.546 [0.000, 5.000],  loss: 19.497309, mean_q: 304.908111, mean_eps: 0.558656\n",
            " 491488/1000000: episode: 715, duration: 4.069s, episode steps: 684, steps per second: 168, episode reward: 155.000, mean reward:  0.227 [ 0.000, 30.000], mean action: 2.392 [0.000, 5.000],  loss: 36.006375, mean_q: 273.009869, mean_eps: 0.557969\n",
            " 492149/1000000: episode: 716, duration: 3.902s, episode steps: 661, steps per second: 169, episode reward: 120.000, mean reward:  0.182 [ 0.000, 30.000], mean action: 2.460 [0.000, 5.000],  loss: 29.593883, mean_q: 274.990874, mean_eps: 0.557364\n",
            " 492675/1000000: episode: 717, duration: 3.154s, episode steps: 526, steps per second: 167, episode reward: 80.000, mean reward:  0.152 [ 0.000, 25.000], mean action: 2.500 [0.000, 5.000],  loss: 22.193544, mean_q: 285.111310, mean_eps: 0.556830\n",
            " 493477/1000000: episode: 718, duration: 4.743s, episode steps: 802, steps per second: 169, episode reward: 195.000, mean reward:  0.243 [ 0.000, 30.000], mean action: 2.763 [0.000, 5.000],  loss: 25.278506, mean_q: 296.010121, mean_eps: 0.556232\n",
            " 494281/1000000: episode: 719, duration: 4.782s, episode steps: 804, steps per second: 168, episode reward: 195.000, mean reward:  0.243 [ 0.000, 30.000], mean action: 2.454 [0.000, 5.000],  loss: 30.888286, mean_q: 301.410783, mean_eps: 0.555509\n",
            " 494727/1000000: episode: 720, duration: 2.643s, episode steps: 446, steps per second: 169, episode reward: 25.000, mean reward:  0.056 [ 0.000, 10.000], mean action: 2.379 [0.000, 5.000],  loss: 48.300849, mean_q: 304.113248, mean_eps: 0.554947\n",
            " 495346/1000000: episode: 721, duration: 3.718s, episode steps: 619, steps per second: 166, episode reward: 135.000, mean reward:  0.218 [ 0.000, 30.000], mean action: 2.468 [0.000, 5.000],  loss: 40.360319, mean_q: 299.247165, mean_eps: 0.554468\n",
            " 496293/1000000: episode: 722, duration: 5.669s, episode steps: 947, steps per second: 167, episode reward: 210.000, mean reward:  0.222 [ 0.000, 30.000], mean action: 2.526 [0.000, 5.000],  loss: 41.952001, mean_q: 302.205728, mean_eps: 0.553763\n",
            " 496974/1000000: episode: 723, duration: 4.103s, episode steps: 681, steps per second: 166, episode reward: 135.000, mean reward:  0.198 [ 0.000, 30.000], mean action: 2.514 [0.000, 5.000],  loss: 21.385705, mean_q: 301.663384, mean_eps: 0.553030\n",
            " 497602/1000000: episode: 724, duration: 3.708s, episode steps: 628, steps per second: 169, episode reward: 75.000, mean reward:  0.119 [ 0.000, 25.000], mean action: 2.382 [0.000, 5.000],  loss: 26.212757, mean_q: 290.368041, mean_eps: 0.552441\n",
            " 498579/1000000: episode: 725, duration: 5.837s, episode steps: 977, steps per second: 167, episode reward: 260.000, mean reward:  0.266 [ 0.000, 30.000], mean action: 2.629 [0.000, 5.000],  loss: 25.484848, mean_q: 292.663395, mean_eps: 0.551719\n",
            " 499221/1000000: episode: 726, duration: 3.861s, episode steps: 642, steps per second: 166, episode reward: 180.000, mean reward:  0.280 [ 0.000, 30.000], mean action: 2.456 [0.000, 5.000],  loss: 42.856556, mean_q: 266.367782, mean_eps: 0.550990\n",
            " 499863/1000000: episode: 727, duration: 3.822s, episode steps: 642, steps per second: 168, episode reward: 155.000, mean reward:  0.241 [ 0.000, 30.000], mean action: 2.645 [0.000, 5.000],  loss: 27.221868, mean_q: 275.635001, mean_eps: 0.550413\n",
            " 500359/1000000: episode: 728, duration: 2.981s, episode steps: 496, steps per second: 166, episode reward: 45.000, mean reward:  0.091 [ 0.000, 15.000], mean action: 2.706 [0.000, 5.000],  loss: 26.112886, mean_q: 289.271755, mean_eps: 0.549901\n",
            " 501418/1000000: episode: 729, duration: 6.320s, episode steps: 1059, steps per second: 168, episode reward: 440.000, mean reward:  0.415 [ 0.000, 200.000], mean action: 2.487 [0.000, 5.000],  loss: 39.657198, mean_q: 293.919486, mean_eps: 0.549201\n",
            " 502064/1000000: episode: 730, duration: 3.868s, episode steps: 646, steps per second: 167, episode reward: 65.000, mean reward:  0.101 [ 0.000, 20.000], mean action: 2.539 [0.000, 5.000],  loss: 59.814825, mean_q: 291.419219, mean_eps: 0.548434\n",
            " 502717/1000000: episode: 731, duration: 3.921s, episode steps: 653, steps per second: 167, episode reward: 135.000, mean reward:  0.207 [ 0.000, 30.000], mean action: 2.636 [0.000, 5.000],  loss: 27.733860, mean_q: 290.283188, mean_eps: 0.547849\n",
            " 503738/1000000: episode: 732, duration: 6.112s, episode steps: 1021, steps per second: 167, episode reward: 425.000, mean reward:  0.416 [ 0.000, 200.000], mean action: 2.351 [0.000, 5.000],  loss: 20.676323, mean_q: 296.781988, mean_eps: 0.547096\n",
            " 504688/1000000: episode: 733, duration: 5.730s, episode steps: 950, steps per second: 166, episode reward: 105.000, mean reward:  0.111 [ 0.000, 30.000], mean action: 2.591 [0.000, 5.000],  loss: 34.574586, mean_q: 304.124662, mean_eps: 0.546209\n",
            " 506140/1000000: episode: 734, duration: 8.580s, episode steps: 1452, steps per second: 169, episode reward: 440.000, mean reward:  0.303 [ 0.000, 30.000], mean action: 2.494 [0.000, 5.000],  loss: 36.203976, mean_q: 280.023497, mean_eps: 0.545128\n",
            " 506542/1000000: episode: 735, duration: 2.425s, episode steps: 402, steps per second: 166, episode reward: 35.000, mean reward:  0.087 [ 0.000, 10.000], mean action: 2.361 [0.000, 5.000],  loss: 50.507827, mean_q: 264.182145, mean_eps: 0.544294\n",
            " 507140/1000000: episode: 736, duration: 3.576s, episode steps: 598, steps per second: 167, episode reward: 90.000, mean reward:  0.151 [ 0.000, 25.000], mean action: 2.520 [0.000, 5.000],  loss: 58.634431, mean_q: 287.919539, mean_eps: 0.543844\n",
            " 507962/1000000: episode: 737, duration: 4.891s, episode steps: 822, steps per second: 168, episode reward: 180.000, mean reward:  0.219 [ 0.000, 30.000], mean action: 2.698 [0.000, 5.000],  loss: 41.795009, mean_q: 288.867186, mean_eps: 0.543205\n",
            " 508519/1000000: episode: 738, duration: 3.333s, episode steps: 557, steps per second: 167, episode reward: 30.000, mean reward:  0.054 [ 0.000, 10.000], mean action: 2.368 [0.000, 5.000],  loss: 34.650406, mean_q: 273.467663, mean_eps: 0.542584\n",
            " 509408/1000000: episode: 739, duration: 5.321s, episode steps: 889, steps per second: 167, episode reward: 170.000, mean reward:  0.191 [ 0.000, 30.000], mean action: 2.472 [0.000, 5.000],  loss: 31.197492, mean_q: 287.557203, mean_eps: 0.541933\n",
            " 510157/1000000: episode: 740, duration: 4.465s, episode steps: 749, steps per second: 168, episode reward: 200.000, mean reward:  0.267 [ 0.000, 30.000], mean action: 2.490 [0.000, 5.000],  loss: 42.772979, mean_q: 291.135125, mean_eps: 0.541196\n",
            " 510641/1000000: episode: 741, duration: 2.922s, episode steps: 484, steps per second: 166, episode reward: 75.000, mean reward:  0.155 [ 0.000, 30.000], mean action: 2.450 [0.000, 5.000],  loss: 43.229297, mean_q: 279.975954, mean_eps: 0.540641\n",
            " 511272/1000000: episode: 742, duration: 3.771s, episode steps: 631, steps per second: 167, episode reward: 80.000, mean reward:  0.127 [ 0.000, 20.000], mean action: 2.547 [0.000, 5.000],  loss: 38.008800, mean_q: 284.825155, mean_eps: 0.540140\n",
            " 512010/1000000: episode: 743, duration: 4.444s, episode steps: 738, steps per second: 166, episode reward: 195.000, mean reward:  0.264 [ 0.000, 30.000], mean action: 2.538 [0.000, 5.000],  loss: 29.148677, mean_q: 291.388624, mean_eps: 0.539524\n",
            " 513378/1000000: episode: 744, duration: 8.145s, episode steps: 1368, steps per second: 168, episode reward: 360.000, mean reward:  0.263 [ 0.000, 30.000], mean action: 2.526 [0.000, 5.000],  loss: 28.873767, mean_q: 300.154463, mean_eps: 0.538576\n",
            " 513950/1000000: episode: 745, duration: 3.427s, episode steps: 572, steps per second: 167, episode reward: 70.000, mean reward:  0.122 [ 0.000, 20.000], mean action: 2.524 [0.000, 5.000],  loss: 70.077339, mean_q: 284.673137, mean_eps: 0.537703\n",
            " 514606/1000000: episode: 746, duration: 3.936s, episode steps: 656, steps per second: 167, episode reward: 95.000, mean reward:  0.145 [ 0.000, 20.000], mean action: 2.323 [0.000, 5.000],  loss: 46.088271, mean_q: 284.816080, mean_eps: 0.537150\n",
            " 515613/1000000: episode: 747, duration: 5.994s, episode steps: 1007, steps per second: 168, episode reward: 320.000, mean reward:  0.318 [ 0.000, 30.000], mean action: 2.208 [0.000, 5.000],  loss: 35.438994, mean_q: 297.624142, mean_eps: 0.536402\n",
            " 516316/1000000: episode: 748, duration: 4.211s, episode steps: 703, steps per second: 167, episode reward: 105.000, mean reward:  0.149 [ 0.000, 30.000], mean action: 2.154 [0.000, 5.000],  loss: 18.540835, mean_q: 305.619668, mean_eps: 0.535632\n",
            " 516767/1000000: episode: 749, duration: 2.683s, episode steps: 451, steps per second: 168, episode reward: 60.000, mean reward:  0.133 [ 0.000, 25.000], mean action: 3.027 [0.000, 5.000],  loss: 27.001133, mean_q: 291.386880, mean_eps: 0.535113\n",
            " 517441/1000000: episode: 750, duration: 4.044s, episode steps: 674, steps per second: 167, episode reward: 130.000, mean reward:  0.193 [ 0.000, 30.000], mean action: 2.653 [0.000, 5.000],  loss: 32.995624, mean_q: 292.070124, mean_eps: 0.534607\n",
            " 518234/1000000: episode: 751, duration: 4.704s, episode steps: 793, steps per second: 169, episode reward: 285.000, mean reward:  0.359 [ 0.000, 30.000], mean action: 2.097 [0.000, 5.000],  loss: 32.344177, mean_q: 285.250941, mean_eps: 0.533947\n",
            " 518907/1000000: episode: 752, duration: 4.038s, episode steps: 673, steps per second: 167, episode reward: 105.000, mean reward:  0.156 [ 0.000, 15.000], mean action: 2.545 [0.000, 5.000],  loss: 31.293692, mean_q: 285.955698, mean_eps: 0.533287\n",
            " 519299/1000000: episode: 753, duration: 2.335s, episode steps: 392, steps per second: 168, episode reward: 45.000, mean reward:  0.115 [ 0.000, 15.000], mean action: 2.566 [0.000, 5.000],  loss: 34.050582, mean_q: 290.005979, mean_eps: 0.532808\n",
            " 519715/1000000: episode: 754, duration: 2.518s, episode steps: 416, steps per second: 165, episode reward: 30.000, mean reward:  0.072 [ 0.000, 15.000], mean action: 2.541 [0.000, 5.000],  loss: 54.551209, mean_q: 290.078714, mean_eps: 0.532444\n",
            " 520273/1000000: episode: 755, duration: 3.319s, episode steps: 558, steps per second: 168, episode reward: 135.000, mean reward:  0.242 [ 0.000, 30.000], mean action: 2.486 [0.000, 5.000],  loss: 61.097444, mean_q: 290.805892, mean_eps: 0.532006\n",
            " 520837/1000000: episode: 756, duration: 3.428s, episode steps: 564, steps per second: 165, episode reward: 150.000, mean reward:  0.266 [ 0.000, 20.000], mean action: 2.282 [0.000, 5.000],  loss: 45.285112, mean_q: 286.646511, mean_eps: 0.531501\n",
            " 521525/1000000: episode: 757, duration: 4.133s, episode steps: 688, steps per second: 166, episode reward: 70.000, mean reward:  0.102 [ 0.000, 15.000], mean action: 2.219 [0.000, 5.000],  loss: 49.039988, mean_q: 289.287026, mean_eps: 0.530938\n",
            " 522170/1000000: episode: 758, duration: 3.856s, episode steps: 645, steps per second: 167, episode reward: 40.000, mean reward:  0.062 [ 0.000, 10.000], mean action: 2.349 [0.000, 5.000],  loss: 41.493798, mean_q: 289.742674, mean_eps: 0.530338\n",
            " 522804/1000000: episode: 759, duration: 3.837s, episode steps: 634, steps per second: 165, episode reward: 105.000, mean reward:  0.166 [ 0.000, 30.000], mean action: 2.347 [0.000, 5.000],  loss: 37.750116, mean_q: 290.987132, mean_eps: 0.529762\n",
            " 523679/1000000: episode: 760, duration: 5.225s, episode steps: 875, steps per second: 167, episode reward: 215.000, mean reward:  0.246 [ 0.000, 30.000], mean action: 2.718 [0.000, 5.000],  loss: 35.573446, mean_q: 284.564326, mean_eps: 0.529083\n",
            " 524165/1000000: episode: 761, duration: 2.888s, episode steps: 486, steps per second: 168, episode reward: 80.000, mean reward:  0.165 [ 0.000, 25.000], mean action: 2.821 [0.000, 5.000],  loss: 32.399515, mean_q: 270.249941, mean_eps: 0.528471\n",
            " 524812/1000000: episode: 762, duration: 3.847s, episode steps: 647, steps per second: 168, episode reward: 170.000, mean reward:  0.263 [ 0.000, 30.000], mean action: 2.439 [0.000, 5.000],  loss: 37.347393, mean_q: 280.305513, mean_eps: 0.527961\n",
            " 525395/1000000: episode: 763, duration: 3.522s, episode steps: 583, steps per second: 166, episode reward: 40.000, mean reward:  0.069 [ 0.000, 20.000], mean action: 2.401 [0.000, 5.000],  loss: 33.480200, mean_q: 285.759512, mean_eps: 0.527407\n",
            " 525893/1000000: episode: 764, duration: 2.941s, episode steps: 498, steps per second: 169, episode reward: 45.000, mean reward:  0.090 [ 0.000, 15.000], mean action: 2.277 [0.000, 5.000],  loss: 37.692055, mean_q: 287.837438, mean_eps: 0.526921\n",
            " 526784/1000000: episode: 765, duration: 5.258s, episode steps: 891, steps per second: 169, episode reward: 210.000, mean reward:  0.236 [ 0.000, 30.000], mean action: 2.632 [0.000, 5.000],  loss: 45.543818, mean_q: 289.098562, mean_eps: 0.526296\n",
            " 527743/1000000: episode: 766, duration: 5.722s, episode steps: 959, steps per second: 168, episode reward: 240.000, mean reward:  0.250 [ 0.000, 30.000], mean action: 2.597 [0.000, 5.000],  loss: 21.607418, mean_q: 272.172377, mean_eps: 0.525463\n",
            " 528348/1000000: episode: 767, duration: 3.576s, episode steps: 605, steps per second: 169, episode reward: 120.000, mean reward:  0.198 [ 0.000, 30.000], mean action: 2.608 [0.000, 5.000],  loss: 14.620392, mean_q: 283.020345, mean_eps: 0.524759\n",
            " 529158/1000000: episode: 768, duration: 4.797s, episode steps: 810, steps per second: 169, episode reward: 185.000, mean reward:  0.228 [ 0.000, 30.000], mean action: 2.549 [0.000, 5.000],  loss: 21.882386, mean_q: 293.254534, mean_eps: 0.524123\n",
            " 529784/1000000: episode: 769, duration: 3.720s, episode steps: 626, steps per second: 168, episode reward: 135.000, mean reward:  0.216 [ 0.000, 30.000], mean action: 2.460 [0.000, 5.000],  loss: 34.608935, mean_q: 292.601672, mean_eps: 0.523477\n",
            " 530618/1000000: episode: 770, duration: 4.958s, episode steps: 834, steps per second: 168, episode reward: 235.000, mean reward:  0.282 [ 0.000, 30.000], mean action: 2.458 [0.000, 5.000],  loss: 31.488514, mean_q: 288.186662, mean_eps: 0.522820\n",
            " 531290/1000000: episode: 771, duration: 4.028s, episode steps: 672, steps per second: 167, episode reward: 110.000, mean reward:  0.164 [ 0.000, 30.000], mean action: 2.307 [0.000, 5.000],  loss: 18.841938, mean_q: 274.607749, mean_eps: 0.522142\n",
            " 531682/1000000: episode: 772, duration: 2.373s, episode steps: 392, steps per second: 165, episode reward: 50.000, mean reward:  0.128 [ 0.000, 20.000], mean action: 2.768 [0.000, 5.000],  loss: 22.891197, mean_q: 281.895894, mean_eps: 0.521663\n",
            " 532496/1000000: episode: 773, duration: 4.828s, episode steps: 814, steps per second: 169, episode reward: 215.000, mean reward:  0.264 [ 0.000, 30.000], mean action: 2.377 [0.000, 5.000],  loss: 29.731636, mean_q: 288.101143, mean_eps: 0.521120\n",
            " 532892/1000000: episode: 774, duration: 2.407s, episode steps: 396, steps per second: 165, episode reward: 105.000, mean reward:  0.265 [ 0.000, 30.000], mean action: 2.432 [0.000, 5.000],  loss: 17.417712, mean_q: 283.651932, mean_eps: 0.520576\n",
            " 533556/1000000: episode: 775, duration: 3.958s, episode steps: 664, steps per second: 168, episode reward: 110.000, mean reward:  0.166 [ 0.000, 30.000], mean action: 2.715 [0.000, 5.000],  loss: 36.970523, mean_q: 285.966100, mean_eps: 0.520099\n",
            " 534846/1000000: episode: 776, duration: 7.661s, episode steps: 1290, steps per second: 168, episode reward: 385.000, mean reward:  0.298 [ 0.000, 30.000], mean action: 2.171 [0.000, 5.000],  loss: 44.146623, mean_q: 301.490740, mean_eps: 0.519220\n",
            " 535255/1000000: episode: 777, duration: 2.413s, episode steps: 409, steps per second: 169, episode reward: 55.000, mean reward:  0.134 [ 0.000, 20.000], mean action: 2.521 [0.000, 5.000],  loss: 47.827097, mean_q: 301.110057, mean_eps: 0.518455\n",
            " 535848/1000000: episode: 778, duration: 3.550s, episode steps: 593, steps per second: 167, episode reward: 115.000, mean reward:  0.194 [ 0.000, 25.000], mean action: 2.356 [0.000, 5.000],  loss: 67.256724, mean_q: 287.186687, mean_eps: 0.518004\n",
            " 537255/1000000: episode: 779, duration: 8.402s, episode steps: 1407, steps per second: 167, episode reward: 500.000, mean reward:  0.355 [ 0.000, 200.000], mean action: 2.345 [0.000, 5.000],  loss: 72.850436, mean_q: 294.038765, mean_eps: 0.517104\n",
            " 537888/1000000: episode: 780, duration: 3.795s, episode steps: 633, steps per second: 167, episode reward: 105.000, mean reward:  0.166 [ 0.000, 25.000], mean action: 2.133 [0.000, 5.000],  loss: 73.853717, mean_q: 268.539850, mean_eps: 0.516186\n",
            " 538608/1000000: episode: 781, duration: 4.300s, episode steps: 720, steps per second: 167, episode reward: 120.000, mean reward:  0.167 [ 0.000, 30.000], mean action: 2.675 [0.000, 5.000],  loss: 35.656917, mean_q: 274.894650, mean_eps: 0.515577\n",
            " 539180/1000000: episode: 782, duration: 3.442s, episode steps: 572, steps per second: 166, episode reward: 155.000, mean reward:  0.271 [ 0.000, 30.000], mean action: 2.439 [0.000, 5.000],  loss: 33.754137, mean_q: 278.900242, mean_eps: 0.514996\n",
            " 539864/1000000: episode: 783, duration: 4.095s, episode steps: 684, steps per second: 167, episode reward: 135.000, mean reward:  0.197 [ 0.000, 30.000], mean action: 2.713 [0.000, 5.000],  loss: 26.124932, mean_q: 285.892319, mean_eps: 0.514431\n",
            " 541023/1000000: episode: 784, duration: 6.869s, episode steps: 1159, steps per second: 169, episode reward: 480.000, mean reward:  0.414 [ 0.000, 30.000], mean action: 2.616 [0.000, 5.000],  loss: 20.949017, mean_q: 276.598676, mean_eps: 0.513601\n",
            " 541738/1000000: episode: 785, duration: 4.274s, episode steps: 715, steps per second: 167, episode reward: 170.000, mean reward:  0.238 [ 0.000, 30.000], mean action: 2.562 [0.000, 5.000],  loss: 23.017813, mean_q: 275.062341, mean_eps: 0.512758\n",
            " 542284/1000000: episode: 786, duration: 3.326s, episode steps: 546, steps per second: 164, episode reward: 55.000, mean reward:  0.101 [ 0.000, 20.000], mean action: 2.606 [0.000, 5.000],  loss: 30.354529, mean_q: 276.799686, mean_eps: 0.512191\n",
            " 543080/1000000: episode: 787, duration: 4.773s, episode steps: 796, steps per second: 167, episode reward: 185.000, mean reward:  0.232 [ 0.000, 30.000], mean action: 2.433 [0.000, 5.000],  loss: 30.238150, mean_q: 283.377460, mean_eps: 0.511587\n",
            " 543749/1000000: episode: 788, duration: 4.020s, episode steps: 669, steps per second: 166, episode reward: 180.000, mean reward:  0.269 [ 0.000, 30.000], mean action: 2.335 [0.000, 5.000],  loss: 28.831057, mean_q: 278.416233, mean_eps: 0.510927\n",
            " 544255/1000000: episode: 789, duration: 3.046s, episode steps: 506, steps per second: 166, episode reward: 85.000, mean reward:  0.168 [ 0.000, 30.000], mean action: 2.601 [0.000, 5.000],  loss: 28.916109, mean_q: 275.882498, mean_eps: 0.510399\n",
            " 545174/1000000: episode: 790, duration: 5.509s, episode steps: 919, steps per second: 167, episode reward: 210.000, mean reward:  0.229 [ 0.000, 30.000], mean action: 2.544 [0.000, 5.000],  loss: 27.363264, mean_q: 284.949582, mean_eps: 0.509757\n",
            " 546009/1000000: episode: 791, duration: 5.012s, episode steps: 835, steps per second: 167, episode reward: 200.000, mean reward:  0.240 [ 0.000, 30.000], mean action: 2.581 [0.000, 5.000],  loss: 14.883366, mean_q: 279.910086, mean_eps: 0.508968\n",
            " 546819/1000000: episode: 792, duration: 4.806s, episode steps: 810, steps per second: 169, episode reward: 180.000, mean reward:  0.222 [ 0.000, 30.000], mean action: 2.309 [0.000, 5.000],  loss: 27.260543, mean_q: 273.176513, mean_eps: 0.508228\n",
            " 547373/1000000: episode: 793, duration: 3.380s, episode steps: 554, steps per second: 164, episode reward: 85.000, mean reward:  0.153 [ 0.000, 30.000], mean action: 2.422 [0.000, 5.000],  loss: 20.161373, mean_q: 280.821346, mean_eps: 0.507614\n",
            " 547871/1000000: episode: 794, duration: 3.004s, episode steps: 498, steps per second: 166, episode reward: 15.000, mean reward:  0.030 [ 0.000, 10.000], mean action: 2.333 [0.000, 5.000],  loss: 39.587875, mean_q: 275.916795, mean_eps: 0.507141\n",
            " 548405/1000000: episode: 795, duration: 3.223s, episode steps: 534, steps per second: 166, episode reward: 65.000, mean reward:  0.122 [ 0.000, 20.000], mean action: 2.644 [0.000, 5.000],  loss: 34.693178, mean_q: 281.743841, mean_eps: 0.506676\n",
            " 549490/1000000: episode: 796, duration: 6.532s, episode steps: 1085, steps per second: 166, episode reward: 305.000, mean reward:  0.281 [ 0.000, 30.000], mean action: 2.570 [0.000, 5.000],  loss: 26.917274, mean_q: 298.698952, mean_eps: 0.505948\n",
            " 550109/1000000: episode: 797, duration: 3.686s, episode steps: 619, steps per second: 168, episode reward: 85.000, mean reward:  0.137 [ 0.000, 20.000], mean action: 2.628 [0.000, 5.000],  loss: 59.842255, mean_q: 313.661085, mean_eps: 0.505181\n",
            " 550969/1000000: episode: 798, duration: 5.157s, episode steps: 860, steps per second: 167, episode reward: 240.000, mean reward:  0.279 [ 0.000, 30.000], mean action: 2.244 [0.000, 5.000],  loss: 54.430931, mean_q: 288.225833, mean_eps: 0.504515\n",
            " 551473/1000000: episode: 799, duration: 3.014s, episode steps: 504, steps per second: 167, episode reward: 105.000, mean reward:  0.208 [ 0.000, 30.000], mean action: 2.550 [0.000, 5.000],  loss: 41.642052, mean_q: 267.243811, mean_eps: 0.503902\n",
            " 552095/1000000: episode: 800, duration: 3.714s, episode steps: 622, steps per second: 167, episode reward: 180.000, mean reward:  0.289 [ 0.000, 30.000], mean action: 2.484 [0.000, 5.000],  loss: 32.657785, mean_q: 269.621142, mean_eps: 0.503395\n",
            " 552799/1000000: episode: 801, duration: 4.229s, episode steps: 704, steps per second: 166, episode reward: 135.000, mean reward:  0.192 [ 0.000, 30.000], mean action: 2.476 [0.000, 5.000],  loss: 21.269402, mean_q: 281.647328, mean_eps: 0.502798\n",
            " 553732/1000000: episode: 802, duration: 5.559s, episode steps: 933, steps per second: 168, episode reward: 450.000, mean reward:  0.482 [ 0.000, 200.000], mean action: 2.436 [0.000, 5.000],  loss: 38.798601, mean_q: 274.634762, mean_eps: 0.502061\n",
            " 554403/1000000: episode: 803, duration: 3.993s, episode steps: 671, steps per second: 168, episode reward: 120.000, mean reward:  0.179 [ 0.000, 30.000], mean action: 2.516 [0.000, 5.000],  loss: 50.337176, mean_q: 257.927927, mean_eps: 0.501340\n",
            " 554774/1000000: episode: 804, duration: 2.212s, episode steps: 371, steps per second: 168, episode reward: 105.000, mean reward:  0.283 [ 0.000, 30.000], mean action: 2.887 [0.000, 5.000],  loss: 29.341383, mean_q: 263.254097, mean_eps: 0.500871\n",
            " 555710/1000000: episode: 805, duration: 5.568s, episode steps: 936, steps per second: 168, episode reward: 205.000, mean reward:  0.219 [ 0.000, 30.000], mean action: 2.682 [0.000, 5.000],  loss: 48.472774, mean_q: 283.890267, mean_eps: 0.500283\n",
            " 556351/1000000: episode: 806, duration: 3.807s, episode steps: 641, steps per second: 168, episode reward: 110.000, mean reward:  0.172 [ 0.000, 30.000], mean action: 2.894 [0.000, 5.000],  loss: 39.074039, mean_q: 290.277783, mean_eps: 0.499573\n",
            " 556933/1000000: episode: 807, duration: 3.474s, episode steps: 582, steps per second: 168, episode reward: 155.000, mean reward:  0.266 [ 0.000, 30.000], mean action: 2.340 [0.000, 5.000],  loss: 27.075778, mean_q: 288.466580, mean_eps: 0.499023\n",
            " 557269/1000000: episode: 808, duration: 2.040s, episode steps: 336, steps per second: 165, episode reward: 30.000, mean reward:  0.089 [ 0.000, 10.000], mean action: 2.551 [0.000, 5.000],  loss: 26.659395, mean_q: 284.722299, mean_eps: 0.498610\n",
            " 558200/1000000: episode: 809, duration: 5.529s, episode steps: 931, steps per second: 168, episode reward: 230.000, mean reward:  0.247 [ 0.000, 30.000], mean action: 2.405 [0.000, 5.000],  loss: 71.327773, mean_q: 290.736588, mean_eps: 0.498039\n",
            " 558904/1000000: episode: 810, duration: 4.194s, episode steps: 704, steps per second: 168, episode reward: 180.000, mean reward:  0.256 [ 0.000, 30.000], mean action: 2.457 [0.000, 5.000],  loss: 28.942672, mean_q: 283.984987, mean_eps: 0.497304\n",
            " 559486/1000000: episode: 811, duration: 3.451s, episode steps: 582, steps per second: 169, episode reward: 105.000, mean reward:  0.180 [ 0.000, 30.000], mean action: 2.356 [0.000, 5.000],  loss: 24.219568, mean_q: 281.408969, mean_eps: 0.496725\n",
            " 560334/1000000: episode: 812, duration: 5.028s, episode steps: 848, steps per second: 169, episode reward: 410.000, mean reward:  0.483 [ 0.000, 200.000], mean action: 2.636 [0.000, 5.000],  loss: 42.594687, mean_q: 274.905332, mean_eps: 0.496081\n",
            " 560863/1000000: episode: 813, duration: 3.156s, episode steps: 529, steps per second: 168, episode reward: 75.000, mean reward:  0.142 [ 0.000, 25.000], mean action: 2.527 [0.000, 5.000],  loss: 46.955383, mean_q: 266.341208, mean_eps: 0.495462\n",
            " 561225/1000000: episode: 814, duration: 2.176s, episode steps: 362, steps per second: 166, episode reward: 30.000, mean reward:  0.083 [ 0.000, 15.000], mean action: 2.506 [0.000, 5.000],  loss: 37.642813, mean_q: 266.009594, mean_eps: 0.495061\n",
            " 561738/1000000: episode: 815, duration: 3.083s, episode steps: 513, steps per second: 166, episode reward: 130.000, mean reward:  0.253 [ 0.000, 25.000], mean action: 2.409 [0.000, 5.000],  loss: 52.147121, mean_q: 288.506696, mean_eps: 0.494667\n",
            " 562383/1000000: episode: 816, duration: 3.904s, episode steps: 645, steps per second: 165, episode reward: 130.000, mean reward:  0.202 [ 0.000, 20.000], mean action: 2.315 [0.000, 5.000],  loss: 64.560302, mean_q: 297.056210, mean_eps: 0.494146\n",
            " 563344/1000000: episode: 817, duration: 5.756s, episode steps: 961, steps per second: 167, episode reward: 255.000, mean reward:  0.265 [ 0.000, 30.000], mean action: 2.676 [0.000, 5.000],  loss: 41.415543, mean_q: 300.779387, mean_eps: 0.493423\n",
            " 564323/1000000: episode: 818, duration: 5.908s, episode steps: 979, steps per second: 166, episode reward: 210.000, mean reward:  0.215 [ 0.000, 30.000], mean action: 2.535 [0.000, 5.000],  loss: 21.889831, mean_q: 280.685717, mean_eps: 0.492550\n",
            " 565085/1000000: episode: 819, duration: 4.528s, episode steps: 762, steps per second: 168, episode reward: 155.000, mean reward:  0.203 [ 0.000, 30.000], mean action: 2.509 [0.000, 5.000],  loss: 21.057055, mean_q: 271.182656, mean_eps: 0.491767\n",
            " 565743/1000000: episode: 820, duration: 3.922s, episode steps: 658, steps per second: 168, episode reward: 120.000, mean reward:  0.182 [ 0.000, 30.000], mean action: 2.556 [0.000, 5.000],  loss: 15.565036, mean_q: 270.714549, mean_eps: 0.491128\n",
            " 566397/1000000: episode: 821, duration: 3.935s, episode steps: 654, steps per second: 166, episode reward: 105.000, mean reward:  0.161 [ 0.000, 30.000], mean action: 2.261 [0.000, 5.000],  loss: 22.647664, mean_q: 275.667460, mean_eps: 0.490537\n",
            " 567318/1000000: episode: 822, duration: 5.415s, episode steps: 921, steps per second: 170, episode reward: 315.000, mean reward:  0.342 [ 0.000, 30.000], mean action: 2.208 [0.000, 5.000],  loss: 31.029744, mean_q: 283.998173, mean_eps: 0.489829\n",
            " 568418/1000000: episode: 823, duration: 6.507s, episode steps: 1100, steps per second: 169, episode reward: 165.000, mean reward:  0.150 [ 0.000, 30.000], mean action: 2.256 [0.000, 5.000],  loss: 34.946065, mean_q: 285.030335, mean_eps: 0.488919\n",
            " 569331/1000000: episode: 824, duration: 5.384s, episode steps: 913, steps per second: 170, episode reward: 165.000, mean reward:  0.181 [ 0.000, 30.000], mean action: 2.602 [0.000, 5.000],  loss: 32.575948, mean_q: 292.294862, mean_eps: 0.488013\n",
            " 570253/1000000: episode: 825, duration: 5.467s, episode steps: 922, steps per second: 169, episode reward: 210.000, mean reward:  0.228 [ 0.000, 30.000], mean action: 2.734 [0.000, 5.000],  loss: 22.834242, mean_q: 289.400313, mean_eps: 0.487188\n",
            " 570695/1000000: episode: 826, duration: 2.642s, episode steps: 442, steps per second: 167, episode reward: 80.000, mean reward:  0.181 [ 0.000, 30.000], mean action: 2.455 [0.000, 5.000],  loss: 25.849017, mean_q: 272.031217, mean_eps: 0.486574\n",
            " 571204/1000000: episode: 827, duration: 3.018s, episode steps: 509, steps per second: 169, episode reward: 45.000, mean reward:  0.088 [ 0.000, 15.000], mean action: 2.061 [0.000, 5.000],  loss: 35.228863, mean_q: 269.714561, mean_eps: 0.486146\n",
            " 572006/1000000: episode: 828, duration: 4.773s, episode steps: 802, steps per second: 168, episode reward: 170.000, mean reward:  0.212 [ 0.000, 30.000], mean action: 2.531 [0.000, 5.000],  loss: 32.286091, mean_q: 275.411404, mean_eps: 0.485556\n",
            " 572759/1000000: episode: 829, duration: 4.488s, episode steps: 753, steps per second: 168, episode reward: 155.000, mean reward:  0.206 [ 0.000, 30.000], mean action: 2.467 [0.000, 5.000],  loss: 19.290342, mean_q: 272.739166, mean_eps: 0.484856\n",
            " 573281/1000000: episode: 830, duration: 3.130s, episode steps: 522, steps per second: 167, episode reward: 20.000, mean reward:  0.038 [ 0.000, 10.000], mean action: 2.360 [0.000, 5.000],  loss: 19.290095, mean_q: 272.955112, mean_eps: 0.484282\n",
            " 574147/1000000: episode: 831, duration: 5.177s, episode steps: 866, steps per second: 167, episode reward: 310.000, mean reward:  0.358 [ 0.000, 30.000], mean action: 2.292 [0.000, 5.000],  loss: 39.837418, mean_q: 280.553779, mean_eps: 0.483658\n",
            " 574491/1000000: episode: 832, duration: 2.069s, episode steps: 344, steps per second: 166, episode reward: 40.000, mean reward:  0.116 [ 0.000, 25.000], mean action: 2.270 [0.000, 5.000],  loss: 42.362216, mean_q: 272.923163, mean_eps: 0.483113\n",
            " 574879/1000000: episode: 833, duration: 2.329s, episode steps: 388, steps per second: 167, episode reward: 105.000, mean reward:  0.271 [ 0.000, 25.000], mean action: 2.325 [0.000, 5.000],  loss: 63.730891, mean_q: 274.394847, mean_eps: 0.482784\n",
            " 575552/1000000: episode: 834, duration: 4.002s, episode steps: 673, steps per second: 168, episode reward: 150.000, mean reward:  0.223 [ 0.000, 25.000], mean action: 2.406 [0.000, 5.000],  loss: 59.565156, mean_q: 287.550315, mean_eps: 0.482307\n",
            " 576401/1000000: episode: 835, duration: 5.127s, episode steps: 849, steps per second: 166, episode reward: 255.000, mean reward:  0.300 [ 0.000, 30.000], mean action: 2.420 [0.000, 5.000],  loss: 41.586514, mean_q: 290.715233, mean_eps: 0.481622\n",
            " 576784/1000000: episode: 836, duration: 2.356s, episode steps: 383, steps per second: 163, episode reward: 60.000, mean reward:  0.157 [ 0.000, 15.000], mean action: 2.373 [0.000, 5.000],  loss: 31.789545, mean_q: 277.819052, mean_eps: 0.481067\n",
            " 578012/1000000: episode: 837, duration: 7.379s, episode steps: 1228, steps per second: 166, episode reward: 375.000, mean reward:  0.305 [ 0.000, 30.000], mean action: 2.633 [0.000, 5.000],  loss: 46.596385, mean_q: 291.999051, mean_eps: 0.480342\n",
            " 578648/1000000: episode: 838, duration: 3.904s, episode steps: 636, steps per second: 163, episode reward: 135.000, mean reward:  0.212 [ 0.000, 30.000], mean action: 2.226 [0.000, 5.000],  loss: 20.386458, mean_q: 282.385964, mean_eps: 0.479503\n",
            " 579473/1000000: episode: 839, duration: 4.987s, episode steps: 825, steps per second: 165, episode reward: 335.000, mean reward:  0.406 [ 0.000, 200.000], mean action: 2.326 [0.000, 5.000],  loss: 33.263495, mean_q: 284.673859, mean_eps: 0.478846\n",
            " 580104/1000000: episode: 840, duration: 3.763s, episode steps: 631, steps per second: 168, episode reward: 210.000, mean reward:  0.333 [ 0.000, 30.000], mean action: 2.826 [0.000, 5.000],  loss: 52.273485, mean_q: 268.944784, mean_eps: 0.478191\n",
            " 581038/1000000: episode: 841, duration: 5.634s, episode steps: 934, steps per second: 166, episode reward: 225.000, mean reward:  0.241 [ 0.000, 30.000], mean action: 2.197 [0.000, 5.000],  loss: 18.453918, mean_q: 278.854396, mean_eps: 0.477487\n",
            " 581921/1000000: episode: 842, duration: 5.350s, episode steps: 883, steps per second: 165, episode reward: 240.000, mean reward:  0.272 [ 0.000, 30.000], mean action: 2.584 [0.000, 5.000],  loss: 25.182143, mean_q: 287.243980, mean_eps: 0.476669\n",
            " 582385/1000000: episode: 843, duration: 2.769s, episode steps: 464, steps per second: 168, episode reward: 55.000, mean reward:  0.119 [ 0.000, 20.000], mean action: 2.112 [0.000, 5.000],  loss: 18.381853, mean_q: 275.673344, mean_eps: 0.476063\n",
            " 582868/1000000: episode: 844, duration: 2.902s, episode steps: 483, steps per second: 166, episode reward: 60.000, mean reward:  0.124 [ 0.000, 15.000], mean action: 2.435 [0.000, 5.000],  loss: 26.919801, mean_q: 270.896614, mean_eps: 0.475637\n",
            " 583564/1000000: episode: 845, duration: 4.185s, episode steps: 696, steps per second: 166, episode reward: 355.000, mean reward:  0.510 [ 0.000, 200.000], mean action: 2.921 [0.000, 5.000],  loss: 48.143436, mean_q: 285.664737, mean_eps: 0.475106\n",
            " 584373/1000000: episode: 846, duration: 4.807s, episode steps: 809, steps per second: 168, episode reward: 95.000, mean reward:  0.117 [ 0.000, 20.000], mean action: 2.732 [0.000, 5.000],  loss: 54.797796, mean_q: 277.880287, mean_eps: 0.474429\n",
            " 584831/1000000: episode: 847, duration: 2.734s, episode steps: 458, steps per second: 167, episode reward: 35.000, mean reward:  0.076 [ 0.000, 25.000], mean action: 2.581 [0.000, 5.000],  loss: 32.507334, mean_q: 283.092198, mean_eps: 0.473859\n",
            " 585624/1000000: episode: 848, duration: 4.702s, episode steps: 793, steps per second: 169, episode reward: 170.000, mean reward:  0.214 [ 0.000, 30.000], mean action: 2.393 [0.000, 5.000],  loss: 38.605746, mean_q: 286.310180, mean_eps: 0.473296\n",
            " 586925/1000000: episode: 849, duration: 7.725s, episode steps: 1301, steps per second: 168, episode reward: 345.000, mean reward:  0.265 [ 0.000, 200.000], mean action: 2.955 [0.000, 5.000],  loss: 42.376457, mean_q: 286.429245, mean_eps: 0.472353\n",
            " 587617/1000000: episode: 850, duration: 4.160s, episode steps: 692, steps per second: 166, episode reward: 135.000, mean reward:  0.195 [ 0.000, 30.000], mean action: 2.691 [0.000, 5.000],  loss: 33.787265, mean_q: 267.053934, mean_eps: 0.471457\n",
            " 588303/1000000: episode: 851, duration: 4.070s, episode steps: 686, steps per second: 169, episode reward: 150.000, mean reward:  0.219 [ 0.000, 25.000], mean action: 2.682 [0.000, 5.000],  loss: 31.105240, mean_q: 258.801480, mean_eps: 0.470836\n",
            " 588986/1000000: episode: 852, duration: 4.082s, episode steps: 683, steps per second: 167, episode reward: 120.000, mean reward:  0.176 [ 0.000, 30.000], mean action: 2.572 [0.000, 5.000],  loss: 25.528305, mean_q: 275.729717, mean_eps: 0.470220\n",
            " 589612/1000000: episode: 853, duration: 3.739s, episode steps: 626, steps per second: 167, episode reward: 110.000, mean reward:  0.176 [ 0.000, 30.000], mean action: 2.540 [0.000, 5.000],  loss: 17.767623, mean_q: 278.124264, mean_eps: 0.469631\n",
            " 590276/1000000: episode: 854, duration: 3.973s, episode steps: 664, steps per second: 167, episode reward: 135.000, mean reward:  0.203 [ 0.000, 30.000], mean action: 2.581 [0.000, 5.000],  loss: 18.157242, mean_q: 283.406552, mean_eps: 0.469051\n",
            " 590976/1000000: episode: 855, duration: 4.158s, episode steps: 700, steps per second: 168, episode reward: 180.000, mean reward:  0.257 [ 0.000, 30.000], mean action: 2.770 [0.000, 5.000],  loss: 28.187699, mean_q: 281.652562, mean_eps: 0.468437\n",
            " 591611/1000000: episode: 856, duration: 3.864s, episode steps: 635, steps per second: 164, episode reward: 105.000, mean reward:  0.165 [ 0.000, 25.000], mean action: 2.346 [0.000, 5.000],  loss: 21.028847, mean_q: 270.368259, mean_eps: 0.467836\n",
            " 592110/1000000: episode: 857, duration: 2.964s, episode steps: 499, steps per second: 168, episode reward: 80.000, mean reward:  0.160 [ 0.000, 30.000], mean action: 2.190 [0.000, 5.000],  loss: 25.834276, mean_q: 277.984541, mean_eps: 0.467326\n",
            " 592749/1000000: episode: 858, duration: 3.869s, episode steps: 639, steps per second: 165, episode reward: 75.000, mean reward:  0.117 [ 0.000, 25.000], mean action: 2.476 [0.000, 5.000],  loss: 38.474599, mean_q: 281.053138, mean_eps: 0.466814\n",
            " 593722/1000000: episode: 859, duration: 5.834s, episode steps: 973, steps per second: 167, episode reward: 425.000, mean reward:  0.437 [ 0.000, 200.000], mean action: 2.630 [0.000, 5.000],  loss: 40.682439, mean_q: 282.221032, mean_eps: 0.466089\n",
            " 594499/1000000: episode: 860, duration: 4.663s, episode steps: 777, steps per second: 167, episode reward: 120.000, mean reward:  0.154 [ 0.000, 30.000], mean action: 2.623 [0.000, 5.000],  loss: 32.021619, mean_q: 271.616045, mean_eps: 0.465301\n",
            " 595138/1000000: episode: 861, duration: 3.810s, episode steps: 639, steps per second: 168, episode reward: 110.000, mean reward:  0.172 [ 0.000, 30.000], mean action: 2.139 [0.000, 5.000],  loss: 19.252481, mean_q: 267.240420, mean_eps: 0.464664\n",
            " 595760/1000000: episode: 862, duration: 3.729s, episode steps: 622, steps per second: 167, episode reward: 105.000, mean reward:  0.169 [ 0.000, 30.000], mean action: 2.251 [0.000, 5.000],  loss: 14.906569, mean_q: 276.614214, mean_eps: 0.464096\n",
            " 596563/1000000: episode: 863, duration: 4.805s, episode steps: 803, steps per second: 167, episode reward: 155.000, mean reward:  0.193 [ 0.000, 30.000], mean action: 2.514 [0.000, 5.000],  loss: 25.305743, mean_q: 279.767386, mean_eps: 0.463455\n",
            " 597120/1000000: episode: 864, duration: 3.382s, episode steps: 557, steps per second: 165, episode reward: 105.000, mean reward:  0.189 [ 0.000, 30.000], mean action: 2.343 [0.000, 5.000],  loss: 26.298869, mean_q: 268.384137, mean_eps: 0.462843\n",
            " 597623/1000000: episode: 865, duration: 2.977s, episode steps: 503, steps per second: 169, episode reward: 30.000, mean reward:  0.060 [ 0.000, 15.000], mean action: 2.475 [0.000, 5.000],  loss: 33.003931, mean_q: 273.105425, mean_eps: 0.462366\n",
            " 598421/1000000: episode: 866, duration: 4.723s, episode steps: 798, steps per second: 169, episode reward: 155.000, mean reward:  0.194 [ 0.000, 30.000], mean action: 2.604 [0.000, 5.000],  loss: 35.569766, mean_q: 284.064753, mean_eps: 0.461781\n",
            " 599200/1000000: episode: 867, duration: 4.727s, episode steps: 779, steps per second: 165, episode reward: 135.000, mean reward:  0.173 [ 0.000, 30.000], mean action: 2.506 [0.000, 5.000],  loss: 25.304868, mean_q: 280.474971, mean_eps: 0.461071\n",
            " 600042/1000000: episode: 868, duration: 5.025s, episode steps: 842, steps per second: 168, episode reward: 210.000, mean reward:  0.249 [ 0.000, 30.000], mean action: 2.575 [0.000, 5.000],  loss: 20.710503, mean_q: 265.062278, mean_eps: 0.460342\n",
            " 600494/1000000: episode: 869, duration: 2.743s, episode steps: 452, steps per second: 165, episode reward: 50.000, mean reward:  0.111 [ 0.000, 20.000], mean action: 2.467 [0.000, 5.000],  loss: 15.425542, mean_q: 258.655724, mean_eps: 0.459759\n",
            " 601129/1000000: episode: 870, duration: 3.748s, episode steps: 635, steps per second: 169, episode reward: 120.000, mean reward:  0.189 [ 0.000, 30.000], mean action: 2.921 [0.000, 5.000],  loss: 31.021491, mean_q: 273.204139, mean_eps: 0.459270\n",
            " 601623/1000000: episode: 871, duration: 2.970s, episode steps: 494, steps per second: 166, episode reward: 80.000, mean reward:  0.162 [ 0.000, 20.000], mean action: 2.870 [0.000, 5.000],  loss: 37.416579, mean_q: 283.026849, mean_eps: 0.458762\n",
            " 602260/1000000: episode: 872, duration: 3.793s, episode steps: 637, steps per second: 168, episode reward: 155.000, mean reward:  0.243 [ 0.000, 30.000], mean action: 2.421 [0.000, 5.000],  loss: 27.479952, mean_q: 281.692872, mean_eps: 0.458253\n",
            " 603186/1000000: episode: 873, duration: 5.571s, episode steps: 926, steps per second: 166, episode reward: 210.000, mean reward:  0.227 [ 0.000, 30.000], mean action: 2.697 [0.000, 5.000],  loss: 26.749446, mean_q: 271.270163, mean_eps: 0.457550\n",
            " 603751/1000000: episode: 874, duration: 3.376s, episode steps: 565, steps per second: 167, episode reward: 75.000, mean reward:  0.133 [ 0.000, 25.000], mean action: 2.143 [0.000, 5.000],  loss: 22.282173, mean_q: 259.835927, mean_eps: 0.456879\n",
            " 604145/1000000: episode: 875, duration: 2.349s, episode steps: 394, steps per second: 168, episode reward: 30.000, mean reward:  0.076 [ 0.000, 10.000], mean action: 2.221 [0.000, 5.000],  loss: 34.535309, mean_q: 267.482318, mean_eps: 0.456447\n",
            " 604674/1000000: episode: 876, duration: 3.173s, episode steps: 529, steps per second: 167, episode reward: 80.000, mean reward:  0.151 [ 0.000, 15.000], mean action: 2.420 [0.000, 5.000],  loss: 61.335441, mean_q: 283.928314, mean_eps: 0.456032\n",
            " 605304/1000000: episode: 877, duration: 3.763s, episode steps: 630, steps per second: 167, episode reward: 110.000, mean reward:  0.175 [ 0.000, 25.000], mean action: 2.692 [0.000, 5.000],  loss: 50.425413, mean_q: 283.745031, mean_eps: 0.455510\n",
            " 606421/1000000: episode: 878, duration: 6.652s, episode steps: 1117, steps per second: 168, episode reward: 395.000, mean reward:  0.354 [ 0.000, 200.000], mean action: 2.437 [0.000, 5.000],  loss: 45.624831, mean_q: 278.206209, mean_eps: 0.454724\n",
            " 607059/1000000: episode: 879, duration: 3.783s, episode steps: 638, steps per second: 169, episode reward: 155.000, mean reward:  0.243 [ 0.000, 30.000], mean action: 2.627 [0.000, 5.000],  loss: 65.342608, mean_q: 259.700666, mean_eps: 0.453934\n",
            " 607912/1000000: episode: 880, duration: 5.059s, episode steps: 853, steps per second: 169, episode reward: 215.000, mean reward:  0.252 [ 0.000, 30.000], mean action: 2.491 [0.000, 5.000],  loss: 45.938571, mean_q: 271.521372, mean_eps: 0.453263\n",
            " 608550/1000000: episode: 881, duration: 3.785s, episode steps: 638, steps per second: 169, episode reward: 110.000, mean reward:  0.172 [ 0.000, 30.000], mean action: 2.759 [0.000, 5.000],  loss: 32.333765, mean_q: 263.813471, mean_eps: 0.452593\n",
            " 609335/1000000: episode: 882, duration: 4.684s, episode steps: 785, steps per second: 168, episode reward: 155.000, mean reward:  0.197 [ 0.000, 30.000], mean action: 2.720 [0.000, 5.000],  loss: 30.788230, mean_q: 279.211558, mean_eps: 0.451952\n",
            " 609927/1000000: episode: 883, duration: 3.517s, episode steps: 592, steps per second: 168, episode reward: 120.000, mean reward:  0.203 [ 0.000, 30.000], mean action: 2.667 [0.000, 5.000],  loss: 20.968086, mean_q: 273.534011, mean_eps: 0.451333\n",
            " 610423/1000000: episode: 884, duration: 2.958s, episode steps: 496, steps per second: 168, episode reward: 65.000, mean reward:  0.131 [ 0.000, 20.000], mean action: 2.397 [0.000, 5.000],  loss: 20.146422, mean_q: 270.636072, mean_eps: 0.450843\n",
            " 611294/1000000: episode: 885, duration: 5.198s, episode steps: 871, steps per second: 168, episode reward: 130.000, mean reward:  0.149 [ 0.000, 25.000], mean action: 2.382 [0.000, 5.000],  loss: 30.611301, mean_q: 287.041783, mean_eps: 0.450228\n",
            " 612129/1000000: episode: 886, duration: 4.955s, episode steps: 835, steps per second: 169, episode reward: 210.000, mean reward:  0.251 [ 0.000, 30.000], mean action: 2.182 [0.000, 5.000],  loss: 36.938091, mean_q: 293.371378, mean_eps: 0.449460\n",
            " 613240/1000000: episode: 887, duration: 6.605s, episode steps: 1111, steps per second: 168, episode reward: 335.000, mean reward:  0.302 [ 0.000, 30.000], mean action: 2.419 [0.000, 5.000],  loss: 15.378156, mean_q: 275.565945, mean_eps: 0.448584\n",
            " 614155/1000000: episode: 888, duration: 5.404s, episode steps: 915, steps per second: 169, episode reward: 245.000, mean reward:  0.268 [ 0.000, 30.000], mean action: 2.558 [0.000, 5.000],  loss: 14.492518, mean_q: 282.381965, mean_eps: 0.447673\n",
            " 614907/1000000: episode: 889, duration: 4.486s, episode steps: 752, steps per second: 168, episode reward: 145.000, mean reward:  0.193 [ 0.000, 30.000], mean action: 2.259 [0.000, 5.000],  loss: 26.868751, mean_q: 283.073193, mean_eps: 0.446923\n",
            " 615614/1000000: episode: 890, duration: 4.182s, episode steps: 707, steps per second: 169, episode reward: 115.000, mean reward:  0.163 [ 0.000, 25.000], mean action: 2.731 [0.000, 5.000],  loss: 36.224303, mean_q: 283.937689, mean_eps: 0.446266\n",
            " 616052/1000000: episode: 891, duration: 2.610s, episode steps: 438, steps per second: 168, episode reward: 70.000, mean reward:  0.160 [ 0.000, 30.000], mean action: 2.011 [0.000, 5.000],  loss: 53.529156, mean_q: 282.265301, mean_eps: 0.445751\n",
            " 616495/1000000: episode: 892, duration: 2.653s, episode steps: 443, steps per second: 167, episode reward: 45.000, mean reward:  0.102 [ 0.000, 30.000], mean action: 1.977 [0.000, 5.000],  loss: 60.043598, mean_q: 282.978736, mean_eps: 0.445354\n",
            " 617171/1000000: episode: 893, duration: 4.036s, episode steps: 676, steps per second: 167, episode reward: 135.000, mean reward:  0.200 [ 0.000, 30.000], mean action: 2.522 [0.000, 5.000],  loss: 41.485857, mean_q: 283.977668, mean_eps: 0.444851\n",
            " 617764/1000000: episode: 894, duration: 3.601s, episode steps: 593, steps per second: 165, episode reward: 105.000, mean reward:  0.177 [ 0.000, 30.000], mean action: 2.956 [0.000, 5.000],  loss: 21.524160, mean_q: 277.961782, mean_eps: 0.444280\n",
            " 619093/1000000: episode: 895, duration: 7.864s, episode steps: 1329, steps per second: 169, episode reward: 385.000, mean reward:  0.290 [ 0.000, 30.000], mean action: 2.341 [0.000, 5.000],  loss: 26.881699, mean_q: 286.915678, mean_eps: 0.443415\n",
            " 619479/1000000: episode: 896, duration: 2.336s, episode steps: 386, steps per second: 165, episode reward: 20.000, mean reward:  0.052 [ 0.000, 10.000], mean action: 2.798 [0.000, 5.000],  loss: 37.090871, mean_q: 272.116184, mean_eps: 0.442643\n",
            " 620373/1000000: episode: 897, duration: 5.357s, episode steps: 894, steps per second: 167, episode reward: 190.000, mean reward:  0.213 [ 0.000, 30.000], mean action: 2.972 [0.000, 5.000],  loss: 63.589783, mean_q: 280.507380, mean_eps: 0.442067\n",
            " 620907/1000000: episode: 898, duration: 3.215s, episode steps: 534, steps per second: 166, episode reward: 45.000, mean reward:  0.084 [ 0.000, 15.000], mean action: 2.431 [0.000, 5.000],  loss: 33.290727, mean_q: 290.859557, mean_eps: 0.441424\n",
            " 621909/1000000: episode: 899, duration: 5.942s, episode steps: 1002, steps per second: 169, episode reward: 495.000, mean reward:  0.494 [ 0.000, 200.000], mean action: 2.671 [0.000, 5.000],  loss: 33.577775, mean_q: 289.738695, mean_eps: 0.440733\n",
            " 622602/1000000: episode: 900, duration: 4.159s, episode steps: 693, steps per second: 167, episode reward: 135.000, mean reward:  0.195 [ 0.000, 30.000], mean action: 2.848 [0.000, 5.000],  loss: 36.307249, mean_q: 288.051352, mean_eps: 0.439971\n",
            " 623201/1000000: episode: 901, duration: 3.571s, episode steps: 599, steps per second: 168, episode reward: 120.000, mean reward:  0.200 [ 0.000, 30.000], mean action: 2.621 [0.000, 5.000],  loss: 22.762673, mean_q: 264.917177, mean_eps: 0.439389\n",
            " 624057/1000000: episode: 902, duration: 5.139s, episode steps: 856, steps per second: 167, episode reward: 225.000, mean reward:  0.263 [ 0.000, 30.000], mean action: 2.637 [0.000, 5.000],  loss: 26.779714, mean_q: 264.167951, mean_eps: 0.438734\n",
            " 625237/1000000: episode: 903, duration: 6.993s, episode steps: 1180, steps per second: 169, episode reward: 265.000, mean reward:  0.225 [ 0.000, 30.000], mean action: 2.473 [0.000, 5.000],  loss: 20.383664, mean_q: 272.786938, mean_eps: 0.437818\n",
            " 625788/1000000: episode: 904, duration: 3.320s, episode steps: 551, steps per second: 166, episode reward: 110.000, mean reward:  0.200 [ 0.000, 30.000], mean action: 2.574 [0.000, 5.000],  loss: 39.929209, mean_q: 300.934639, mean_eps: 0.437039\n",
            " 626322/1000000: episode: 905, duration: 3.217s, episode steps: 534, steps per second: 166, episode reward: 80.000, mean reward:  0.150 [ 0.000, 25.000], mean action: 2.528 [0.000, 5.000],  loss: 37.725379, mean_q: 283.809954, mean_eps: 0.436551\n",
            " 626965/1000000: episode: 906, duration: 3.846s, episode steps: 643, steps per second: 167, episode reward: 145.000, mean reward:  0.226 [ 0.000, 30.000], mean action: 2.474 [0.000, 5.000],  loss: 30.547759, mean_q: 282.688858, mean_eps: 0.436021\n",
            " 627981/1000000: episode: 907, duration: 6.127s, episode steps: 1016, steps per second: 166, episode reward: 285.000, mean reward:  0.281 [ 0.000, 30.000], mean action: 2.344 [0.000, 5.000],  loss: 23.093174, mean_q: 289.736216, mean_eps: 0.435275\n",
            " 628788/1000000: episode: 908, duration: 4.852s, episode steps: 807, steps per second: 166, episode reward: 225.000, mean reward:  0.279 [ 0.000, 30.000], mean action: 2.331 [0.000, 5.000],  loss: 34.756357, mean_q: 265.176304, mean_eps: 0.434454\n",
            " 629300/1000000: episode: 909, duration: 3.074s, episode steps: 512, steps per second: 167, episode reward: 105.000, mean reward:  0.205 [ 0.000, 30.000], mean action: 2.619 [0.000, 5.000],  loss: 22.880379, mean_q: 260.261642, mean_eps: 0.433861\n",
            " 630196/1000000: episode: 910, duration: 5.372s, episode steps: 896, steps per second: 167, episode reward: 225.000, mean reward:  0.251 [ 0.000, 30.000], mean action: 2.521 [0.000, 5.000],  loss: 26.658928, mean_q: 276.284827, mean_eps: 0.433227\n",
            " 630863/1000000: episode: 911, duration: 4.017s, episode steps: 667, steps per second: 166, episode reward: 145.000, mean reward:  0.217 [ 0.000, 30.000], mean action: 2.913 [0.000, 5.000],  loss: 55.422179, mean_q: 297.353398, mean_eps: 0.432524\n",
            " 631373/1000000: episode: 912, duration: 3.067s, episode steps: 510, steps per second: 166, episode reward: 50.000, mean reward:  0.098 [ 0.000, 20.000], mean action: 2.592 [0.000, 5.000],  loss: 47.842526, mean_q: 283.216220, mean_eps: 0.431994\n",
            " 631863/1000000: episode: 913, duration: 2.948s, episode steps: 490, steps per second: 166, episode reward: 155.000, mean reward:  0.316 [ 0.000, 30.000], mean action: 2.490 [0.000, 5.000],  loss: 33.236878, mean_q: 276.680619, mean_eps: 0.431544\n",
            " 632361/1000000: episode: 914, duration: 3.018s, episode steps: 498, steps per second: 165, episode reward: 105.000, mean reward:  0.211 [ 0.000, 25.000], mean action: 2.536 [0.000, 5.000],  loss: 27.879466, mean_q: 277.558386, mean_eps: 0.431100\n",
            " 632845/1000000: episode: 915, duration: 2.922s, episode steps: 484, steps per second: 166, episode reward: 50.000, mean reward:  0.103 [ 0.000, 15.000], mean action: 2.591 [0.000, 5.000],  loss: 39.996528, mean_q: 277.690699, mean_eps: 0.430658\n",
            " 633697/1000000: episode: 916, duration: 5.088s, episode steps: 852, steps per second: 167, episode reward: 390.000, mean reward:  0.458 [ 0.000, 200.000], mean action: 2.170 [0.000, 5.000],  loss: 40.494190, mean_q: 274.007586, mean_eps: 0.430057\n",
            " 634097/1000000: episode: 917, duration: 2.407s, episode steps: 400, steps per second: 166, episode reward: 45.000, mean reward:  0.113 [ 0.000, 15.000], mean action: 2.862 [0.000, 5.000],  loss: 33.837276, mean_q: 261.830868, mean_eps: 0.429493\n",
            " 634592/1000000: episode: 918, duration: 3.005s, episode steps: 495, steps per second: 165, episode reward: 45.000, mean reward:  0.091 [ 0.000, 15.000], mean action: 2.507 [0.000, 5.000],  loss: 50.560081, mean_q: 264.736632, mean_eps: 0.429090\n",
            " 635398/1000000: episode: 919, duration: 4.803s, episode steps: 806, steps per second: 168, episode reward: 210.000, mean reward:  0.261 [ 0.000, 30.000], mean action: 2.633 [0.000, 5.000],  loss: 46.625416, mean_q: 284.607140, mean_eps: 0.428505\n",
            " 636438/1000000: episode: 920, duration: 6.216s, episode steps: 1040, steps per second: 167, episode reward: 505.000, mean reward:  0.486 [ 0.000, 200.000], mean action: 2.748 [0.000, 5.000],  loss: 56.452529, mean_q: 281.286326, mean_eps: 0.427674\n",
            " 636985/1000000: episode: 921, duration: 3.279s, episode steps: 547, steps per second: 167, episode reward: 105.000, mean reward:  0.192 [ 0.000, 30.000], mean action: 2.580 [0.000, 5.000],  loss: 37.975862, mean_q: 244.802024, mean_eps: 0.426960\n",
            " 637665/1000000: episode: 922, duration: 4.076s, episode steps: 680, steps per second: 167, episode reward: 135.000, mean reward:  0.199 [ 0.000, 30.000], mean action: 2.444 [0.000, 5.000],  loss: 41.587244, mean_q: 267.384261, mean_eps: 0.426408\n",
            " 638480/1000000: episode: 923, duration: 4.866s, episode steps: 815, steps per second: 167, episode reward: 210.000, mean reward:  0.258 [ 0.000, 30.000], mean action: 2.745 [0.000, 5.000],  loss: 29.020341, mean_q: 276.567956, mean_eps: 0.425735\n",
            " 639517/1000000: episode: 924, duration: 6.168s, episode steps: 1037, steps per second: 168, episode reward: 235.000, mean reward:  0.227 [ 0.000, 30.000], mean action: 2.273 [0.000, 5.000],  loss: 19.688829, mean_q: 279.020112, mean_eps: 0.424902\n",
            " 639981/1000000: episode: 925, duration: 2.779s, episode steps: 464, steps per second: 167, episode reward: 15.000, mean reward:  0.032 [ 0.000, 10.000], mean action: 2.800 [0.000, 5.000],  loss: 36.490979, mean_q: 289.217621, mean_eps: 0.424226\n",
            " 640654/1000000: episode: 926, duration: 4.006s, episode steps: 673, steps per second: 168, episode reward: 95.000, mean reward:  0.141 [ 0.000, 25.000], mean action: 2.487 [0.000, 5.000],  loss: 51.805466, mean_q: 286.935897, mean_eps: 0.423715\n",
            " 641431/1000000: episode: 927, duration: 4.616s, episode steps: 777, steps per second: 168, episode reward: 180.000, mean reward:  0.232 [ 0.000, 30.000], mean action: 2.695 [0.000, 5.000],  loss: 34.831352, mean_q: 279.544280, mean_eps: 0.423062\n",
            " 642214/1000000: episode: 928, duration: 4.725s, episode steps: 783, steps per second: 166, episode reward: 125.000, mean reward:  0.160 [ 0.000, 20.000], mean action: 2.373 [0.000, 5.000],  loss: 28.703405, mean_q: 264.030368, mean_eps: 0.422360\n",
            " 642844/1000000: episode: 929, duration: 3.769s, episode steps: 630, steps per second: 167, episode reward: 110.000, mean reward:  0.175 [ 0.000, 30.000], mean action: 2.644 [0.000, 5.000],  loss: 21.671517, mean_q: 281.038000, mean_eps: 0.421724\n",
            " 643431/1000000: episode: 930, duration: 3.489s, episode steps: 587, steps per second: 168, episode reward: 45.000, mean reward:  0.077 [ 0.000, 15.000], mean action: 2.635 [0.000, 5.000],  loss: 21.381220, mean_q: 283.288414, mean_eps: 0.421177\n",
            " 644128/1000000: episode: 931, duration: 4.152s, episode steps: 697, steps per second: 168, episode reward: 115.000, mean reward:  0.165 [ 0.000, 30.000], mean action: 2.588 [0.000, 5.000],  loss: 25.077077, mean_q: 281.853551, mean_eps: 0.420599\n",
            " 644456/1000000: episode: 932, duration: 1.997s, episode steps: 328, steps per second: 164, episode reward: 20.000, mean reward:  0.061 [ 0.000, 20.000], mean action: 2.378 [0.000, 5.000],  loss: 36.988725, mean_q: 288.614168, mean_eps: 0.420138\n",
            " 644975/1000000: episode: 933, duration: 3.083s, episode steps: 519, steps per second: 168, episode reward: 120.000, mean reward:  0.231 [ 0.000, 30.000], mean action: 2.701 [0.000, 5.000],  loss: 73.127628, mean_q: 290.173099, mean_eps: 0.419756\n",
            " 645899/1000000: episode: 934, duration: 5.462s, episode steps: 924, steps per second: 169, episode reward: 210.000, mean reward:  0.227 [ 0.000, 30.000], mean action: 2.560 [0.000, 5.000],  loss: 39.700275, mean_q: 273.140950, mean_eps: 0.419107\n",
            " 646803/1000000: episode: 935, duration: 5.406s, episode steps: 904, steps per second: 167, episode reward: 160.000, mean reward:  0.177 [ 0.000, 30.000], mean action: 2.379 [0.000, 5.000],  loss: 24.277292, mean_q: 261.883605, mean_eps: 0.418285\n",
            " 647175/1000000: episode: 936, duration: 2.209s, episode steps: 372, steps per second: 168, episode reward: 40.000, mean reward:  0.108 [ 0.000, 15.000], mean action: 2.449 [0.000, 5.000],  loss: 24.103749, mean_q: 283.649183, mean_eps: 0.417710\n",
            " 647873/1000000: episode: 937, duration: 4.116s, episode steps: 698, steps per second: 170, episode reward: 105.000, mean reward:  0.150 [ 0.000, 30.000], mean action: 2.456 [0.000, 5.000],  loss: 52.666715, mean_q: 279.912029, mean_eps: 0.417229\n",
            " 648696/1000000: episode: 938, duration: 4.888s, episode steps: 823, steps per second: 168, episode reward: 210.000, mean reward:  0.255 [ 0.000, 30.000], mean action: 2.067 [0.000, 5.000],  loss: 25.647249, mean_q: 272.023437, mean_eps: 0.416544\n",
            " 649105/1000000: episode: 939, duration: 2.433s, episode steps: 409, steps per second: 168, episode reward: 80.000, mean reward:  0.196 [ 0.000, 20.000], mean action: 2.435 [0.000, 5.000],  loss: 20.687054, mean_q: 259.327241, mean_eps: 0.415990\n",
            " 649811/1000000: episode: 940, duration: 4.184s, episode steps: 706, steps per second: 169, episode reward: 60.000, mean reward:  0.085 [ 0.000, 20.000], mean action: 2.547 [0.000, 5.000],  loss: 29.796200, mean_q: 270.374355, mean_eps: 0.415488\n",
            " 651311/1000000: episode: 941, duration: 8.906s, episode steps: 1500, steps per second: 168, episode reward: 625.000, mean reward:  0.417 [ 0.000, 200.000], mean action: 2.255 [0.000, 5.000],  loss: 50.506068, mean_q: 278.233774, mean_eps: 0.414496\n",
            " 651689/1000000: episode: 942, duration: 2.294s, episode steps: 378, steps per second: 165, episode reward: 90.000, mean reward:  0.238 [ 0.000, 25.000], mean action: 2.426 [0.000, 5.000],  loss: 105.544626, mean_q: 261.816792, mean_eps: 0.413650\n",
            " 652701/1000000: episode: 943, duration: 6.037s, episode steps: 1012, steps per second: 168, episode reward: 345.000, mean reward:  0.341 [ 0.000, 30.000], mean action: 2.713 [0.000, 5.000],  loss: 48.664040, mean_q: 274.819522, mean_eps: 0.413025\n",
            " 653404/1000000: episode: 944, duration: 4.158s, episode steps: 703, steps per second: 169, episode reward: 215.000, mean reward:  0.306 [ 0.000, 30.000], mean action: 2.394 [0.000, 5.000],  loss: 26.466715, mean_q: 269.704647, mean_eps: 0.412253\n",
            " 654009/1000000: episode: 945, duration: 3.598s, episode steps: 605, steps per second: 168, episode reward: 110.000, mean reward:  0.182 [ 0.000, 30.000], mean action: 2.587 [0.000, 5.000],  loss: 19.939419, mean_q: 266.609775, mean_eps: 0.411665\n",
            " 654705/1000000: episode: 946, duration: 4.148s, episode steps: 696, steps per second: 168, episode reward: 135.000, mean reward:  0.194 [ 0.000, 30.000], mean action: 2.444 [0.000, 5.000],  loss: 25.855658, mean_q: 263.195013, mean_eps: 0.411079\n",
            " 655370/1000000: episode: 947, duration: 3.917s, episode steps: 665, steps per second: 170, episode reward: 155.000, mean reward:  0.233 [ 0.000, 30.000], mean action: 2.456 [0.000, 5.000],  loss: 25.268121, mean_q: 260.899642, mean_eps: 0.410467\n",
            " 656125/1000000: episode: 948, duration: 4.493s, episode steps: 755, steps per second: 168, episode reward: 150.000, mean reward:  0.199 [ 0.000, 25.000], mean action: 2.649 [0.000, 5.000],  loss: 20.191157, mean_q: 272.985396, mean_eps: 0.409828\n",
            " 656642/1000000: episode: 949, duration: 3.120s, episode steps: 517, steps per second: 166, episode reward: 85.000, mean reward:  0.164 [ 0.000, 30.000], mean action: 2.128 [0.000, 5.000],  loss: 32.974357, mean_q: 276.814028, mean_eps: 0.409255\n",
            " 657035/1000000: episode: 950, duration: 2.359s, episode steps: 393, steps per second: 167, episode reward: 35.000, mean reward:  0.089 [ 0.000, 15.000], mean action: 2.394 [0.000, 5.000],  loss: 48.992800, mean_q: 275.410165, mean_eps: 0.408846\n",
            " 657431/1000000: episode: 951, duration: 2.360s, episode steps: 396, steps per second: 168, episode reward: 40.000, mean reward:  0.101 [ 0.000, 15.000], mean action: 2.717 [0.000, 5.000],  loss: 48.420593, mean_q: 281.123654, mean_eps: 0.408491\n",
            " 658220/1000000: episode: 952, duration: 4.703s, episode steps: 789, steps per second: 168, episode reward: 150.000, mean reward:  0.190 [ 0.000, 30.000], mean action: 2.740 [0.000, 5.000],  loss: 57.498222, mean_q: 288.382867, mean_eps: 0.407957\n",
            " 658803/1000000: episode: 953, duration: 3.474s, episode steps: 583, steps per second: 168, episode reward: 10.000, mean reward:  0.017 [ 0.000,  5.000], mean action: 2.295 [0.000, 5.000],  loss: 36.570679, mean_q: 277.747924, mean_eps: 0.407340\n",
            " 660225/1000000: episode: 954, duration: 8.418s, episode steps: 1422, steps per second: 169, episode reward: 590.000, mean reward:  0.415 [ 0.000, 30.000], mean action: 2.667 [0.000, 5.000],  loss: 37.205201, mean_q: 279.092021, mean_eps: 0.406438\n",
            " 660873/1000000: episode: 955, duration: 3.867s, episode steps: 648, steps per second: 168, episode reward: 180.000, mean reward:  0.278 [ 0.000, 30.000], mean action: 2.418 [0.000, 5.000],  loss: 50.798799, mean_q: 252.832383, mean_eps: 0.405506\n",
            " 661263/1000000: episode: 956, duration: 2.345s, episode steps: 390, steps per second: 166, episode reward: 30.000, mean reward:  0.077 [ 0.000, 15.000], mean action: 2.138 [0.000, 5.000],  loss: 34.197890, mean_q: 268.138496, mean_eps: 0.405039\n",
            " 662021/1000000: episode: 957, duration: 4.538s, episode steps: 758, steps per second: 167, episode reward: 145.000, mean reward:  0.191 [ 0.000, 30.000], mean action: 3.142 [0.000, 5.000],  loss: 45.718034, mean_q: 275.713173, mean_eps: 0.404523\n",
            " 663053/1000000: episode: 958, duration: 6.193s, episode steps: 1032, steps per second: 167, episode reward: 210.000, mean reward:  0.203 [ 0.000, 30.000], mean action: 2.560 [0.000, 5.000],  loss: 33.083175, mean_q: 270.751134, mean_eps: 0.403717\n",
            " 664007/1000000: episode: 959, duration: 5.692s, episode steps: 954, steps per second: 168, episode reward: 370.000, mean reward:  0.388 [ 0.000, 200.000], mean action: 2.302 [0.000, 5.000],  loss: 49.527363, mean_q: 281.574571, mean_eps: 0.402823\n",
            " 665084/1000000: episode: 960, duration: 6.413s, episode steps: 1077, steps per second: 168, episode reward: 265.000, mean reward:  0.246 [ 0.000, 30.000], mean action: 2.213 [0.000, 5.000],  loss: 39.882264, mean_q: 278.937838, mean_eps: 0.401909\n",
            " 665656/1000000: episode: 961, duration: 3.425s, episode steps: 572, steps per second: 167, episode reward: 115.000, mean reward:  0.201 [ 0.000, 30.000], mean action: 2.628 [0.000, 5.000],  loss: 50.279005, mean_q: 281.826974, mean_eps: 0.401167\n",
            " 666547/1000000: episode: 962, duration: 5.300s, episode steps: 891, steps per second: 168, episode reward: 345.000, mean reward:  0.387 [ 0.000, 200.000], mean action: 2.816 [0.000, 5.000],  loss: 48.657754, mean_q: 269.698570, mean_eps: 0.400509\n",
            " 667513/1000000: episode: 963, duration: 5.753s, episode steps: 966, steps per second: 168, episode reward: 465.000, mean reward:  0.481 [ 0.000, 200.000], mean action: 2.300 [0.000, 5.000],  loss: 34.400538, mean_q: 267.551467, mean_eps: 0.399673\n",
            " 668158/1000000: episode: 964, duration: 3.854s, episode steps: 645, steps per second: 167, episode reward: 215.000, mean reward:  0.333 [ 0.000, 30.000], mean action: 2.515 [0.000, 5.000],  loss: 28.050972, mean_q: 262.031571, mean_eps: 0.398948\n",
            " 668676/1000000: episode: 965, duration: 3.086s, episode steps: 518, steps per second: 168, episode reward: 90.000, mean reward:  0.174 [ 0.000, 25.000], mean action: 2.618 [0.000, 5.000],  loss: 14.474901, mean_q: 265.146399, mean_eps: 0.398425\n",
            " 669148/1000000: episode: 966, duration: 2.807s, episode steps: 472, steps per second: 168, episode reward: 65.000, mean reward:  0.138 [ 0.000, 20.000], mean action: 2.458 [0.000, 5.000],  loss: 26.170980, mean_q: 266.923950, mean_eps: 0.397980\n",
            " 670343/1000000: episode: 967, duration: 7.075s, episode steps: 1195, steps per second: 169, episode reward: 550.000, mean reward:  0.460 [ 0.000, 200.000], mean action: 2.110 [0.000, 5.000],  loss: 45.740163, mean_q: 276.807686, mean_eps: 0.397229\n",
            " 670935/1000000: episode: 968, duration: 3.534s, episode steps: 592, steps per second: 168, episode reward: 80.000, mean reward:  0.135 [ 0.000, 20.000], mean action: 1.949 [0.000, 5.000],  loss: 32.717509, mean_q: 278.683134, mean_eps: 0.396425\n",
            " 671976/1000000: episode: 969, duration: 6.200s, episode steps: 1041, steps per second: 168, episode reward: 205.000, mean reward:  0.197 [ 0.000, 30.000], mean action: 2.548 [0.000, 5.000],  loss: 26.764109, mean_q: 276.750484, mean_eps: 0.395690\n",
            " 672596/1000000: episode: 970, duration: 3.733s, episode steps: 620, steps per second: 166, episode reward: 100.000, mean reward:  0.161 [ 0.000, 30.000], mean action: 2.503 [0.000, 5.000],  loss: 24.595423, mean_q: 268.853099, mean_eps: 0.394943\n",
            " 673653/1000000: episode: 971, duration: 6.284s, episode steps: 1057, steps per second: 168, episode reward: 510.000, mean reward:  0.482 [ 0.000, 200.000], mean action: 2.267 [0.000, 5.000],  loss: 41.921980, mean_q: 268.534179, mean_eps: 0.394188\n",
            " 674214/1000000: episode: 972, duration: 3.349s, episode steps: 561, steps per second: 168, episode reward: 105.000, mean reward:  0.187 [ 0.000, 25.000], mean action: 2.367 [0.000, 5.000],  loss: 39.945221, mean_q: 273.159236, mean_eps: 0.393460\n",
            " 674922/1000000: episode: 973, duration: 4.232s, episode steps: 708, steps per second: 167, episode reward: 165.000, mean reward:  0.233 [ 0.000, 30.000], mean action: 2.082 [0.000, 5.000],  loss: 36.106966, mean_q: 272.226860, mean_eps: 0.392889\n",
            " 675998/1000000: episode: 974, duration: 6.483s, episode steps: 1076, steps per second: 166, episode reward: 180.000, mean reward:  0.167 [ 0.000, 25.000], mean action: 2.303 [0.000, 5.000],  loss: 30.520850, mean_q: 285.006633, mean_eps: 0.392086\n",
            " 676661/1000000: episode: 975, duration: 3.956s, episode steps: 663, steps per second: 168, episode reward: 105.000, mean reward:  0.158 [ 0.000, 30.000], mean action: 2.834 [0.000, 5.000],  loss: 35.028825, mean_q: 286.630672, mean_eps: 0.391304\n",
            " 677493/1000000: episode: 976, duration: 4.964s, episode steps: 832, steps per second: 168, episode reward: 210.000, mean reward:  0.252 [ 0.000, 30.000], mean action: 2.075 [0.000, 5.000],  loss: 24.149626, mean_q: 267.702077, mean_eps: 0.390631\n",
            " 678235/1000000: episode: 977, duration: 4.423s, episode steps: 742, steps per second: 168, episode reward: 180.000, mean reward:  0.243 [ 0.000, 30.000], mean action: 2.280 [0.000, 5.000],  loss: 23.759912, mean_q: 265.567466, mean_eps: 0.389923\n",
            " 678853/1000000: episode: 978, duration: 3.698s, episode steps: 618, steps per second: 167, episode reward: 165.000, mean reward:  0.267 [ 0.000, 30.000], mean action: 2.767 [0.000, 5.000],  loss: 25.416631, mean_q: 265.683692, mean_eps: 0.389311\n",
            " 679269/1000000: episode: 979, duration: 2.484s, episode steps: 416, steps per second: 167, episode reward: 40.000, mean reward:  0.096 [ 0.000, 15.000], mean action: 2.416 [0.000, 5.000],  loss: 28.967512, mean_q: 264.939511, mean_eps: 0.388846\n",
            " 680322/1000000: episode: 980, duration: 6.241s, episode steps: 1053, steps per second: 169, episode reward: 265.000, mean reward:  0.252 [ 0.000, 30.000], mean action: 2.387 [0.000, 5.000],  loss: 42.205483, mean_q: 281.040989, mean_eps: 0.388184\n",
            " 680958/1000000: episode: 981, duration: 3.814s, episode steps: 636, steps per second: 167, episode reward: 105.000, mean reward:  0.165 [ 0.000, 25.000], mean action: 2.546 [0.000, 5.000],  loss: 22.070975, mean_q: 279.429991, mean_eps: 0.387424\n",
            " 682436/1000000: episode: 982, duration: 8.791s, episode steps: 1478, steps per second: 168, episode reward: 435.000, mean reward:  0.294 [ 0.000, 30.000], mean action: 2.982 [0.000, 5.000],  loss: 32.389546, mean_q: 274.154580, mean_eps: 0.386473\n",
            " 682882/1000000: episode: 983, duration: 2.668s, episode steps: 446, steps per second: 167, episode reward: 80.000, mean reward:  0.179 [ 0.000, 20.000], mean action: 2.388 [0.000, 5.000],  loss: 64.239909, mean_q: 250.840938, mean_eps: 0.385607\n",
            " 683818/1000000: episode: 984, duration: 5.637s, episode steps: 936, steps per second: 166, episode reward: 255.000, mean reward:  0.272 [ 0.000, 30.000], mean action: 2.244 [0.000, 5.000],  loss: 53.340361, mean_q: 267.994908, mean_eps: 0.384985\n",
            " 684641/1000000: episode: 985, duration: 4.917s, episode steps: 823, steps per second: 167, episode reward: 135.000, mean reward:  0.164 [ 0.000, 30.000], mean action: 2.254 [0.000, 5.000],  loss: 24.328309, mean_q: 280.616769, mean_eps: 0.384194\n",
            " 685022/1000000: episode: 986, duration: 2.288s, episode steps: 381, steps per second: 167, episode reward: 30.000, mean reward:  0.079 [ 0.000, 15.000], mean action: 2.588 [0.000, 5.000],  loss: 31.938894, mean_q: 256.407510, mean_eps: 0.383652\n",
            " 685620/1000000: episode: 987, duration: 3.563s, episode steps: 598, steps per second: 168, episode reward: 55.000, mean reward:  0.092 [ 0.000, 20.000], mean action: 2.323 [0.000, 5.000],  loss: 43.609728, mean_q: 258.932881, mean_eps: 0.383212\n",
            " 686572/1000000: episode: 988, duration: 5.631s, episode steps: 952, steps per second: 169, episode reward: 215.000, mean reward:  0.226 [ 0.000, 30.000], mean action: 2.399 [0.000, 5.000],  loss: 33.306925, mean_q: 271.355347, mean_eps: 0.382514\n",
            " 686987/1000000: episode: 989, duration: 2.472s, episode steps: 415, steps per second: 168, episode reward: 80.000, mean reward:  0.193 [ 0.000, 20.000], mean action: 2.634 [0.000, 5.000],  loss: 14.157709, mean_q: 267.795483, mean_eps: 0.381899\n",
            " 688318/1000000: episode: 990, duration: 7.880s, episode steps: 1331, steps per second: 169, episode reward: 340.000, mean reward:  0.255 [ 0.000, 30.000], mean action: 2.750 [0.000, 5.000],  loss: 37.889538, mean_q: 264.852825, mean_eps: 0.381113\n",
            " 689419/1000000: episode: 991, duration: 6.524s, episode steps: 1101, steps per second: 169, episode reward: 470.000, mean reward:  0.427 [ 0.000, 200.000], mean action: 2.463 [0.000, 5.000],  loss: 35.981390, mean_q: 270.415063, mean_eps: 0.380019\n",
            " 690088/1000000: episode: 992, duration: 4.012s, episode steps: 669, steps per second: 167, episode reward: 165.000, mean reward:  0.247 [ 0.000, 25.000], mean action: 2.345 [0.000, 5.000],  loss: 59.939845, mean_q: 277.962737, mean_eps: 0.379222\n",
            " 690698/1000000: episode: 993, duration: 3.597s, episode steps: 610, steps per second: 170, episode reward: 155.000, mean reward:  0.254 [ 0.000, 30.000], mean action: 2.877 [0.000, 5.000],  loss: 46.781046, mean_q: 271.898428, mean_eps: 0.378647\n",
            " 691398/1000000: episode: 994, duration: 4.167s, episode steps: 700, steps per second: 168, episode reward: 180.000, mean reward:  0.257 [ 0.000, 30.000], mean action: 2.211 [0.000, 5.000],  loss: 31.885233, mean_q: 273.543677, mean_eps: 0.378057\n",
            " 691895/1000000: episode: 995, duration: 2.956s, episode steps: 497, steps per second: 168, episode reward: 50.000, mean reward:  0.101 [ 0.000, 15.000], mean action: 2.362 [0.000, 5.000],  loss: 38.073758, mean_q: 273.721966, mean_eps: 0.377519\n",
            " 692266/1000000: episode: 996, duration: 2.231s, episode steps: 371, steps per second: 166, episode reward: 55.000, mean reward:  0.148 [ 0.000, 30.000], mean action: 3.086 [0.000, 5.000],  loss: 32.579744, mean_q: 276.641715, mean_eps: 0.377128\n",
            " 692732/1000000: episode: 997, duration: 2.802s, episode steps: 466, steps per second: 166, episode reward: 80.000, mean reward:  0.172 [ 0.000, 25.000], mean action: 2.039 [0.000, 5.000],  loss: 60.054161, mean_q: 280.588016, mean_eps: 0.376751\n",
            " 693286/1000000: episode: 998, duration: 3.306s, episode steps: 554, steps per second: 168, episode reward: 65.000, mean reward:  0.117 [ 0.000, 20.000], mean action: 2.088 [0.000, 5.000],  loss: 56.964948, mean_q: 277.337136, mean_eps: 0.376292\n",
            " 693917/1000000: episode: 999, duration: 3.750s, episode steps: 631, steps per second: 168, episode reward: 75.000, mean reward:  0.119 [ 0.000, 25.000], mean action: 2.409 [0.000, 5.000],  loss: 49.148124, mean_q: 279.511142, mean_eps: 0.375759\n",
            " 694309/1000000: episode: 1000, duration: 2.358s, episode steps: 392, steps per second: 166, episode reward: 25.000, mean reward:  0.064 [ 0.000, 15.000], mean action: 2.844 [0.000, 5.000],  loss: 29.488268, mean_q: 278.100788, mean_eps: 0.375299\n",
            " 695444/1000000: episode: 1001, duration: 6.778s, episode steps: 1135, steps per second: 167, episode reward: 315.000, mean reward:  0.278 [ 0.000, 30.000], mean action: 2.131 [0.000, 5.000],  loss: 39.043062, mean_q: 279.149927, mean_eps: 0.374612\n",
            " 696116/1000000: episode: 1002, duration: 4.011s, episode steps: 672, steps per second: 168, episode reward: 155.000, mean reward:  0.231 [ 0.000, 30.000], mean action: 2.835 [0.000, 5.000],  loss: 25.843987, mean_q: 263.043209, mean_eps: 0.373798\n",
            " 696937/1000000: episode: 1003, duration: 4.925s, episode steps: 821, steps per second: 167, episode reward: 180.000, mean reward:  0.219 [ 0.000, 30.000], mean action: 2.819 [0.000, 5.000],  loss: 23.554781, mean_q: 268.092168, mean_eps: 0.373127\n",
            " 697446/1000000: episode: 1004, duration: 3.040s, episode steps: 509, steps per second: 167, episode reward: 125.000, mean reward:  0.246 [ 0.000, 25.000], mean action: 2.193 [0.000, 5.000],  loss: 22.544121, mean_q: 275.381711, mean_eps: 0.372528\n",
            " 698101/1000000: episode: 1005, duration: 3.895s, episode steps: 655, steps per second: 168, episode reward: 60.000, mean reward:  0.092 [ 0.000, 20.000], mean action: 2.327 [0.000, 5.000],  loss: 57.058185, mean_q: 274.177377, mean_eps: 0.372004\n",
            " 698821/1000000: episode: 1006, duration: 4.309s, episode steps: 720, steps per second: 167, episode reward: 150.000, mean reward:  0.208 [ 0.000, 25.000], mean action: 2.390 [0.000, 5.000],  loss: 32.899773, mean_q: 277.204342, mean_eps: 0.371386\n",
            " 699758/1000000: episode: 1007, duration: 5.561s, episode steps: 937, steps per second: 168, episode reward: 335.000, mean reward:  0.358 [ 0.000, 30.000], mean action: 2.458 [0.000, 5.000],  loss: 36.676662, mean_q: 281.874906, mean_eps: 0.370640\n",
            " 700569/1000000: episode: 1008, duration: 4.862s, episode steps: 811, steps per second: 167, episode reward: 150.000, mean reward:  0.185 [ 0.000, 25.000], mean action: 2.385 [0.000, 5.000],  loss: 29.564795, mean_q: 282.141750, mean_eps: 0.369853\n",
            " 701375/1000000: episode: 1009, duration: 4.848s, episode steps: 806, steps per second: 166, episode reward: 240.000, mean reward:  0.298 [ 0.000, 30.000], mean action: 3.063 [0.000, 5.000],  loss: 27.747541, mean_q: 274.842544, mean_eps: 0.369126\n",
            " 701922/1000000: episode: 1010, duration: 3.267s, episode steps: 547, steps per second: 167, episode reward: 75.000, mean reward:  0.137 [ 0.000, 20.000], mean action: 2.106 [0.000, 5.000],  loss: 26.687705, mean_q: 273.852972, mean_eps: 0.368517\n",
            " 702727/1000000: episode: 1011, duration: 4.816s, episode steps: 805, steps per second: 167, episode reward: 180.000, mean reward:  0.224 [ 0.000, 30.000], mean action: 2.923 [0.000, 5.000],  loss: 32.808386, mean_q: 268.485500, mean_eps: 0.367908\n",
            " 703371/1000000: episode: 1012, duration: 3.908s, episode steps: 644, steps per second: 165, episode reward: 100.000, mean reward:  0.155 [ 0.000, 30.000], mean action: 2.565 [0.000, 5.000],  loss: 18.958775, mean_q: 258.648344, mean_eps: 0.367256\n",
            " 704009/1000000: episode: 1013, duration: 3.859s, episode steps: 638, steps per second: 165, episode reward: 175.000, mean reward:  0.274 [ 0.000, 25.000], mean action: 2.406 [0.000, 5.000],  loss: 34.604276, mean_q: 269.772491, mean_eps: 0.366679\n",
            " 704657/1000000: episode: 1014, duration: 3.915s, episode steps: 648, steps per second: 166, episode reward: 135.000, mean reward:  0.208 [ 0.000, 30.000], mean action: 1.937 [0.000, 5.000],  loss: 39.223717, mean_q: 272.356499, mean_eps: 0.366101\n",
            " 705315/1000000: episode: 1015, duration: 3.966s, episode steps: 658, steps per second: 166, episode reward: 160.000, mean reward:  0.243 [ 0.000, 30.000], mean action: 2.094 [0.000, 5.000],  loss: 29.146439, mean_q: 267.398586, mean_eps: 0.365513\n",
            " 706187/1000000: episode: 1016, duration: 5.212s, episode steps: 872, steps per second: 167, episode reward: 205.000, mean reward:  0.235 [ 0.000, 25.000], mean action: 2.289 [0.000, 5.000],  loss: 39.861570, mean_q: 272.334791, mean_eps: 0.364825\n",
            " 706541/1000000: episode: 1017, duration: 2.124s, episode steps: 354, steps per second: 167, episode reward: 55.000, mean reward:  0.155 [ 0.000, 20.000], mean action: 2.867 [0.000, 5.000],  loss: 31.413389, mean_q: 273.361764, mean_eps: 0.364273\n",
            " 707262/1000000: episode: 1018, duration: 4.304s, episode steps: 721, steps per second: 168, episode reward: 105.000, mean reward:  0.146 [ 0.000, 30.000], mean action: 2.646 [0.000, 5.000],  loss: 55.014976, mean_q: 268.356350, mean_eps: 0.363789\n",
            " 707696/1000000: episode: 1019, duration: 2.603s, episode steps: 434, steps per second: 167, episode reward: 30.000, mean reward:  0.069 [ 0.000, 10.000], mean action: 2.343 [0.000, 5.000],  loss: 57.106216, mean_q: 265.299711, mean_eps: 0.363269\n",
            " 708109/1000000: episode: 1020, duration: 2.492s, episode steps: 413, steps per second: 166, episode reward: 105.000, mean reward:  0.254 [ 0.000, 25.000], mean action: 2.547 [0.000, 5.000],  loss: 54.023876, mean_q: 268.392470, mean_eps: 0.362888\n",
            " 708750/1000000: episode: 1021, duration: 3.849s, episode steps: 641, steps per second: 167, episode reward: 210.000, mean reward:  0.328 [ 0.000, 30.000], mean action: 2.710 [0.000, 5.000],  loss: 73.750988, mean_q: 275.653314, mean_eps: 0.362414\n",
            " 709318/1000000: episode: 1022, duration: 3.430s, episode steps: 568, steps per second: 166, episode reward: 100.000, mean reward:  0.176 [ 0.000, 20.000], mean action: 2.570 [0.000, 5.000],  loss: 42.300956, mean_q: 273.471442, mean_eps: 0.361870\n",
            " 710175/1000000: episode: 1023, duration: 5.209s, episode steps: 857, steps per second: 165, episode reward: 210.000, mean reward:  0.245 [ 0.000, 30.000], mean action: 2.118 [0.000, 5.000],  loss: 29.803762, mean_q: 274.973507, mean_eps: 0.361229\n",
            " 710878/1000000: episode: 1024, duration: 4.253s, episode steps: 703, steps per second: 165, episode reward: 135.000, mean reward:  0.192 [ 0.000, 25.000], mean action: 2.203 [0.000, 5.000],  loss: 27.103276, mean_q: 263.804044, mean_eps: 0.360527\n",
            " 712185/1000000: episode: 1025, duration: 7.828s, episode steps: 1307, steps per second: 167, episode reward: 210.000, mean reward:  0.161 [ 0.000, 30.000], mean action: 2.689 [0.000, 5.000],  loss: 26.285040, mean_q: 274.970280, mean_eps: 0.359622\n",
            " 713222/1000000: episode: 1026, duration: 6.214s, episode steps: 1037, steps per second: 167, episode reward: 260.000, mean reward:  0.251 [ 0.000, 30.000], mean action: 2.187 [0.000, 5.000],  loss: 24.270772, mean_q: 265.361543, mean_eps: 0.358567\n",
            " 713793/1000000: episode: 1027, duration: 3.429s, episode steps: 571, steps per second: 167, episode reward: 110.000, mean reward:  0.193 [ 0.000, 30.000], mean action: 2.392 [0.000, 5.000],  loss: 22.090784, mean_q: 275.101780, mean_eps: 0.357844\n",
            " 714501/1000000: episode: 1028, duration: 4.277s, episode steps: 708, steps per second: 166, episode reward: 140.000, mean reward:  0.198 [ 0.000, 30.000], mean action: 2.232 [0.000, 5.000],  loss: 27.413818, mean_q: 257.726246, mean_eps: 0.357268\n",
            " 715135/1000000: episode: 1029, duration: 3.832s, episode steps: 634, steps per second: 165, episode reward: 120.000, mean reward:  0.189 [ 0.000, 30.000], mean action: 2.238 [0.000, 5.000],  loss: 26.913601, mean_q: 253.901310, mean_eps: 0.356664\n",
            " 715790/1000000: episode: 1030, duration: 3.958s, episode steps: 655, steps per second: 166, episode reward: 120.000, mean reward:  0.183 [ 0.000, 30.000], mean action: 2.569 [0.000, 5.000],  loss: 16.885003, mean_q: 253.498864, mean_eps: 0.356084\n",
            " 716440/1000000: episode: 1031, duration: 3.932s, episode steps: 650, steps per second: 165, episode reward: 135.000, mean reward:  0.208 [ 0.000, 30.000], mean action: 2.157 [0.000, 5.000],  loss: 19.975653, mean_q: 254.964522, mean_eps: 0.355497\n",
            " 717247/1000000: episode: 1032, duration: 4.879s, episode steps: 807, steps per second: 165, episode reward: 205.000, mean reward:  0.254 [ 0.000, 30.000], mean action: 2.475 [0.000, 5.000],  loss: 21.115024, mean_q: 266.242054, mean_eps: 0.354841\n",
            " 717681/1000000: episode: 1033, duration: 2.584s, episode steps: 434, steps per second: 168, episode reward: 50.000, mean reward:  0.115 [ 0.000, 15.000], mean action: 2.627 [0.000, 5.000],  loss: 27.727257, mean_q: 262.442699, mean_eps: 0.354283\n",
            " 718271/1000000: episode: 1034, duration: 3.561s, episode steps: 590, steps per second: 166, episode reward: 75.000, mean reward:  0.127 [ 0.000, 15.000], mean action: 2.000 [0.000, 5.000],  loss: 51.615479, mean_q: 273.182978, mean_eps: 0.353822\n",
            " 719212/1000000: episode: 1035, duration: 5.623s, episode steps: 941, steps per second: 167, episode reward: 390.000, mean reward:  0.414 [ 0.000, 200.000], mean action: 2.405 [0.000, 5.000],  loss: 39.516200, mean_q: 277.707447, mean_eps: 0.353133\n",
            " 719990/1000000: episode: 1036, duration: 4.672s, episode steps: 778, steps per second: 167, episode reward: 120.000, mean reward:  0.154 [ 0.000, 30.000], mean action: 2.856 [0.000, 5.000],  loss: 27.225791, mean_q: 270.241609, mean_eps: 0.352360\n",
            " 720627/1000000: episode: 1037, duration: 3.821s, episode steps: 637, steps per second: 167, episode reward: 140.000, mean reward:  0.220 [ 0.000, 30.000], mean action: 3.027 [0.000, 5.000],  loss: 18.982288, mean_q: 262.031552, mean_eps: 0.351723\n",
            " 721448/1000000: episode: 1038, duration: 4.936s, episode steps: 821, steps per second: 166, episode reward: 190.000, mean reward:  0.231 [ 0.000, 25.000], mean action: 2.197 [0.000, 5.000],  loss: 15.780941, mean_q: 266.599826, mean_eps: 0.351067\n",
            " 722388/1000000: episode: 1039, duration: 5.623s, episode steps: 940, steps per second: 167, episode reward: 240.000, mean reward:  0.255 [ 0.000, 30.000], mean action: 2.944 [0.000, 5.000],  loss: 22.698534, mean_q: 270.454940, mean_eps: 0.350274\n",
            " 722980/1000000: episode: 1040, duration: 3.528s, episode steps: 592, steps per second: 168, episode reward: 120.000, mean reward:  0.203 [ 0.000, 30.000], mean action: 2.189 [0.000, 5.000],  loss: 12.538993, mean_q: 261.707644, mean_eps: 0.349585\n",
            " 723816/1000000: episode: 1041, duration: 4.977s, episode steps: 836, steps per second: 168, episode reward: 385.000, mean reward:  0.461 [ 0.000, 200.000], mean action: 2.670 [0.000, 5.000],  loss: 28.589372, mean_q: 257.984352, mean_eps: 0.348942\n",
            " 724519/1000000: episode: 1042, duration: 4.214s, episode steps: 703, steps per second: 167, episode reward: 85.000, mean reward:  0.121 [ 0.000, 20.000], mean action: 2.743 [0.000, 5.000],  loss: 35.669666, mean_q: 252.530180, mean_eps: 0.348250\n",
            " 725360/1000000: episode: 1043, duration: 5.071s, episode steps: 841, steps per second: 166, episode reward: 225.000, mean reward:  0.268 [ 0.000, 30.000], mean action: 2.295 [0.000, 5.000],  loss: 21.967268, mean_q: 268.657457, mean_eps: 0.347555\n",
            " 726373/1000000: episode: 1044, duration: 6.042s, episode steps: 1013, steps per second: 168, episode reward: 195.000, mean reward:  0.192 [ 0.000, 30.000], mean action: 2.993 [0.000, 5.000],  loss: 22.902846, mean_q: 265.077181, mean_eps: 0.346721\n",
            " 727012/1000000: episode: 1045, duration: 3.826s, episode steps: 639, steps per second: 167, episode reward: 105.000, mean reward:  0.164 [ 0.000, 30.000], mean action: 2.474 [0.000, 5.000],  loss: 19.753171, mean_q: 252.815450, mean_eps: 0.345977\n",
            " 727368/1000000: episode: 1046, duration: 2.167s, episode steps: 356, steps per second: 164, episode reward: 10.000, mean reward:  0.028 [ 0.000,  5.000], mean action: 3.244 [0.000, 5.000],  loss: 28.622465, mean_q: 256.221628, mean_eps: 0.345529\n",
            " 727719/1000000: episode: 1047, duration: 2.127s, episode steps: 351, steps per second: 165, episode reward: 50.000, mean reward:  0.142 [ 0.000, 20.000], mean action: 2.886 [0.000, 5.000],  loss: 40.431800, mean_q: 268.261040, mean_eps: 0.345211\n",
            " 728449/1000000: episode: 1048, duration: 4.351s, episode steps: 730, steps per second: 168, episode reward: 135.000, mean reward:  0.185 [ 0.000, 30.000], mean action: 2.584 [0.000, 5.000],  loss: 67.371152, mean_q: 268.374245, mean_eps: 0.344725\n",
            " 728907/1000000: episode: 1049, duration: 2.753s, episode steps: 458, steps per second: 166, episode reward: 180.000, mean reward:  0.393 [ 0.000, 30.000], mean action: 2.345 [0.000, 5.000],  loss: 28.710631, mean_q: 263.608714, mean_eps: 0.344190\n",
            " 729615/1000000: episode: 1050, duration: 4.228s, episode steps: 708, steps per second: 167, episode reward: 145.000, mean reward:  0.205 [ 0.000, 30.000], mean action: 2.508 [0.000, 5.000],  loss: 25.306194, mean_q: 261.230814, mean_eps: 0.343666\n",
            " 729947/1000000: episode: 1051, duration: 2.018s, episode steps: 332, steps per second: 164, episode reward: 20.000, mean reward:  0.060 [ 0.000, 10.000], mean action: 2.560 [0.000, 5.000],  loss: 26.294940, mean_q: 260.476084, mean_eps: 0.343198\n",
            " 730343/1000000: episode: 1052, duration: 2.400s, episode steps: 396, steps per second: 165, episode reward: 80.000, mean reward:  0.202 [ 0.000, 30.000], mean action: 2.452 [0.000, 5.000],  loss: 48.574625, mean_q: 261.401847, mean_eps: 0.342870\n",
            " 730830/1000000: episode: 1053, duration: 2.917s, episode steps: 487, steps per second: 167, episode reward: 65.000, mean reward:  0.133 [ 0.000, 20.000], mean action: 2.647 [0.000, 5.000],  loss: 71.964088, mean_q: 261.601699, mean_eps: 0.342473\n",
            " 731508/1000000: episode: 1054, duration: 4.067s, episode steps: 678, steps per second: 167, episode reward: 185.000, mean reward:  0.273 [ 0.000, 30.000], mean action: 2.515 [0.000, 5.000],  loss: 54.963300, mean_q: 265.633624, mean_eps: 0.341948\n",
            " 731946/1000000: episode: 1055, duration: 2.640s, episode steps: 438, steps per second: 166, episode reward: 30.000, mean reward:  0.068 [ 0.000, 10.000], mean action: 2.566 [0.000, 5.000],  loss: 40.479036, mean_q: 267.193656, mean_eps: 0.341446\n",
            " 733095/1000000: episode: 1056, duration: 6.814s, episode steps: 1149, steps per second: 169, episode reward: 320.000, mean reward:  0.279 [ 0.000, 30.000], mean action: 2.865 [0.000, 5.000],  loss: 37.515658, mean_q: 275.144516, mean_eps: 0.340732\n",
            " 734263/1000000: episode: 1057, duration: 6.932s, episode steps: 1168, steps per second: 168, episode reward: 295.000, mean reward:  0.253 [ 0.000, 30.000], mean action: 2.746 [0.000, 5.000],  loss: 38.231004, mean_q: 250.584873, mean_eps: 0.339689\n",
            " 735085/1000000: episode: 1058, duration: 4.941s, episode steps: 822, steps per second: 166, episode reward: 120.000, mean reward:  0.146 [ 0.000, 20.000], mean action: 2.745 [0.000, 5.000],  loss: 21.822607, mean_q: 238.676890, mean_eps: 0.338794\n",
            " 735452/1000000: episode: 1059, duration: 2.186s, episode steps: 367, steps per second: 168, episode reward: 100.000, mean reward:  0.272 [ 0.000, 25.000], mean action: 2.616 [0.000, 5.000],  loss: 20.864926, mean_q: 277.518683, mean_eps: 0.338259\n",
            " 736633/1000000: episode: 1060, duration: 7.021s, episode steps: 1181, steps per second: 168, episode reward: 390.000, mean reward:  0.330 [ 0.000, 30.000], mean action: 2.315 [0.000, 5.000],  loss: 41.126049, mean_q: 275.179560, mean_eps: 0.337562\n",
            " 737460/1000000: episode: 1061, duration: 4.973s, episode steps: 827, steps per second: 166, episode reward: 125.000, mean reward:  0.151 [ 0.000, 30.000], mean action: 2.852 [0.000, 5.000],  loss: 22.629415, mean_q: 259.080485, mean_eps: 0.336659\n",
            " 738086/1000000: episode: 1062, duration: 3.723s, episode steps: 626, steps per second: 168, episode reward: 105.000, mean reward:  0.168 [ 0.000, 30.000], mean action: 2.393 [0.000, 5.000],  loss: 14.927680, mean_q: 252.179255, mean_eps: 0.336005\n",
            " 738842/1000000: episode: 1063, duration: 4.483s, episode steps: 756, steps per second: 169, episode reward: 105.000, mean reward:  0.139 [ 0.000, 30.000], mean action: 2.844 [0.000, 5.000],  loss: 14.530973, mean_q: 256.158394, mean_eps: 0.335383\n",
            " 739579/1000000: episode: 1064, duration: 4.421s, episode steps: 737, steps per second: 167, episode reward: 110.000, mean reward:  0.149 [ 0.000, 30.000], mean action: 2.357 [0.000, 5.000],  loss: 16.025534, mean_q: 254.032352, mean_eps: 0.334711\n",
            " 740127/1000000: episode: 1065, duration: 3.315s, episode steps: 548, steps per second: 165, episode reward: 65.000, mean reward:  0.119 [ 0.000, 20.000], mean action: 2.250 [0.000, 5.000],  loss: 14.229153, mean_q: 259.587067, mean_eps: 0.334133\n",
            " 741051/1000000: episode: 1066, duration: 5.496s, episode steps: 924, steps per second: 168, episode reward: 235.000, mean reward:  0.254 [ 0.000, 30.000], mean action: 2.121 [0.000, 5.000],  loss: 27.080442, mean_q: 258.633058, mean_eps: 0.333470\n",
            " 742072/1000000: episode: 1067, duration: 6.065s, episode steps: 1021, steps per second: 168, episode reward: 250.000, mean reward:  0.245 [ 0.000, 30.000], mean action: 2.388 [0.000, 5.000],  loss: 16.627594, mean_q: 247.263530, mean_eps: 0.332595\n",
            " 742450/1000000: episode: 1068, duration: 2.299s, episode steps: 378, steps per second: 164, episode reward: 45.000, mean reward:  0.119 [ 0.000, 15.000], mean action: 2.206 [0.000, 5.000],  loss: 22.696557, mean_q: 273.209549, mean_eps: 0.331966\n",
            " 743094/1000000: episode: 1069, duration: 3.869s, episode steps: 644, steps per second: 166, episode reward: 135.000, mean reward:  0.210 [ 0.000, 30.000], mean action: 2.385 [0.000, 5.000],  loss: 36.139545, mean_q: 268.974955, mean_eps: 0.331506\n",
            " 743761/1000000: episode: 1070, duration: 4.037s, episode steps: 667, steps per second: 165, episode reward: 120.000, mean reward:  0.180 [ 0.000, 30.000], mean action: 2.021 [0.000, 5.000],  loss: 25.897927, mean_q: 271.086270, mean_eps: 0.330916\n",
            " 744717/1000000: episode: 1071, duration: 5.689s, episode steps: 956, steps per second: 168, episode reward: 215.000, mean reward:  0.225 [ 0.000, 30.000], mean action: 2.696 [0.000, 5.000],  loss: 23.696385, mean_q: 259.996609, mean_eps: 0.330185\n",
            " 745384/1000000: episode: 1072, duration: 3.994s, episode steps: 667, steps per second: 167, episode reward: 105.000, mean reward:  0.157 [ 0.000, 30.000], mean action: 2.442 [0.000, 5.000],  loss: 22.415147, mean_q: 249.012670, mean_eps: 0.329455\n",
            " 746017/1000000: episode: 1073, duration: 3.789s, episode steps: 633, steps per second: 167, episode reward: 110.000, mean reward:  0.174 [ 0.000, 30.000], mean action: 3.216 [0.000, 5.000],  loss: 28.443934, mean_q: 261.019861, mean_eps: 0.328870\n",
            " 747143/1000000: episode: 1074, duration: 6.697s, episode steps: 1126, steps per second: 168, episode reward: 150.000, mean reward:  0.133 [ 0.000, 30.000], mean action: 2.612 [0.000, 5.000],  loss: 18.067598, mean_q: 249.937124, mean_eps: 0.328078\n",
            " 747770/1000000: episode: 1075, duration: 3.834s, episode steps: 627, steps per second: 164, episode reward: 80.000, mean reward:  0.128 [ 0.000, 20.000], mean action: 1.545 [0.000, 5.000],  loss: 13.920713, mean_q: 255.249709, mean_eps: 0.327290\n",
            " 748529/1000000: episode: 1076, duration: 4.522s, episode steps: 759, steps per second: 168, episode reward: 135.000, mean reward:  0.178 [ 0.000, 30.000], mean action: 2.792 [0.000, 5.000],  loss: 29.188795, mean_q: 269.728978, mean_eps: 0.326666\n",
            " 749315/1000000: episode: 1077, duration: 4.737s, episode steps: 786, steps per second: 166, episode reward: 135.000, mean reward:  0.172 [ 0.000, 30.000], mean action: 2.088 [0.000, 5.000],  loss: 24.723773, mean_q: 258.393128, mean_eps: 0.325971\n",
            " 750025/1000000: episode: 1078, duration: 4.237s, episode steps: 710, steps per second: 168, episode reward: 120.000, mean reward:  0.169 [ 0.000, 30.000], mean action: 3.120 [0.000, 5.000],  loss: 22.749536, mean_q: 250.559729, mean_eps: 0.325297\n",
            " 750710/1000000: episode: 1079, duration: 4.154s, episode steps: 685, steps per second: 165, episode reward: 110.000, mean reward:  0.161 [ 0.000, 30.000], mean action: 2.442 [0.000, 5.000],  loss: 24.725207, mean_q: 250.839011, mean_eps: 0.324670\n",
            " 751508/1000000: episode: 1080, duration: 4.804s, episode steps: 798, steps per second: 166, episode reward: 195.000, mean reward:  0.244 [ 0.000, 30.000], mean action: 2.342 [0.000, 5.000],  loss: 17.441917, mean_q: 260.496327, mean_eps: 0.324002\n",
            " 752321/1000000: episode: 1081, duration: 4.853s, episode steps: 813, steps per second: 168, episode reward: 210.000, mean reward:  0.258 [ 0.000, 30.000], mean action: 2.469 [0.000, 5.000],  loss: 19.308673, mean_q: 252.109926, mean_eps: 0.323277\n",
            " 753090/1000000: episode: 1082, duration: 4.645s, episode steps: 769, steps per second: 166, episode reward: 110.000, mean reward:  0.143 [ 0.000, 30.000], mean action: 2.914 [0.000, 5.000],  loss: 23.006048, mean_q: 245.191936, mean_eps: 0.322566\n",
            " 753951/1000000: episode: 1083, duration: 5.154s, episode steps: 861, steps per second: 167, episode reward: 240.000, mean reward:  0.279 [ 0.000, 30.000], mean action: 2.533 [0.000, 5.000],  loss: 20.712484, mean_q: 254.580245, mean_eps: 0.321832\n",
            " 754589/1000000: episode: 1084, duration: 3.861s, episode steps: 638, steps per second: 165, episode reward: 105.000, mean reward:  0.165 [ 0.000, 30.000], mean action: 2.390 [0.000, 5.000],  loss: 22.727668, mean_q: 262.725893, mean_eps: 0.321157\n",
            " 755492/1000000: episode: 1085, duration: 5.464s, episode steps: 903, steps per second: 165, episode reward: 210.000, mean reward:  0.233 [ 0.000, 30.000], mean action: 2.364 [0.000, 5.000],  loss: 23.512227, mean_q: 253.154867, mean_eps: 0.320464\n",
            " 756605/1000000: episode: 1086, duration: 6.775s, episode steps: 1113, steps per second: 164, episode reward: 255.000, mean reward:  0.229 [ 0.000, 30.000], mean action: 2.134 [0.000, 5.000],  loss: 18.029823, mean_q: 236.437575, mean_eps: 0.319557\n",
            " 756977/1000000: episode: 1087, duration: 2.243s, episode steps: 372, steps per second: 166, episode reward: 30.000, mean reward:  0.081 [ 0.000, 15.000], mean action: 2.634 [0.000, 5.000],  loss: 18.436981, mean_q: 240.333497, mean_eps: 0.318889\n",
            " 758337/1000000: episode: 1088, duration: 8.112s, episode steps: 1360, steps per second: 168, episode reward: 525.000, mean reward:  0.386 [ 0.000, 30.000], mean action: 2.735 [0.000, 5.000],  loss: 48.724396, mean_q: 256.036083, mean_eps: 0.318109\n",
            " 759236/1000000: episode: 1089, duration: 5.449s, episode steps: 899, steps per second: 165, episode reward: 210.000, mean reward:  0.234 [ 0.000, 30.000], mean action: 2.615 [0.000, 5.000],  loss: 69.171313, mean_q: 239.842545, mean_eps: 0.317093\n",
            " 760339/1000000: episode: 1090, duration: 6.541s, episode steps: 1103, steps per second: 169, episode reward: 515.000, mean reward:  0.467 [ 0.000, 200.000], mean action: 2.209 [0.000, 5.000],  loss: 24.786966, mean_q: 238.211028, mean_eps: 0.316192\n",
            " 761031/1000000: episode: 1091, duration: 4.133s, episode steps: 692, steps per second: 167, episode reward: 140.000, mean reward:  0.202 [ 0.000, 30.000], mean action: 2.753 [0.000, 5.000],  loss: 26.469288, mean_q: 238.810186, mean_eps: 0.315384\n",
            " 761374/1000000: episode: 1092, duration: 2.039s, episode steps: 343, steps per second: 168, episode reward: 70.000, mean reward:  0.204 [ 0.000, 25.000], mean action: 2.630 [0.000, 5.000],  loss: 16.754192, mean_q: 246.393482, mean_eps: 0.314918\n",
            " 762046/1000000: episode: 1093, duration: 4.032s, episode steps: 672, steps per second: 167, episode reward: 125.000, mean reward:  0.186 [ 0.000, 25.000], mean action: 2.430 [0.000, 5.000],  loss: 44.917733, mean_q: 253.546809, mean_eps: 0.314461\n",
            " 763320/1000000: episode: 1094, duration: 7.632s, episode steps: 1274, steps per second: 167, episode reward: 400.000, mean reward:  0.314 [ 0.000, 30.000], mean action: 2.140 [0.000, 5.000],  loss: 28.558433, mean_q: 246.679736, mean_eps: 0.313586\n",
            " 764687/1000000: episode: 1095, duration: 8.117s, episode steps: 1367, steps per second: 168, episode reward: 260.000, mean reward:  0.190 [ 0.000, 30.000], mean action: 2.541 [0.000, 5.000],  loss: 20.917983, mean_q: 234.099681, mean_eps: 0.312397\n",
            " 765500/1000000: episode: 1096, duration: 4.856s, episode steps: 813, steps per second: 167, episode reward: 180.000, mean reward:  0.221 [ 0.000, 30.000], mean action: 2.530 [0.000, 5.000],  loss: 17.844058, mean_q: 241.110022, mean_eps: 0.311416\n",
            " 766298/1000000: episode: 1097, duration: 4.761s, episode steps: 798, steps per second: 168, episode reward: 210.000, mean reward:  0.263 [ 0.000, 30.000], mean action: 2.014 [0.000, 5.000],  loss: 17.198461, mean_q: 247.553797, mean_eps: 0.310691\n",
            " 767027/1000000: episode: 1098, duration: 4.374s, episode steps: 729, steps per second: 167, episode reward: 135.000, mean reward:  0.185 [ 0.000, 30.000], mean action: 2.041 [0.000, 5.000],  loss: 17.635267, mean_q: 242.994809, mean_eps: 0.310004\n",
            " 767606/1000000: episode: 1099, duration: 3.450s, episode steps: 579, steps per second: 168, episode reward: 55.000, mean reward:  0.095 [ 0.000, 20.000], mean action: 2.377 [0.000, 5.000],  loss: 20.710211, mean_q: 245.314496, mean_eps: 0.309416\n",
            " 768522/1000000: episode: 1100, duration: 5.404s, episode steps: 916, steps per second: 170, episode reward: 115.000, mean reward:  0.126 [ 0.000, 20.000], mean action: 2.799 [0.000, 5.000],  loss: 33.561430, mean_q: 260.702719, mean_eps: 0.308743\n",
            " 769289/1000000: episode: 1101, duration: 4.559s, episode steps: 767, steps per second: 168, episode reward: 210.000, mean reward:  0.274 [ 0.000, 30.000], mean action: 1.793 [0.000, 5.000],  loss: 31.285540, mean_q: 262.165712, mean_eps: 0.307985\n",
            " 770274/1000000: episode: 1102, duration: 5.806s, episode steps: 985, steps per second: 170, episode reward: 185.000, mean reward:  0.188 [ 0.000, 30.000], mean action: 2.472 [0.000, 5.000],  loss: 16.784031, mean_q: 252.231541, mean_eps: 0.307197\n",
            " 770927/1000000: episode: 1103, duration: 3.878s, episode steps: 653, steps per second: 168, episode reward: 95.000, mean reward:  0.145 [ 0.000, 20.000], mean action: 2.562 [0.000, 5.000],  loss: 23.213231, mean_q: 269.963679, mean_eps: 0.306460\n",
            " 772160/1000000: episode: 1104, duration: 7.282s, episode steps: 1233, steps per second: 169, episode reward: 240.000, mean reward:  0.195 [ 0.000, 30.000], mean action: 2.666 [0.000, 5.000],  loss: 28.827051, mean_q: 268.855796, mean_eps: 0.305611\n",
            " 773148/1000000: episode: 1105, duration: 5.816s, episode steps: 988, steps per second: 170, episode reward: 215.000, mean reward:  0.218 [ 0.000, 30.000], mean action: 2.537 [0.000, 5.000],  loss: 20.567500, mean_q: 260.320902, mean_eps: 0.304612\n",
            " 773797/1000000: episode: 1106, duration: 3.871s, episode steps: 649, steps per second: 168, episode reward: 140.000, mean reward:  0.216 [ 0.000, 25.000], mean action: 2.643 [0.000, 5.000],  loss: 23.630054, mean_q: 269.497749, mean_eps: 0.303875\n",
            " 774603/1000000: episode: 1107, duration: 4.809s, episode steps: 806, steps per second: 168, episode reward: 260.000, mean reward:  0.323 [ 0.000, 30.000], mean action: 2.189 [0.000, 5.000],  loss: 29.872648, mean_q: 251.822736, mean_eps: 0.303220\n",
            " 774998/1000000: episode: 1108, duration: 2.396s, episode steps: 395, steps per second: 165, episode reward: 20.000, mean reward:  0.051 [ 0.000, 10.000], mean action: 2.547 [0.000, 5.000],  loss: 22.887626, mean_q: 228.091970, mean_eps: 0.302680\n",
            " 775639/1000000: episode: 1109, duration: 3.801s, episode steps: 641, steps per second: 169, episode reward: 130.000, mean reward:  0.203 [ 0.000, 25.000], mean action: 2.459 [0.000, 5.000],  loss: 41.356883, mean_q: 241.589051, mean_eps: 0.302214\n",
            " 776361/1000000: episode: 1110, duration: 4.347s, episode steps: 722, steps per second: 166, episode reward: 210.000, mean reward:  0.291 [ 0.000, 30.000], mean action: 2.168 [0.000, 5.000],  loss: 31.706372, mean_q: 258.834479, mean_eps: 0.301600\n",
            " 776929/1000000: episode: 1111, duration: 3.402s, episode steps: 568, steps per second: 167, episode reward: 105.000, mean reward:  0.185 [ 0.000, 30.000], mean action: 2.109 [0.000, 5.000],  loss: 26.261536, mean_q: 237.507652, mean_eps: 0.301020\n",
            " 777718/1000000: episode: 1112, duration: 4.686s, episode steps: 789, steps per second: 168, episode reward: 290.000, mean reward:  0.368 [ 0.000, 30.000], mean action: 2.470 [0.000, 5.000],  loss: 23.651160, mean_q: 235.239845, mean_eps: 0.300409\n",
            " 778371/1000000: episode: 1113, duration: 3.893s, episode steps: 653, steps per second: 168, episode reward: 95.000, mean reward:  0.145 [ 0.000, 30.000], mean action: 1.819 [0.000, 5.000],  loss: 18.567997, mean_q: 254.171514, mean_eps: 0.299760\n",
            " 778740/1000000: episode: 1114, duration: 2.231s, episode steps: 369, steps per second: 165, episode reward: 90.000, mean reward:  0.244 [ 0.000, 25.000], mean action: 2.724 [0.000, 5.000],  loss: 33.296625, mean_q: 266.569613, mean_eps: 0.299300\n",
            " 779706/1000000: episode: 1115, duration: 5.781s, episode steps: 966, steps per second: 167, episode reward: 210.000, mean reward:  0.217 [ 0.000, 30.000], mean action: 1.902 [0.000, 5.000],  loss: 51.979575, mean_q: 261.258017, mean_eps: 0.298700\n",
            " 780387/1000000: episode: 1116, duration: 4.034s, episode steps: 681, steps per second: 169, episode reward: 140.000, mean reward:  0.206 [ 0.000, 30.000], mean action: 2.330 [0.000, 5.000],  loss: 23.274366, mean_q: 238.571416, mean_eps: 0.297959\n",
            " 781045/1000000: episode: 1117, duration: 3.886s, episode steps: 658, steps per second: 169, episode reward: 55.000, mean reward:  0.084 [ 0.000, 20.000], mean action: 2.495 [0.000, 5.000],  loss: 17.507532, mean_q: 252.028775, mean_eps: 0.297356\n",
            " 781685/1000000: episode: 1118, duration: 3.803s, episode steps: 640, steps per second: 168, episode reward: 145.000, mean reward:  0.227 [ 0.000, 30.000], mean action: 2.619 [0.000, 5.000],  loss: 49.139123, mean_q: 263.045603, mean_eps: 0.296772\n",
            " 782751/1000000: episode: 1119, duration: 6.343s, episode steps: 1066, steps per second: 168, episode reward: 285.000, mean reward:  0.267 [ 0.000, 30.000], mean action: 2.725 [0.000, 5.000],  loss: 44.209769, mean_q: 253.270267, mean_eps: 0.296004\n",
            " 783162/1000000: episode: 1120, duration: 2.479s, episode steps: 411, steps per second: 166, episode reward: 15.000, mean reward:  0.036 [ 0.000, 10.000], mean action: 2.331 [0.000, 5.000],  loss: 31.181448, mean_q: 239.560314, mean_eps: 0.295340\n",
            " 783819/1000000: episode: 1121, duration: 3.883s, episode steps: 657, steps per second: 169, episode reward: 70.000, mean reward:  0.107 [ 0.000, 25.000], mean action: 2.164 [0.000, 5.000],  loss: 32.401584, mean_q: 254.499713, mean_eps: 0.294859\n",
            " 784694/1000000: episode: 1122, duration: 5.164s, episode steps: 875, steps per second: 169, episode reward: 255.000, mean reward:  0.291 [ 0.000, 25.000], mean action: 2.487 [0.000, 5.000],  loss: 31.543202, mean_q: 260.118475, mean_eps: 0.294170\n",
            " 785477/1000000: episode: 1123, duration: 4.695s, episode steps: 783, steps per second: 167, episode reward: 120.000, mean reward:  0.153 [ 0.000, 30.000], mean action: 2.885 [0.000, 5.000],  loss: 14.248233, mean_q: 242.972789, mean_eps: 0.293423\n",
            " 786262/1000000: episode: 1124, duration: 4.742s, episode steps: 785, steps per second: 166, episode reward: 155.000, mean reward:  0.197 [ 0.000, 30.000], mean action: 1.678 [0.000, 5.000],  loss: 18.860504, mean_q: 244.842140, mean_eps: 0.292718\n",
            " 787231/1000000: episode: 1125, duration: 5.761s, episode steps: 969, steps per second: 168, episode reward: 150.000, mean reward:  0.155 [ 0.000, 20.000], mean action: 2.096 [0.000, 5.000],  loss: 29.217069, mean_q: 267.905612, mean_eps: 0.291929\n",
            " 788406/1000000: episode: 1126, duration: 6.937s, episode steps: 1175, steps per second: 169, episode reward: 260.000, mean reward:  0.221 [ 0.000, 30.000], mean action: 2.618 [0.000, 5.000],  loss: 13.770838, mean_q: 252.355504, mean_eps: 0.290964\n",
            " 788786/1000000: episode: 1127, duration: 2.260s, episode steps: 380, steps per second: 168, episode reward: 45.000, mean reward:  0.118 [ 0.000, 15.000], mean action: 2.642 [0.000, 5.000],  loss: 12.902958, mean_q: 231.454008, mean_eps: 0.290264\n",
            " 789477/1000000: episode: 1128, duration: 4.103s, episode steps: 691, steps per second: 168, episode reward: 165.000, mean reward:  0.239 [ 0.000, 30.000], mean action: 2.221 [0.000, 5.000],  loss: 34.656494, mean_q: 251.999145, mean_eps: 0.289782\n",
            " 790106/1000000: episode: 1129, duration: 3.817s, episode steps: 629, steps per second: 165, episode reward: 185.000, mean reward:  0.294 [ 0.000, 30.000], mean action: 2.091 [0.000, 5.000],  loss: 34.704479, mean_q: 261.765308, mean_eps: 0.289188\n",
            " 790939/1000000: episode: 1130, duration: 4.942s, episode steps: 833, steps per second: 169, episode reward: 135.000, mean reward:  0.162 [ 0.000, 30.000], mean action: 2.427 [0.000, 5.000],  loss: 24.379749, mean_q: 258.525509, mean_eps: 0.288530\n",
            " 791755/1000000: episode: 1131, duration: 4.811s, episode steps: 816, steps per second: 170, episode reward: 210.000, mean reward:  0.257 [ 0.000, 30.000], mean action: 2.828 [0.000, 5.000],  loss: 27.866890, mean_q: 247.235823, mean_eps: 0.287788\n",
            " 792194/1000000: episode: 1132, duration: 2.610s, episode steps: 439, steps per second: 168, episode reward: 80.000, mean reward:  0.182 [ 0.000, 25.000], mean action: 3.314 [0.000, 5.000],  loss: 27.060306, mean_q: 246.171205, mean_eps: 0.287223\n",
            " 792816/1000000: episode: 1133, duration: 3.741s, episode steps: 622, steps per second: 166, episode reward: 90.000, mean reward:  0.145 [ 0.000, 25.000], mean action: 3.003 [0.000, 5.000],  loss: 35.406234, mean_q: 248.145380, mean_eps: 0.286746\n",
            " 793522/1000000: episode: 1134, duration: 4.214s, episode steps: 706, steps per second: 168, episode reward: 115.000, mean reward:  0.163 [ 0.000, 20.000], mean action: 2.541 [0.000, 5.000],  loss: 30.356145, mean_q: 259.911732, mean_eps: 0.286148\n",
            " 794183/1000000: episode: 1135, duration: 3.955s, episode steps: 661, steps per second: 167, episode reward: 120.000, mean reward:  0.182 [ 0.000, 30.000], mean action: 2.735 [0.000, 5.000],  loss: 40.878076, mean_q: 261.126609, mean_eps: 0.285533\n",
            " 795239/1000000: episode: 1136, duration: 6.246s, episode steps: 1056, steps per second: 169, episode reward: 180.000, mean reward:  0.170 [ 0.000, 30.000], mean action: 2.853 [0.000, 5.000],  loss: 24.738369, mean_q: 263.696106, mean_eps: 0.284761\n",
            " 796043/1000000: episode: 1137, duration: 4.794s, episode steps: 804, steps per second: 168, episode reward: 155.000, mean reward:  0.193 [ 0.000, 30.000], mean action: 2.274 [0.000, 5.000],  loss: 26.993625, mean_q: 271.690201, mean_eps: 0.283924\n",
            " 796835/1000000: episode: 1138, duration: 4.693s, episode steps: 792, steps per second: 169, episode reward: 310.000, mean reward:  0.391 [ 0.000, 200.000], mean action: 2.343 [0.000, 5.000],  loss: 28.246437, mean_q: 254.774075, mean_eps: 0.283205\n",
            " 797211/1000000: episode: 1139, duration: 2.263s, episode steps: 376, steps per second: 166, episode reward: 50.000, mean reward:  0.133 [ 0.000, 20.000], mean action: 2.505 [0.000, 5.000],  loss: 36.642135, mean_q: 257.780297, mean_eps: 0.282680\n",
            " 797621/1000000: episode: 1140, duration: 2.421s, episode steps: 410, steps per second: 169, episode reward: 75.000, mean reward:  0.183 [ 0.000, 25.000], mean action: 1.963 [0.000, 5.000],  loss: 66.469243, mean_q: 259.898782, mean_eps: 0.282326\n",
            " 798019/1000000: episode: 1141, duration: 2.386s, episode steps: 398, steps per second: 167, episode reward: 55.000, mean reward:  0.138 [ 0.000, 25.000], mean action: 2.148 [0.000, 5.000],  loss: 55.512333, mean_q: 258.244218, mean_eps: 0.281962\n",
            " 798806/1000000: episode: 1142, duration: 4.670s, episode steps: 787, steps per second: 169, episode reward: 155.000, mean reward:  0.197 [ 0.000, 30.000], mean action: 2.593 [0.000, 5.000],  loss: 48.246294, mean_q: 255.924657, mean_eps: 0.281429\n",
            " 799466/1000000: episode: 1143, duration: 3.933s, episode steps: 660, steps per second: 168, episode reward: 180.000, mean reward:  0.273 [ 0.000, 30.000], mean action: 2.253 [0.000, 5.000],  loss: 23.568590, mean_q: 243.213765, mean_eps: 0.280778\n",
            " 800059/1000000: episode: 1144, duration: 3.550s, episode steps: 593, steps per second: 167, episode reward: 110.000, mean reward:  0.185 [ 0.000, 30.000], mean action: 2.160 [0.000, 5.000],  loss: 16.364678, mean_q: 245.448613, mean_eps: 0.280214\n",
            " 800700/1000000: episode: 1145, duration: 3.840s, episode steps: 641, steps per second: 167, episode reward: 180.000, mean reward:  0.281 [ 0.000, 30.000], mean action: 2.565 [0.000, 5.000],  loss: 12.675208, mean_q: 245.835753, mean_eps: 0.279659\n",
            " 801051/1000000: episode: 1146, duration: 2.123s, episode steps: 351, steps per second: 165, episode reward: 30.000, mean reward:  0.085 [ 0.000, 15.000], mean action: 1.823 [0.000, 5.000],  loss: 24.843064, mean_q: 248.861356, mean_eps: 0.279212\n",
            " 801653/1000000: episode: 1147, duration: 3.596s, episode steps: 602, steps per second: 167, episode reward: 20.000, mean reward:  0.033 [ 0.000, 10.000], mean action: 2.008 [0.000, 5.000],  loss: 43.389801, mean_q: 257.916857, mean_eps: 0.278784\n",
            " 802441/1000000: episode: 1148, duration: 4.710s, episode steps: 788, steps per second: 167, episode reward: 155.000, mean reward:  0.197 [ 0.000, 30.000], mean action: 2.751 [0.000, 5.000],  loss: 43.484322, mean_q: 255.296757, mean_eps: 0.278158\n",
            " 803188/1000000: episode: 1149, duration: 4.445s, episode steps: 747, steps per second: 168, episode reward: 155.000, mean reward:  0.207 [ 0.000, 30.000], mean action: 2.522 [0.000, 5.000],  loss: 18.384086, mean_q: 239.837398, mean_eps: 0.277467\n",
            " 803884/1000000: episode: 1150, duration: 4.206s, episode steps: 696, steps per second: 165, episode reward: 150.000, mean reward:  0.216 [ 0.000, 30.000], mean action: 2.142 [0.000, 5.000],  loss: 21.537301, mean_q: 248.707148, mean_eps: 0.276818\n",
            " 804255/1000000: episode: 1151, duration: 2.209s, episode steps: 371, steps per second: 168, episode reward: 100.000, mean reward:  0.270 [ 0.000, 20.000], mean action: 1.911 [0.000, 5.000],  loss: 26.492724, mean_q: 253.539871, mean_eps: 0.276338\n",
            " 805095/1000000: episode: 1152, duration: 5.010s, episode steps: 840, steps per second: 168, episode reward: 185.000, mean reward:  0.220 [ 0.000, 30.000], mean action: 2.405 [0.000, 5.000],  loss: 36.626393, mean_q: 254.237976, mean_eps: 0.275793\n",
            " 805583/1000000: episode: 1153, duration: 2.922s, episode steps: 488, steps per second: 167, episode reward: 90.000, mean reward:  0.184 [ 0.000, 25.000], mean action: 1.928 [0.000, 5.000],  loss: 32.884054, mean_q: 244.836358, mean_eps: 0.275195\n",
            " 806100/1000000: episode: 1154, duration: 3.100s, episode steps: 517, steps per second: 167, episode reward: 50.000, mean reward:  0.097 [ 0.000, 20.000], mean action: 2.383 [0.000, 5.000],  loss: 27.454632, mean_q: 247.122247, mean_eps: 0.274743\n",
            " 806648/1000000: episode: 1155, duration: 3.267s, episode steps: 548, steps per second: 168, episode reward: 110.000, mean reward:  0.201 [ 0.000, 30.000], mean action: 2.411 [0.000, 5.000],  loss: 39.626301, mean_q: 261.367129, mean_eps: 0.274264\n",
            " 808201/1000000: episode: 1156, duration: 9.272s, episode steps: 1553, steps per second: 167, episode reward: 495.000, mean reward:  0.319 [ 0.000, 30.000], mean action: 2.012 [0.000, 5.000],  loss: 36.285912, mean_q: 235.035524, mean_eps: 0.273318\n",
            " 808892/1000000: episode: 1157, duration: 4.124s, episode steps: 691, steps per second: 168, episode reward: 160.000, mean reward:  0.232 [ 0.000, 25.000], mean action: 1.809 [0.000, 5.000],  loss: 31.686027, mean_q: 200.596497, mean_eps: 0.272309\n",
            " 809592/1000000: episode: 1158, duration: 4.152s, episode steps: 700, steps per second: 169, episode reward: 140.000, mean reward:  0.200 [ 0.000, 30.000], mean action: 2.776 [0.000, 5.000],  loss: 23.982232, mean_q: 249.192045, mean_eps: 0.271683\n",
            " 810323/1000000: episode: 1159, duration: 4.350s, episode steps: 731, steps per second: 168, episode reward: 135.000, mean reward:  0.185 [ 0.000, 30.000], mean action: 2.330 [0.000, 5.000],  loss: 18.426627, mean_q: 248.271470, mean_eps: 0.271039\n",
            " 811577/1000000: episode: 1160, duration: 7.370s, episode steps: 1254, steps per second: 170, episode reward: 460.000, mean reward:  0.367 [ 0.000, 30.000], mean action: 3.146 [0.000, 5.000],  loss: 17.834938, mean_q: 246.539484, mean_eps: 0.270145\n",
            " 812149/1000000: episode: 1161, duration: 3.442s, episode steps: 572, steps per second: 166, episode reward: 120.000, mean reward:  0.210 [ 0.000, 30.000], mean action: 2.229 [0.000, 5.000],  loss: 12.048946, mean_q: 214.609793, mean_eps: 0.269324\n",
            " 812813/1000000: episode: 1162, duration: 3.997s, episode steps: 664, steps per second: 166, episode reward: 110.000, mean reward:  0.166 [ 0.000, 30.000], mean action: 2.369 [0.000, 5.000],  loss: 29.110883, mean_q: 240.956918, mean_eps: 0.268768\n",
            " 813632/1000000: episode: 1163, duration: 4.910s, episode steps: 819, steps per second: 167, episode reward: 135.000, mean reward:  0.165 [ 0.000, 30.000], mean action: 2.664 [0.000, 5.000],  loss: 17.013180, mean_q: 245.934423, mean_eps: 0.268100\n",
            " 814292/1000000: episode: 1164, duration: 3.933s, episode steps: 660, steps per second: 168, episode reward: 120.000, mean reward:  0.182 [ 0.000, 30.000], mean action: 2.556 [0.000, 5.000],  loss: 11.349223, mean_q: 231.808338, mean_eps: 0.267435\n",
            " 814669/1000000: episode: 1165, duration: 2.292s, episode steps: 377, steps per second: 164, episode reward: 15.000, mean reward:  0.040 [ 0.000, 10.000], mean action: 2.398 [0.000, 5.000],  loss: 19.454794, mean_q: 231.864684, mean_eps: 0.266968\n",
            " 815827/1000000: episode: 1166, duration: 6.909s, episode steps: 1158, steps per second: 168, episode reward: 235.000, mean reward:  0.203 [ 0.000, 25.000], mean action: 2.341 [0.000, 5.000],  loss: 31.491746, mean_q: 257.689051, mean_eps: 0.266277\n",
            " 816633/1000000: episode: 1167, duration: 4.818s, episode steps: 806, steps per second: 167, episode reward: 210.000, mean reward:  0.261 [ 0.000, 30.000], mean action: 2.073 [0.000, 5.000],  loss: 31.316604, mean_q: 260.329335, mean_eps: 0.265393\n",
            " 817438/1000000: episode: 1168, duration: 4.817s, episode steps: 805, steps per second: 167, episode reward: 410.000, mean reward:  0.509 [ 0.000, 200.000], mean action: 3.396 [0.000, 5.000],  loss: 25.005738, mean_q: 237.475084, mean_eps: 0.264668\n",
            " 818299/1000000: episode: 1169, duration: 5.126s, episode steps: 861, steps per second: 168, episode reward: 185.000, mean reward:  0.215 [ 0.000, 30.000], mean action: 3.252 [0.000, 5.000],  loss: 32.540423, mean_q: 234.980544, mean_eps: 0.263919\n",
            " 818951/1000000: episode: 1170, duration: 3.906s, episode steps: 652, steps per second: 167, episode reward: 205.000, mean reward:  0.314 [ 0.000, 30.000], mean action: 3.112 [0.000, 5.000],  loss: 19.847091, mean_q: 228.099160, mean_eps: 0.263238\n",
            " 820081/1000000: episode: 1171, duration: 6.739s, episode steps: 1130, steps per second: 168, episode reward: 510.000, mean reward:  0.451 [ 0.000, 200.000], mean action: 2.602 [0.000, 5.000],  loss: 29.175696, mean_q: 250.442177, mean_eps: 0.262436\n",
            " 820730/1000000: episode: 1172, duration: 3.860s, episode steps: 649, steps per second: 168, episode reward: 95.000, mean reward:  0.146 [ 0.000, 20.000], mean action: 2.622 [0.000, 5.000],  loss: 19.013481, mean_q: 245.194499, mean_eps: 0.261635\n",
            " 821912/1000000: episode: 1173, duration: 6.967s, episode steps: 1182, steps per second: 170, episode reward: 295.000, mean reward:  0.250 [ 0.000, 30.000], mean action: 2.559 [0.000, 5.000],  loss: 19.681787, mean_q: 245.912381, mean_eps: 0.260812\n",
            " 822307/1000000: episode: 1174, duration: 2.371s, episode steps: 395, steps per second: 167, episode reward: 80.000, mean reward:  0.203 [ 0.000, 20.000], mean action: 2.278 [0.000, 5.000],  loss: 31.249663, mean_q: 219.381194, mean_eps: 0.260102\n",
            " 822904/1000000: episode: 1175, duration: 3.601s, episode steps: 597, steps per second: 166, episode reward: 165.000, mean reward:  0.276 [ 0.000, 25.000], mean action: 2.228 [0.000, 5.000],  loss: 35.303649, mean_q: 235.530138, mean_eps: 0.259655\n",
            " 823751/1000000: episode: 1176, duration: 5.049s, episode steps: 847, steps per second: 168, episode reward: 210.000, mean reward:  0.248 [ 0.000, 30.000], mean action: 2.671 [0.000, 5.000],  loss: 35.832871, mean_q: 256.611103, mean_eps: 0.259006\n",
            " 824543/1000000: episode: 1177, duration: 4.652s, episode steps: 792, steps per second: 170, episode reward: 185.000, mean reward:  0.234 [ 0.000, 30.000], mean action: 2.014 [0.000, 5.000],  loss: 16.835669, mean_q: 231.338396, mean_eps: 0.258268\n",
            " 825154/1000000: episode: 1178, duration: 3.671s, episode steps: 611, steps per second: 166, episode reward: 75.000, mean reward:  0.123 [ 0.000, 20.000], mean action: 2.195 [0.000, 5.000],  loss: 14.021979, mean_q: 235.944779, mean_eps: 0.257637\n",
            " 825967/1000000: episode: 1179, duration: 4.820s, episode steps: 813, steps per second: 169, episode reward: 185.000, mean reward:  0.228 [ 0.000, 30.000], mean action: 1.936 [0.000, 5.000],  loss: 24.600368, mean_q: 245.673548, mean_eps: 0.256996\n",
            " 826849/1000000: episode: 1180, duration: 5.239s, episode steps: 882, steps per second: 168, episode reward: 195.000, mean reward:  0.221 [ 0.000, 25.000], mean action: 2.314 [0.000, 5.000],  loss: 15.987463, mean_q: 231.582299, mean_eps: 0.256233\n",
            " 827548/1000000: episode: 1181, duration: 4.173s, episode steps: 699, steps per second: 168, episode reward: 115.000, mean reward:  0.165 [ 0.000, 20.000], mean action: 2.498 [0.000, 5.000],  loss: 16.638809, mean_q: 250.076287, mean_eps: 0.255522\n",
            " 827926/1000000: episode: 1182, duration: 2.268s, episode steps: 378, steps per second: 167, episode reward: 55.000, mean reward:  0.146 [ 0.000, 20.000], mean action: 2.278 [0.000, 5.000],  loss: 28.920570, mean_q: 257.206862, mean_eps: 0.255037\n",
            " 828599/1000000: episode: 1183, duration: 4.016s, episode steps: 673, steps per second: 168, episode reward: 80.000, mean reward:  0.119 [ 0.000, 20.000], mean action: 1.649 [0.000, 5.000],  loss: 45.642171, mean_q: 257.882627, mean_eps: 0.254564\n",
            " 829371/1000000: episode: 1184, duration: 4.601s, episode steps: 772, steps per second: 168, episode reward: 155.000, mean reward:  0.201 [ 0.000, 30.000], mean action: 2.294 [0.000, 5.000],  loss: 25.968548, mean_q: 249.823682, mean_eps: 0.253914\n",
            " 830088/1000000: episode: 1185, duration: 4.274s, episode steps: 717, steps per second: 168, episode reward: 80.000, mean reward:  0.112 [ 0.000, 20.000], mean action: 2.131 [0.000, 5.000],  loss: 16.015354, mean_q: 238.012861, mean_eps: 0.253244\n",
            " 831064/1000000: episode: 1186, duration: 5.782s, episode steps: 976, steps per second: 169, episode reward: 225.000, mean reward:  0.231 [ 0.000, 30.000], mean action: 3.259 [0.000, 5.000],  loss: 22.915185, mean_q: 251.637419, mean_eps: 0.252482\n",
            " 831686/1000000: episode: 1187, duration: 3.712s, episode steps: 622, steps per second: 168, episode reward: 75.000, mean reward:  0.121 [ 0.000, 25.000], mean action: 2.420 [0.000, 5.000],  loss: 18.949714, mean_q: 238.951474, mean_eps: 0.251763\n",
            " 832537/1000000: episode: 1188, duration: 5.117s, episode steps: 851, steps per second: 166, episode reward: 160.000, mean reward:  0.188 [ 0.000, 25.000], mean action: 1.931 [0.000, 5.000],  loss: 20.678620, mean_q: 249.678286, mean_eps: 0.251100\n",
            " 833149/1000000: episode: 1189, duration: 3.670s, episode steps: 612, steps per second: 167, episode reward: 140.000, mean reward:  0.229 [ 0.000, 30.000], mean action: 2.523 [0.000, 5.000],  loss: 20.307431, mean_q: 248.839572, mean_eps: 0.250442\n",
            " 834279/1000000: episode: 1190, duration: 6.742s, episode steps: 1130, steps per second: 168, episode reward: 445.000, mean reward:  0.394 [ 0.000, 200.000], mean action: 2.924 [0.000, 5.000],  loss: 32.701125, mean_q: 241.718261, mean_eps: 0.249658\n",
            " 834899/1000000: episode: 1191, duration: 3.701s, episode steps: 620, steps per second: 168, episode reward: 110.000, mean reward:  0.177 [ 0.000, 30.000], mean action: 1.921 [0.000, 5.000],  loss: 35.456803, mean_q: 216.137621, mean_eps: 0.248870\n",
            " 835412/1000000: episode: 1192, duration: 3.082s, episode steps: 513, steps per second: 166, episode reward: 50.000, mean reward:  0.097 [ 0.000, 20.000], mean action: 2.224 [0.000, 5.000],  loss: 25.257854, mean_q: 240.573096, mean_eps: 0.248360\n",
            " 835978/1000000: episode: 1193, duration: 3.382s, episode steps: 566, steps per second: 167, episode reward: 45.000, mean reward:  0.080 [ 0.000, 15.000], mean action: 2.214 [0.000, 5.000],  loss: 32.832649, mean_q: 249.468334, mean_eps: 0.247875\n",
            " 836662/1000000: episode: 1194, duration: 4.155s, episode steps: 684, steps per second: 165, episode reward: 115.000, mean reward:  0.168 [ 0.000, 30.000], mean action: 2.477 [0.000, 5.000],  loss: 39.497986, mean_q: 253.430570, mean_eps: 0.247312\n",
            " 837532/1000000: episode: 1195, duration: 5.226s, episode steps: 870, steps per second: 166, episode reward: 155.000, mean reward:  0.178 [ 0.000, 30.000], mean action: 2.789 [0.000, 5.000],  loss: 26.768909, mean_q: 243.782604, mean_eps: 0.246613\n",
            " 839421/1000000: episode: 1196, duration: 11.265s, episode steps: 1889, steps per second: 168, episode reward: 655.000, mean reward:  0.347 [ 0.000, 200.000], mean action: 2.491 [0.000, 5.000],  loss: 30.581084, mean_q: 222.737215, mean_eps: 0.245372\n",
            " 840271/1000000: episode: 1197, duration: 5.099s, episode steps: 850, steps per second: 167, episode reward: 175.000, mean reward:  0.206 [ 0.000, 30.000], mean action: 3.042 [0.000, 5.000],  loss: 37.760502, mean_q: 210.077504, mean_eps: 0.244139\n",
            " 841694/1000000: episode: 1198, duration: 8.423s, episode steps: 1423, steps per second: 169, episode reward: 265.000, mean reward:  0.186 [ 0.000, 30.000], mean action: 2.492 [0.000, 5.000],  loss: 19.710584, mean_q: 231.054484, mean_eps: 0.243116\n",
            " 842495/1000000: episode: 1199, duration: 4.796s, episode steps: 801, steps per second: 167, episode reward: 170.000, mean reward:  0.212 [ 0.000, 30.000], mean action: 2.046 [0.000, 5.000],  loss: 20.176313, mean_q: 203.858292, mean_eps: 0.242115\n",
            " 842890/1000000: episode: 1200, duration: 2.376s, episode steps: 395, steps per second: 166, episode reward: 60.000, mean reward:  0.152 [ 0.000, 15.000], mean action: 3.124 [0.000, 5.000],  loss: 17.065875, mean_q: 235.377054, mean_eps: 0.241577\n",
            " 843506/1000000: episode: 1201, duration: 3.636s, episode steps: 616, steps per second: 169, episode reward: 135.000, mean reward:  0.219 [ 0.000, 30.000], mean action: 2.857 [0.000, 5.000],  loss: 33.506401, mean_q: 239.527382, mean_eps: 0.241122\n",
            " 844280/1000000: episode: 1202, duration: 4.592s, episode steps: 774, steps per second: 169, episode reward: 115.000, mean reward:  0.149 [ 0.000, 25.000], mean action: 2.579 [0.000, 5.000],  loss: 29.364006, mean_q: 246.074874, mean_eps: 0.240497\n",
            " 845065/1000000: episode: 1203, duration: 4.658s, episode steps: 785, steps per second: 169, episode reward: 115.000, mean reward:  0.146 [ 0.000, 20.000], mean action: 2.787 [0.000, 5.000],  loss: 25.930923, mean_q: 254.314267, mean_eps: 0.239795\n",
            " 845709/1000000: episode: 1204, duration: 3.890s, episode steps: 644, steps per second: 166, episode reward: 150.000, mean reward:  0.233 [ 0.000, 30.000], mean action: 2.436 [0.000, 5.000],  loss: 38.827187, mean_q: 254.515978, mean_eps: 0.239152\n",
            " 846646/1000000: episode: 1205, duration: 5.521s, episode steps: 937, steps per second: 170, episode reward: 210.000, mean reward:  0.224 [ 0.000, 30.000], mean action: 2.195 [0.000, 5.000],  loss: 17.752186, mean_q: 248.719255, mean_eps: 0.238441\n",
            " 847205/1000000: episode: 1206, duration: 3.298s, episode steps: 559, steps per second: 169, episode reward: 30.000, mean reward:  0.054 [ 0.000, 15.000], mean action: 2.801 [0.000, 5.000],  loss: 12.822488, mean_q: 238.856610, mean_eps: 0.237767\n",
            " 848285/1000000: episode: 1207, duration: 6.365s, episode steps: 1080, steps per second: 170, episode reward: 190.000, mean reward:  0.176 [ 0.000, 30.000], mean action: 2.424 [0.000, 5.000],  loss: 24.635142, mean_q: 250.259215, mean_eps: 0.237030\n",
            " 849011/1000000: episode: 1208, duration: 4.346s, episode steps: 726, steps per second: 167, episode reward: 125.000, mean reward:  0.172 [ 0.000, 30.000], mean action: 2.977 [0.000, 5.000],  loss: 15.852526, mean_q: 236.217545, mean_eps: 0.236217\n",
            " 849864/1000000: episode: 1209, duration: 5.072s, episode steps: 853, steps per second: 168, episode reward: 330.000, mean reward:  0.387 [ 0.000, 30.000], mean action: 2.749 [0.000, 5.000],  loss: 14.823611, mean_q: 236.296176, mean_eps: 0.235507\n",
            " 850248/1000000: episode: 1210, duration: 2.343s, episode steps: 384, steps per second: 164, episode reward: 75.000, mean reward:  0.195 [ 0.000, 25.000], mean action: 2.099 [0.000, 5.000],  loss: 15.479606, mean_q: 241.265370, mean_eps: 0.234950\n",
            " 851029/1000000: episode: 1211, duration: 4.624s, episode steps: 781, steps per second: 169, episode reward: 130.000, mean reward:  0.166 [ 0.000, 30.000], mean action: 2.501 [0.000, 5.000],  loss: 24.884274, mean_q: 243.748140, mean_eps: 0.234426\n",
            " 851861/1000000: episode: 1212, duration: 4.946s, episode steps: 832, steps per second: 168, episode reward: 180.000, mean reward:  0.216 [ 0.000, 30.000], mean action: 2.219 [0.000, 5.000],  loss: 30.792338, mean_q: 239.436402, mean_eps: 0.233700\n",
            " 852489/1000000: episode: 1213, duration: 3.741s, episode steps: 628, steps per second: 168, episode reward: 100.000, mean reward:  0.159 [ 0.000, 25.000], mean action: 1.933 [0.000, 5.000],  loss: 15.861099, mean_q: 233.262884, mean_eps: 0.233043\n",
            " 853156/1000000: episode: 1214, duration: 3.967s, episode steps: 667, steps per second: 168, episode reward: 175.000, mean reward:  0.262 [ 0.000, 25.000], mean action: 1.924 [0.000, 5.000],  loss: 27.399242, mean_q: 245.289656, mean_eps: 0.232460\n",
            " 853990/1000000: episode: 1215, duration: 5.025s, episode steps: 834, steps per second: 166, episode reward: 385.000, mean reward:  0.462 [ 0.000, 200.000], mean action: 2.856 [0.000, 5.000],  loss: 29.560687, mean_q: 250.444999, mean_eps: 0.231785\n",
            " 854671/1000000: episode: 1216, duration: 4.034s, episode steps: 681, steps per second: 169, episode reward: 120.000, mean reward:  0.176 [ 0.000, 30.000], mean action: 2.438 [0.000, 5.000],  loss: 25.044659, mean_q: 242.210726, mean_eps: 0.231103\n",
            " 855437/1000000: episode: 1217, duration: 4.521s, episode steps: 766, steps per second: 169, episode reward: 325.000, mean reward:  0.424 [ 0.000, 200.000], mean action: 2.296 [0.000, 5.000],  loss: 15.766958, mean_q: 241.476127, mean_eps: 0.230452\n",
            " 856268/1000000: episode: 1218, duration: 4.952s, episode steps: 831, steps per second: 168, episode reward: 180.000, mean reward:  0.217 [ 0.000, 30.000], mean action: 2.353 [0.000, 5.000],  loss: 26.141390, mean_q: 227.400806, mean_eps: 0.229733\n",
            " 857411/1000000: episode: 1219, duration: 6.798s, episode steps: 1143, steps per second: 168, episode reward: 440.000, mean reward:  0.385 [ 0.000, 200.000], mean action: 2.428 [0.000, 5.000],  loss: 23.451592, mean_q: 222.847923, mean_eps: 0.228845\n",
            " 858068/1000000: episode: 1220, duration: 3.970s, episode steps: 657, steps per second: 165, episode reward: 60.000, mean reward:  0.091 [ 0.000, 30.000], mean action: 2.437 [0.000, 5.000],  loss: 29.171498, mean_q: 219.647583, mean_eps: 0.228035\n",
            " 859519/1000000: episode: 1221, duration: 8.695s, episode steps: 1451, steps per second: 167, episode reward: 415.000, mean reward:  0.286 [ 0.000, 200.000], mean action: 1.916 [0.000, 5.000],  loss: 25.252995, mean_q: 241.219599, mean_eps: 0.227086\n",
            " 860291/1000000: episode: 1222, duration: 4.702s, episode steps: 772, steps per second: 164, episode reward: 155.000, mean reward:  0.201 [ 0.000, 30.000], mean action: 2.146 [0.000, 5.000],  loss: 36.198904, mean_q: 215.189112, mean_eps: 0.226086\n",
            " 861166/1000000: episode: 1223, duration: 5.213s, episode steps: 875, steps per second: 168, episode reward: 210.000, mean reward:  0.240 [ 0.000, 30.000], mean action: 3.123 [0.000, 5.000],  loss: 17.089529, mean_q: 225.903302, mean_eps: 0.225345\n",
            " 861713/1000000: episode: 1224, duration: 3.323s, episode steps: 547, steps per second: 165, episode reward: 110.000, mean reward:  0.201 [ 0.000, 25.000], mean action: 2.704 [0.000, 5.000],  loss: 12.581446, mean_q: 225.745581, mean_eps: 0.224705\n",
            " 862276/1000000: episode: 1225, duration: 3.337s, episode steps: 563, steps per second: 169, episode reward: 105.000, mean reward:  0.187 [ 0.000, 25.000], mean action: 2.712 [0.000, 5.000],  loss: 30.231740, mean_q: 234.955982, mean_eps: 0.224205\n",
            " 862911/1000000: episode: 1226, duration: 3.787s, episode steps: 635, steps per second: 168, episode reward: 120.000, mean reward:  0.189 [ 0.000, 20.000], mean action: 3.409 [0.000, 5.000],  loss: 50.365190, mean_q: 244.175670, mean_eps: 0.223666\n",
            " 863541/1000000: episode: 1227, duration: 3.733s, episode steps: 630, steps per second: 169, episode reward: 95.000, mean reward:  0.151 [ 0.000, 25.000], mean action: 2.537 [0.000, 5.000],  loss: 34.627392, mean_q: 254.498076, mean_eps: 0.223097\n",
            " 864635/1000000: episode: 1228, duration: 6.509s, episode steps: 1094, steps per second: 168, episode reward: 295.000, mean reward:  0.270 [ 0.000, 30.000], mean action: 2.765 [0.000, 5.000],  loss: 17.140271, mean_q: 243.295956, mean_eps: 0.222321\n",
            " 865448/1000000: episode: 1229, duration: 4.833s, episode steps: 813, steps per second: 168, episode reward: 180.000, mean reward:  0.221 [ 0.000, 30.000], mean action: 1.881 [0.000, 5.000],  loss: 19.237973, mean_q: 239.675425, mean_eps: 0.221463\n",
            " 866469/1000000: episode: 1230, duration: 6.042s, episode steps: 1021, steps per second: 169, episode reward: 475.000, mean reward:  0.465 [ 0.000, 200.000], mean action: 2.449 [0.000, 5.000],  loss: 46.339522, mean_q: 243.150928, mean_eps: 0.220638\n",
            " 867050/1000000: episode: 1231, duration: 3.445s, episode steps: 581, steps per second: 169, episode reward: 100.000, mean reward:  0.172 [ 0.000, 20.000], mean action: 2.568 [0.000, 5.000],  loss: 28.645446, mean_q: 217.306181, mean_eps: 0.219917\n",
            " 867908/1000000: episode: 1232, duration: 5.102s, episode steps: 858, steps per second: 168, episode reward: 215.000, mean reward:  0.251 [ 0.000, 30.000], mean action: 2.388 [0.000, 5.000],  loss: 38.396801, mean_q: 241.701073, mean_eps: 0.219269\n",
            " 868497/1000000: episode: 1233, duration: 3.519s, episode steps: 589, steps per second: 167, episode reward: 110.000, mean reward:  0.187 [ 0.000, 30.000], mean action: 2.643 [0.000, 5.000],  loss: 32.351520, mean_q: 237.523865, mean_eps: 0.218618\n",
            " 869185/1000000: episode: 1234, duration: 4.101s, episode steps: 688, steps per second: 168, episode reward: 185.000, mean reward:  0.269 [ 0.000, 30.000], mean action: 2.484 [0.000, 5.000],  loss: 33.756005, mean_q: 238.089132, mean_eps: 0.218044\n",
            " 869988/1000000: episode: 1235, duration: 4.798s, episode steps: 803, steps per second: 167, episode reward: 180.000, mean reward:  0.224 [ 0.000, 30.000], mean action: 2.337 [0.000, 5.000],  loss: 25.440599, mean_q: 226.949254, mean_eps: 0.217373\n",
            " 870655/1000000: episode: 1236, duration: 3.988s, episode steps: 667, steps per second: 167, episode reward: 195.000, mean reward:  0.292 [ 0.000, 30.000], mean action: 2.588 [0.000, 5.000],  loss: 11.301207, mean_q: 219.231986, mean_eps: 0.216711\n",
            " 871159/1000000: episode: 1237, duration: 3.064s, episode steps: 504, steps per second: 164, episode reward: 70.000, mean reward:  0.139 [ 0.000, 20.000], mean action: 3.024 [0.000, 5.000],  loss: 16.914130, mean_q: 236.480295, mean_eps: 0.216184\n",
            " 872793/1000000: episode: 1238, duration: 9.637s, episode steps: 1634, steps per second: 170, episode reward: 530.000, mean reward:  0.324 [ 0.000, 30.000], mean action: 2.521 [0.000, 5.000],  loss: 31.779973, mean_q: 231.363429, mean_eps: 0.215222\n",
            " 873484/1000000: episode: 1239, duration: 4.094s, episode steps: 691, steps per second: 169, episode reward: 130.000, mean reward:  0.188 [ 0.000, 30.000], mean action: 3.204 [0.000, 5.000],  loss: 32.486038, mean_q: 183.461828, mean_eps: 0.214176\n",
            " 874822/1000000: episode: 1240, duration: 7.868s, episode steps: 1338, steps per second: 170, episode reward: 365.000, mean reward:  0.273 [ 0.000, 30.000], mean action: 2.509 [0.000, 5.000],  loss: 25.095308, mean_q: 245.349038, mean_eps: 0.213263\n",
            " 875809/1000000: episode: 1241, duration: 5.829s, episode steps: 987, steps per second: 169, episode reward: 395.000, mean reward:  0.400 [ 0.000, 30.000], mean action: 2.609 [0.000, 5.000],  loss: 20.843788, mean_q: 228.892371, mean_eps: 0.212216\n",
            " 876339/1000000: episode: 1242, duration: 3.149s, episode steps: 530, steps per second: 168, episode reward: 90.000, mean reward:  0.170 [ 0.000, 25.000], mean action: 2.298 [0.000, 5.000],  loss: 31.807513, mean_q: 228.433398, mean_eps: 0.211534\n",
            " 877141/1000000: episode: 1243, duration: 4.777s, episode steps: 802, steps per second: 168, episode reward: 185.000, mean reward:  0.231 [ 0.000, 30.000], mean action: 2.824 [0.000, 5.000],  loss: 36.172633, mean_q: 241.557906, mean_eps: 0.210934\n",
            " 877970/1000000: episode: 1244, duration: 4.953s, episode steps: 829, steps per second: 167, episode reward: 145.000, mean reward:  0.175 [ 0.000, 20.000], mean action: 2.631 [0.000, 5.000],  loss: 23.633254, mean_q: 245.694672, mean_eps: 0.210200\n",
            " 878487/1000000: episode: 1245, duration: 3.074s, episode steps: 517, steps per second: 168, episode reward: 55.000, mean reward:  0.106 [ 0.000, 10.000], mean action: 2.188 [0.000, 5.000],  loss: 18.594308, mean_q: 231.434409, mean_eps: 0.209595\n",
            " 878916/1000000: episode: 1246, duration: 2.550s, episode steps: 429, steps per second: 168, episode reward: 75.000, mean reward:  0.175 [ 0.000, 30.000], mean action: 2.308 [0.000, 5.000],  loss: 22.682399, mean_q: 234.511650, mean_eps: 0.209169\n",
            " 879561/1000000: episode: 1247, duration: 3.881s, episode steps: 645, steps per second: 166, episode reward: 110.000, mean reward:  0.171 [ 0.000, 30.000], mean action: 2.944 [0.000, 5.000],  loss: 26.623216, mean_q: 244.837834, mean_eps: 0.208686\n",
            " 880425/1000000: episode: 1248, duration: 5.131s, episode steps: 864, steps per second: 168, episode reward: 285.000, mean reward:  0.330 [ 0.000, 30.000], mean action: 2.819 [0.000, 5.000],  loss: 15.104034, mean_q: 239.760178, mean_eps: 0.208007\n",
            " 880916/1000000: episode: 1249, duration: 2.951s, episode steps: 491, steps per second: 166, episode reward: 80.000, mean reward:  0.163 [ 0.000, 30.000], mean action: 2.733 [0.000, 5.000],  loss: 15.178082, mean_q: 226.474611, mean_eps: 0.207397\n",
            " 881758/1000000: episode: 1250, duration: 5.022s, episode steps: 842, steps per second: 168, episode reward: 220.000, mean reward:  0.261 [ 0.000, 30.000], mean action: 2.101 [0.000, 5.000],  loss: 29.478778, mean_q: 228.507937, mean_eps: 0.206797\n",
            " 882718/1000000: episode: 1251, duration: 5.688s, episode steps: 960, steps per second: 169, episode reward: 195.000, mean reward:  0.203 [ 0.000, 30.000], mean action: 2.285 [0.000, 5.000],  loss: 15.354568, mean_q: 231.385131, mean_eps: 0.205986\n",
            " 883334/1000000: episode: 1252, duration: 3.729s, episode steps: 616, steps per second: 165, episode reward: 95.000, mean reward:  0.154 [ 0.000, 30.000], mean action: 2.964 [0.000, 5.000],  loss: 16.543998, mean_q: 247.825156, mean_eps: 0.205277\n",
            " 884011/1000000: episode: 1253, duration: 4.061s, episode steps: 677, steps per second: 167, episode reward: 150.000, mean reward:  0.222 [ 0.000, 25.000], mean action: 2.601 [0.000, 5.000],  loss: 22.213455, mean_q: 243.031266, mean_eps: 0.204695\n",
            " 885114/1000000: episode: 1254, duration: 6.497s, episode steps: 1103, steps per second: 170, episode reward: 440.000, mean reward:  0.399 [ 0.000, 200.000], mean action: 2.457 [0.000, 5.000],  loss: 29.723133, mean_q: 241.967836, mean_eps: 0.203894\n",
            " 886124/1000000: episode: 1255, duration: 5.979s, episode steps: 1010, steps per second: 169, episode reward: 435.000, mean reward:  0.431 [ 0.000, 200.000], mean action: 2.497 [0.000, 5.000],  loss: 24.128810, mean_q: 228.489049, mean_eps: 0.202943\n",
            " 886961/1000000: episode: 1256, duration: 4.930s, episode steps: 837, steps per second: 170, episode reward: 195.000, mean reward:  0.233 [ 0.000, 30.000], mean action: 2.417 [0.000, 5.000],  loss: 36.306360, mean_q: 219.157326, mean_eps: 0.202112\n",
            " 887619/1000000: episode: 1257, duration: 3.910s, episode steps: 658, steps per second: 168, episode reward: 70.000, mean reward:  0.106 [ 0.000, 15.000], mean action: 2.883 [0.000, 5.000],  loss: 13.746223, mean_q: 226.424397, mean_eps: 0.201439\n",
            " 888232/1000000: episode: 1258, duration: 3.690s, episode steps: 613, steps per second: 166, episode reward: 125.000, mean reward:  0.204 [ 0.000, 25.000], mean action: 2.261 [0.000, 5.000],  loss: 32.295584, mean_q: 243.113947, mean_eps: 0.200867\n",
            " 888682/1000000: episode: 1259, duration: 2.705s, episode steps: 450, steps per second: 166, episode reward: 110.000, mean reward:  0.244 [ 0.000, 30.000], mean action: 2.669 [0.000, 5.000],  loss: 39.648701, mean_q: 242.036961, mean_eps: 0.200389\n",
            " 889078/1000000: episode: 1260, duration: 2.361s, episode steps: 396, steps per second: 168, episode reward: 115.000, mean reward:  0.290 [ 0.000, 30.000], mean action: 2.795 [0.000, 5.000],  loss: 38.272565, mean_q: 234.199028, mean_eps: 0.200008\n",
            " 889723/1000000: episode: 1261, duration: 3.907s, episode steps: 645, steps per second: 165, episode reward: 120.000, mean reward:  0.186 [ 0.000, 30.000], mean action: 2.730 [0.000, 5.000],  loss: 38.166521, mean_q: 236.460452, mean_eps: 0.199540\n",
            " 890660/1000000: episode: 1262, duration: 5.602s, episode steps: 937, steps per second: 167, episode reward: 220.000, mean reward:  0.235 [ 0.000, 30.000], mean action: 2.718 [0.000, 5.000],  loss: 18.947625, mean_q: 231.607157, mean_eps: 0.198828\n",
            " 891506/1000000: episode: 1263, duration: 5.007s, episode steps: 846, steps per second: 169, episode reward: 290.000, mean reward:  0.343 [ 0.000, 30.000], mean action: 2.669 [0.000, 5.000],  loss: 14.631393, mean_q: 223.757566, mean_eps: 0.198026\n",
            " 892596/1000000: episode: 1264, duration: 6.478s, episode steps: 1090, steps per second: 168, episode reward: 345.000, mean reward:  0.317 [ 0.000, 30.000], mean action: 2.300 [0.000, 5.000],  loss: 13.168401, mean_q: 231.981120, mean_eps: 0.197155\n",
            " 893646/1000000: episode: 1265, duration: 6.233s, episode steps: 1050, steps per second: 168, episode reward: 220.000, mean reward:  0.210 [ 0.000, 30.000], mean action: 2.461 [0.000, 5.000],  loss: 16.058439, mean_q: 222.022266, mean_eps: 0.196192\n",
            " 894926/1000000: episode: 1266, duration: 7.678s, episode steps: 1280, steps per second: 167, episode reward: 360.000, mean reward:  0.281 [ 0.000, 30.000], mean action: 2.945 [0.000, 5.000],  loss: 12.259544, mean_q: 233.029473, mean_eps: 0.195143\n",
            " 895607/1000000: episode: 1267, duration: 4.098s, episode steps: 681, steps per second: 166, episode reward: 190.000, mean reward:  0.279 [ 0.000, 30.000], mean action: 2.571 [0.000, 5.000],  loss: 26.087339, mean_q: 204.446436, mean_eps: 0.194261\n",
            " 896555/1000000: episode: 1268, duration: 5.666s, episode steps: 948, steps per second: 167, episode reward: 180.000, mean reward:  0.190 [ 0.000, 30.000], mean action: 2.532 [0.000, 5.000],  loss: 24.700695, mean_q: 234.557906, mean_eps: 0.193528\n",
            " 897278/1000000: episode: 1269, duration: 4.352s, episode steps: 723, steps per second: 166, episode reward: 215.000, mean reward:  0.297 [ 0.000, 30.000], mean action: 2.019 [0.000, 5.000],  loss: 17.280152, mean_q: 234.244956, mean_eps: 0.192776\n",
            " 897678/1000000: episode: 1270, duration: 2.385s, episode steps: 400, steps per second: 168, episode reward: 50.000, mean reward:  0.125 [ 0.000, 25.000], mean action: 3.200 [0.000, 5.000],  loss: 32.207220, mean_q: 233.291267, mean_eps: 0.192270\n",
            " 898175/1000000: episode: 1271, duration: 2.993s, episode steps: 497, steps per second: 166, episode reward: 95.000, mean reward:  0.191 [ 0.000, 25.000], mean action: 2.763 [0.000, 5.000],  loss: 64.086614, mean_q: 235.045269, mean_eps: 0.191867\n",
            " 898528/1000000: episode: 1272, duration: 2.129s, episode steps: 353, steps per second: 166, episode reward: 20.000, mean reward:  0.057 [ 0.000, 10.000], mean action: 2.921 [0.000, 5.000],  loss: 74.100394, mean_q: 241.157179, mean_eps: 0.191484\n",
            " 899084/1000000: episode: 1273, duration: 3.348s, episode steps: 556, steps per second: 166, episode reward: 75.000, mean reward:  0.135 [ 0.000, 15.000], mean action: 2.365 [0.000, 5.000],  loss: 53.909097, mean_q: 248.025780, mean_eps: 0.191075\n",
            " 899822/1000000: episode: 1274, duration: 4.425s, episode steps: 738, steps per second: 167, episode reward: 95.000, mean reward:  0.129 [ 0.000, 20.000], mean action: 2.575 [0.000, 5.000],  loss: 36.014593, mean_q: 249.713216, mean_eps: 0.190493\n",
            " 900591/1000000: episode: 1275, duration: 4.597s, episode steps: 769, steps per second: 167, episode reward: 270.000, mean reward:  0.351 [ 0.000, 30.000], mean action: 1.681 [0.000, 5.000],  loss: 19.522553, mean_q: 229.972194, mean_eps: 0.189815\n",
            " 901086/1000000: episode: 1276, duration: 3.014s, episode steps: 495, steps per second: 164, episode reward: 75.000, mean reward:  0.152 [ 0.000, 15.000], mean action: 3.293 [0.000, 5.000],  loss: 19.031264, mean_q: 215.062275, mean_eps: 0.189246\n",
            " 901666/1000000: episode: 1277, duration: 3.462s, episode steps: 580, steps per second: 168, episode reward: 105.000, mean reward:  0.181 [ 0.000, 30.000], mean action: 2.600 [0.000, 5.000],  loss: 26.854733, mean_q: 224.167924, mean_eps: 0.188762\n",
            " 902319/1000000: episode: 1278, duration: 3.872s, episode steps: 653, steps per second: 169, episode reward: 150.000, mean reward:  0.230 [ 0.000, 30.000], mean action: 2.822 [0.000, 5.000],  loss: 25.455061, mean_q: 230.363239, mean_eps: 0.188207\n",
            " 903016/1000000: episode: 1279, duration: 4.195s, episode steps: 697, steps per second: 166, episode reward: 210.000, mean reward:  0.301 [ 0.000, 30.000], mean action: 3.237 [0.000, 5.000],  loss: 16.953196, mean_q: 226.457900, mean_eps: 0.187600\n",
            " 903815/1000000: episode: 1280, duration: 4.780s, episode steps: 799, steps per second: 167, episode reward: 135.000, mean reward:  0.169 [ 0.000, 30.000], mean action: 2.840 [0.000, 5.000],  loss: 15.081484, mean_q: 219.656636, mean_eps: 0.186926\n",
            " 904905/1000000: episode: 1281, duration: 6.524s, episode steps: 1090, steps per second: 167, episode reward: 430.000, mean reward:  0.394 [ 0.000, 200.000], mean action: 2.932 [0.000, 5.000],  loss: 31.429467, mean_q: 219.121284, mean_eps: 0.186076\n",
            " 905697/1000000: episode: 1282, duration: 4.713s, episode steps: 792, steps per second: 168, episode reward: 210.000, mean reward:  0.265 [ 0.000, 30.000], mean action: 2.662 [0.000, 5.000],  loss: 19.754416, mean_q: 207.614108, mean_eps: 0.185230\n",
            " 906350/1000000: episode: 1283, duration: 3.927s, episode steps: 653, steps per second: 166, episode reward: 180.000, mean reward:  0.276 [ 0.000, 30.000], mean action: 2.842 [0.000, 5.000],  loss: 7.711467, mean_q: 212.142879, mean_eps: 0.184579\n",
            " 907036/1000000: episode: 1284, duration: 4.111s, episode steps: 686, steps per second: 167, episode reward: 125.000, mean reward:  0.182 [ 0.000, 30.000], mean action: 2.972 [0.000, 5.000],  loss: 19.597879, mean_q: 234.228009, mean_eps: 0.183977\n",
            " 907569/1000000: episode: 1285, duration: 3.207s, episode steps: 533, steps per second: 166, episode reward: 65.000, mean reward:  0.122 [ 0.000, 25.000], mean action: 2.424 [0.000, 5.000],  loss: 18.844573, mean_q: 230.637254, mean_eps: 0.183428\n",
            " 907965/1000000: episode: 1286, duration: 2.382s, episode steps: 396, steps per second: 166, episode reward: 65.000, mean reward:  0.164 [ 0.000, 30.000], mean action: 2.960 [0.000, 5.000],  loss: 42.126666, mean_q: 226.326560, mean_eps: 0.183010\n",
            " 908297/1000000: episode: 1287, duration: 2.003s, episode steps: 332, steps per second: 166, episode reward: 35.000, mean reward:  0.105 [ 0.000, 10.000], mean action: 3.238 [0.000, 5.000],  loss: 36.548678, mean_q: 232.641072, mean_eps: 0.182683\n",
            " 908913/1000000: episode: 1288, duration: 3.701s, episode steps: 616, steps per second: 166, episode reward: 65.000, mean reward:  0.106 [ 0.000, 20.000], mean action: 2.875 [0.000, 5.000],  loss: 58.297093, mean_q: 234.301463, mean_eps: 0.182256\n",
            " 909740/1000000: episode: 1289, duration: 4.918s, episode steps: 827, steps per second: 168, episode reward: 210.000, mean reward:  0.254 [ 0.000, 30.000], mean action: 2.603 [0.000, 5.000],  loss: 36.632705, mean_q: 233.561898, mean_eps: 0.181607\n",
            " 910780/1000000: episode: 1290, duration: 6.234s, episode steps: 1040, steps per second: 167, episode reward: 255.000, mean reward:  0.245 [ 0.000, 30.000], mean action: 2.647 [0.000, 5.000],  loss: 10.741175, mean_q: 218.781252, mean_eps: 0.180766\n",
            " 911523/1000000: episode: 1291, duration: 4.458s, episode steps: 743, steps per second: 167, episode reward: 110.000, mean reward:  0.148 [ 0.000, 30.000], mean action: 2.324 [0.000, 5.000],  loss: 8.274235, mean_q: 213.901641, mean_eps: 0.179964\n",
            " 911993/1000000: episode: 1292, duration: 2.838s, episode steps: 470, steps per second: 166, episode reward: 60.000, mean reward:  0.128 [ 0.000, 20.000], mean action: 2.238 [0.000, 5.000],  loss: 11.868834, mean_q: 215.706354, mean_eps: 0.179418\n",
            " 913226/1000000: episode: 1293, duration: 7.332s, episode steps: 1233, steps per second: 168, episode reward: 245.000, mean reward:  0.199 [ 0.000, 30.000], mean action: 2.391 [0.000, 5.000],  loss: 24.986330, mean_q: 222.037866, mean_eps: 0.178652\n",
            " 913758/1000000: episode: 1294, duration: 3.219s, episode steps: 532, steps per second: 165, episode reward: 85.000, mean reward:  0.160 [ 0.000, 25.000], mean action: 1.932 [0.000, 5.000],  loss: 28.385601, mean_q: 202.047898, mean_eps: 0.177858\n",
            " 914403/1000000: episode: 1295, duration: 3.907s, episode steps: 645, steps per second: 165, episode reward: 160.000, mean reward:  0.248 [ 0.000, 30.000], mean action: 1.478 [0.000, 5.000],  loss: 21.307260, mean_q: 222.386503, mean_eps: 0.177328\n",
            " 915067/1000000: episode: 1296, duration: 3.985s, episode steps: 664, steps per second: 167, episode reward: 125.000, mean reward:  0.188 [ 0.000, 25.000], mean action: 1.869 [0.000, 5.000],  loss: 18.355992, mean_q: 234.042038, mean_eps: 0.176739\n",
            " 915580/1000000: episode: 1297, duration: 3.090s, episode steps: 513, steps per second: 166, episode reward: 60.000, mean reward:  0.117 [ 0.000, 15.000], mean action: 2.331 [0.000, 5.000],  loss: 28.090411, mean_q: 239.947942, mean_eps: 0.176209\n",
            " 916273/1000000: episode: 1298, duration: 4.181s, episode steps: 693, steps per second: 166, episode reward: 120.000, mean reward:  0.173 [ 0.000, 30.000], mean action: 1.810 [0.000, 5.000],  loss: 33.236950, mean_q: 234.548779, mean_eps: 0.175667\n",
            " 917026/1000000: episode: 1299, duration: 4.556s, episode steps: 753, steps per second: 165, episode reward: 150.000, mean reward:  0.199 [ 0.000, 30.000], mean action: 2.416 [0.000, 5.000],  loss: 20.599624, mean_q: 223.478620, mean_eps: 0.175016\n",
            " 917694/1000000: episode: 1300, duration: 3.987s, episode steps: 668, steps per second: 168, episode reward: 125.000, mean reward:  0.187 [ 0.000, 30.000], mean action: 1.915 [0.000, 5.000],  loss: 18.152433, mean_q: 221.655306, mean_eps: 0.174376\n",
            " 918339/1000000: episode: 1301, duration: 3.856s, episode steps: 645, steps per second: 167, episode reward: 95.000, mean reward:  0.147 [ 0.000, 25.000], mean action: 2.259 [0.000, 5.000],  loss: 15.181385, mean_q: 236.603268, mean_eps: 0.173786\n",
            " 919193/1000000: episode: 1302, duration: 5.137s, episode steps: 854, steps per second: 166, episode reward: 225.000, mean reward:  0.263 [ 0.000, 30.000], mean action: 1.966 [0.000, 5.000],  loss: 21.893709, mean_q: 236.166874, mean_eps: 0.173111\n",
            " 919843/1000000: episode: 1303, duration: 3.863s, episode steps: 650, steps per second: 168, episode reward: 210.000, mean reward:  0.323 [ 0.000, 30.000], mean action: 2.171 [0.000, 5.000],  loss: 14.140648, mean_q: 220.105741, mean_eps: 0.172434\n",
            " 920556/1000000: episode: 1304, duration: 4.309s, episode steps: 713, steps per second: 165, episode reward: 155.000, mean reward:  0.217 [ 0.000, 30.000], mean action: 2.171 [0.000, 5.000],  loss: 13.235922, mean_q: 224.933623, mean_eps: 0.171821\n",
            " 921998/1000000: episode: 1305, duration: 8.570s, episode steps: 1442, steps per second: 168, episode reward: 420.000, mean reward:  0.291 [ 0.000, 30.000], mean action: 2.939 [0.000, 5.000],  loss: 22.453156, mean_q: 217.009008, mean_eps: 0.170851\n",
            " 922580/1000000: episode: 1306, duration: 3.507s, episode steps: 582, steps per second: 166, episode reward: 60.000, mean reward:  0.103 [ 0.000, 30.000], mean action: 1.543 [0.000, 5.000],  loss: 55.253514, mean_q: 171.233380, mean_eps: 0.169940\n",
            " 923124/1000000: episode: 1307, duration: 3.286s, episode steps: 544, steps per second: 166, episode reward: 55.000, mean reward:  0.101 [ 0.000, 20.000], mean action: 2.408 [0.000, 5.000],  loss: 35.936760, mean_q: 222.827915, mean_eps: 0.169434\n",
            " 923884/1000000: episode: 1308, duration: 4.598s, episode steps: 760, steps per second: 165, episode reward: 245.000, mean reward:  0.322 [ 0.000, 30.000], mean action: 2.836 [0.000, 5.000],  loss: 34.384022, mean_q: 231.040152, mean_eps: 0.168847\n",
            " 924342/1000000: episode: 1309, duration: 2.764s, episode steps: 458, steps per second: 166, episode reward: 90.000, mean reward:  0.197 [ 0.000, 25.000], mean action: 2.509 [0.000, 5.000],  loss: 19.279219, mean_q: 211.681929, mean_eps: 0.168299\n",
            " 925122/1000000: episode: 1310, duration: 4.679s, episode steps: 780, steps per second: 167, episode reward: 235.000, mean reward:  0.301 [ 0.000, 30.000], mean action: 2.246 [0.000, 5.000],  loss: 28.518601, mean_q: 222.065370, mean_eps: 0.167742\n",
            " 926431/1000000: episode: 1311, duration: 7.737s, episode steps: 1309, steps per second: 169, episode reward: 385.000, mean reward:  0.294 [ 0.000, 30.000], mean action: 2.355 [0.000, 5.000],  loss: 16.465955, mean_q: 224.776886, mean_eps: 0.166802\n",
            " 927129/1000000: episode: 1312, duration: 4.138s, episode steps: 698, steps per second: 169, episode reward: 135.000, mean reward:  0.193 [ 0.000, 30.000], mean action: 2.905 [0.000, 5.000],  loss: 12.869503, mean_q: 191.223602, mean_eps: 0.165898\n",
            " 927787/1000000: episode: 1313, duration: 3.920s, episode steps: 658, steps per second: 168, episode reward: 110.000, mean reward:  0.167 [ 0.000, 30.000], mean action: 2.471 [0.000, 5.000],  loss: 25.300201, mean_q: 226.440733, mean_eps: 0.165288\n",
            " 928590/1000000: episode: 1314, duration: 4.773s, episode steps: 803, steps per second: 168, episode reward: 255.000, mean reward:  0.318 [ 0.000, 30.000], mean action: 1.976 [0.000, 5.000],  loss: 17.332121, mean_q: 223.930746, mean_eps: 0.164631\n",
            " 928979/1000000: episode: 1315, duration: 2.315s, episode steps: 389, steps per second: 168, episode reward: 75.000, mean reward:  0.193 [ 0.000, 25.000], mean action: 2.303 [0.000, 5.000],  loss: 17.107162, mean_q: 219.615832, mean_eps: 0.164094\n",
            " 929473/1000000: episode: 1316, duration: 3.003s, episode steps: 494, steps per second: 165, episode reward: 50.000, mean reward:  0.101 [ 0.000, 20.000], mean action: 1.611 [0.000, 5.000],  loss: 40.758205, mean_q: 223.106770, mean_eps: 0.163697\n",
            " 930543/1000000: episode: 1317, duration: 6.346s, episode steps: 1070, steps per second: 169, episode reward: 305.000, mean reward:  0.285 [ 0.000, 30.000], mean action: 2.936 [0.000, 5.000],  loss: 20.625950, mean_q: 240.183942, mean_eps: 0.162993\n",
            " 931299/1000000: episode: 1318, duration: 4.474s, episode steps: 756, steps per second: 169, episode reward: 170.000, mean reward:  0.225 [ 0.000, 30.000], mean action: 3.173 [0.000, 5.000],  loss: 14.254450, mean_q: 225.158332, mean_eps: 0.162172\n",
            " 931958/1000000: episode: 1319, duration: 3.913s, episode steps: 659, steps per second: 168, episode reward: 90.000, mean reward:  0.137 [ 0.000, 30.000], mean action: 2.813 [0.000, 5.000],  loss: 13.151059, mean_q: 226.617371, mean_eps: 0.161535\n",
            " 932632/1000000: episode: 1320, duration: 4.005s, episode steps: 674, steps per second: 168, episode reward: 135.000, mean reward:  0.200 [ 0.000, 25.000], mean action: 1.809 [0.000, 5.000],  loss: 17.140722, mean_q: 234.172887, mean_eps: 0.160935\n",
            " 933711/1000000: episode: 1321, duration: 6.378s, episode steps: 1079, steps per second: 169, episode reward: 420.000, mean reward:  0.389 [ 0.000, 30.000], mean action: 2.095 [0.000, 5.000],  loss: 22.605610, mean_q: 227.917178, mean_eps: 0.160146\n",
            " 934287/1000000: episode: 1322, duration: 3.445s, episode steps: 576, steps per second: 167, episode reward: 125.000, mean reward:  0.217 [ 0.000, 30.000], mean action: 2.082 [0.000, 5.000],  loss: 29.365298, mean_q: 189.319871, mean_eps: 0.159401\n",
            " 935098/1000000: episode: 1323, duration: 4.858s, episode steps: 811, steps per second: 167, episode reward: 135.000, mean reward:  0.166 [ 0.000, 30.000], mean action: 2.245 [0.000, 5.000],  loss: 22.331268, mean_q: 217.114170, mean_eps: 0.158777\n",
            " 935498/1000000: episode: 1324, duration: 2.408s, episode steps: 400, steps per second: 166, episode reward: 85.000, mean reward:  0.212 [ 0.000, 25.000], mean action: 2.098 [0.000, 5.000],  loss: 17.664370, mean_q: 214.416186, mean_eps: 0.158232\n",
            " 936557/1000000: episode: 1325, duration: 6.255s, episode steps: 1059, steps per second: 169, episode reward: 285.000, mean reward:  0.269 [ 0.000, 30.000], mean action: 1.832 [0.000, 5.000],  loss: 28.995135, mean_q: 216.183698, mean_eps: 0.157576\n",
            " 937203/1000000: episode: 1326, duration: 3.921s, episode steps: 646, steps per second: 165, episode reward: 135.000, mean reward:  0.209 [ 0.000, 30.000], mean action: 1.935 [0.000, 5.000],  loss: 19.599031, mean_q: 186.409047, mean_eps: 0.156808\n",
            " 937852/1000000: episode: 1327, duration: 3.971s, episode steps: 649, steps per second: 163, episode reward: 110.000, mean reward:  0.169 [ 0.000, 30.000], mean action: 2.921 [0.000, 5.000],  loss: 18.881032, mean_q: 202.516767, mean_eps: 0.156226\n",
            " 938262/1000000: episode: 1328, duration: 2.524s, episode steps: 410, steps per second: 162, episode reward: 100.000, mean reward:  0.244 [ 0.000, 20.000], mean action: 2.802 [0.000, 5.000],  loss: 14.929008, mean_q: 214.291669, mean_eps: 0.155749\n",
            " 938762/1000000: episode: 1329, duration: 3.074s, episode steps: 500, steps per second: 163, episode reward: 165.000, mean reward:  0.330 [ 0.000, 30.000], mean action: 2.422 [0.000, 5.000],  loss: 37.138577, mean_q: 223.695284, mean_eps: 0.155340\n",
            " 939449/1000000: episode: 1330, duration: 4.208s, episode steps: 687, steps per second: 163, episode reward: 150.000, mean reward:  0.218 [ 0.000, 25.000], mean action: 2.093 [0.000, 5.000],  loss: 34.625102, mean_q: 232.156200, mean_eps: 0.154805\n",
            " 940931/1000000: episode: 1331, duration: 9.000s, episode steps: 1482, steps per second: 165, episode reward: 365.000, mean reward:  0.246 [ 0.000, 30.000], mean action: 2.666 [0.000, 5.000],  loss: 19.419457, mean_q: 224.102933, mean_eps: 0.153829\n",
            " 941726/1000000: episode: 1332, duration: 4.844s, episode steps: 795, steps per second: 164, episode reward: 270.000, mean reward:  0.340 [ 0.000, 30.000], mean action: 2.636 [0.000, 5.000],  loss: 15.262363, mean_q: 187.968945, mean_eps: 0.152805\n",
            " 942130/1000000: episode: 1333, duration: 2.485s, episode steps: 404, steps per second: 163, episode reward: 30.000, mean reward:  0.074 [ 0.000, 10.000], mean action: 2.735 [0.000, 5.000],  loss: 10.379090, mean_q: 215.511237, mean_eps: 0.152265\n",
            " 943502/1000000: episode: 1334, duration: 8.310s, episode steps: 1372, steps per second: 165, episode reward: 395.000, mean reward:  0.288 [ 0.000, 30.000], mean action: 3.129 [0.000, 5.000],  loss: 21.698045, mean_q: 214.821200, mean_eps: 0.151466\n",
            " 944561/1000000: episode: 1335, duration: 6.455s, episode steps: 1059, steps per second: 164, episode reward: 215.000, mean reward:  0.203 [ 0.000, 30.000], mean action: 2.276 [0.000, 5.000],  loss: 19.588527, mean_q: 189.227081, mean_eps: 0.150372\n",
            " 945357/1000000: episode: 1336, duration: 4.874s, episode steps: 796, steps per second: 163, episode reward: 350.000, mean reward:  0.440 [ 0.000, 200.000], mean action: 2.867 [0.000, 5.000],  loss: 13.185928, mean_q: 207.777617, mean_eps: 0.149537\n",
            " 946354/1000000: episode: 1337, duration: 6.082s, episode steps: 997, steps per second: 164, episode reward: 415.000, mean reward:  0.416 [ 0.000, 200.000], mean action: 2.691 [0.000, 5.000],  loss: 37.345461, mean_q: 225.368283, mean_eps: 0.148730\n",
            " 946867/1000000: episode: 1338, duration: 3.171s, episode steps: 513, steps per second: 162, episode reward: 70.000, mean reward:  0.136 [ 0.000, 20.000], mean action: 1.955 [0.000, 5.000],  loss: 27.399096, mean_q: 206.805370, mean_eps: 0.148051\n",
            " 947935/1000000: episode: 1339, duration: 6.419s, episode steps: 1068, steps per second: 166, episode reward: 430.000, mean reward:  0.403 [ 0.000, 30.000], mean action: 1.777 [0.000, 5.000],  loss: 23.685010, mean_q: 216.862663, mean_eps: 0.147340\n",
            " 948809/1000000: episode: 1340, duration: 5.366s, episode steps: 874, steps per second: 163, episode reward: 110.000, mean reward:  0.126 [ 0.000, 20.000], mean action: 2.598 [0.000, 5.000],  loss: 21.130267, mean_q: 199.042503, mean_eps: 0.146466\n",
            " 949777/1000000: episode: 1341, duration: 5.896s, episode steps: 968, steps per second: 164, episode reward: 165.000, mean reward:  0.170 [ 0.000, 25.000], mean action: 2.919 [0.000, 5.000],  loss: 14.998489, mean_q: 232.831295, mean_eps: 0.145637\n",
            " 950437/1000000: episode: 1342, duration: 4.020s, episode steps: 660, steps per second: 164, episode reward: 130.000, mean reward:  0.197 [ 0.000, 25.000], mean action: 2.320 [0.000, 5.000],  loss: 11.748122, mean_q: 219.012993, mean_eps: 0.144904\n",
            " 951237/1000000: episode: 1343, duration: 4.801s, episode steps: 800, steps per second: 167, episode reward: 385.000, mean reward:  0.481 [ 0.000, 200.000], mean action: 2.625 [0.000, 5.000],  loss: 22.596324, mean_q: 213.880019, mean_eps: 0.144247\n",
            " 951610/1000000: episode: 1344, duration: 2.318s, episode steps: 373, steps per second: 161, episode reward: 75.000, mean reward:  0.201 [ 0.000, 25.000], mean action: 2.056 [0.000, 5.000],  loss: 32.358912, mean_q: 222.575285, mean_eps: 0.143719\n",
            " 952569/1000000: episode: 1345, duration: 5.806s, episode steps: 959, steps per second: 165, episode reward: 260.000, mean reward:  0.271 [ 0.000, 30.000], mean action: 2.240 [0.000, 5.000],  loss: 32.514283, mean_q: 219.933461, mean_eps: 0.143120\n",
            " 953378/1000000: episode: 1346, duration: 4.889s, episode steps: 809, steps per second: 165, episode reward: 180.000, mean reward:  0.222 [ 0.000, 30.000], mean action: 2.176 [0.000, 5.000],  loss: 13.399483, mean_q: 197.716202, mean_eps: 0.142324\n",
            " 954060/1000000: episode: 1347, duration: 4.167s, episode steps: 682, steps per second: 164, episode reward: 220.000, mean reward:  0.323 [ 0.000, 25.000], mean action: 2.453 [0.000, 5.000],  loss: 8.589071, mean_q: 210.224445, mean_eps: 0.141653\n",
            " 955942/1000000: episode: 1348, duration: 11.297s, episode steps: 1882, steps per second: 167, episode reward: 660.000, mean reward:  0.351 [ 0.000, 200.000], mean action: 2.658 [0.000, 5.000],  loss: 31.527326, mean_q: 191.744256, mean_eps: 0.140500\n",
            " 956633/1000000: episode: 1349, duration: 4.182s, episode steps: 691, steps per second: 165, episode reward: 160.000, mean reward:  0.232 [ 0.000, 30.000], mean action: 2.980 [0.000, 5.000],  loss: 34.777338, mean_q: 168.197222, mean_eps: 0.139342\n",
            " 957829/1000000: episode: 1350, duration: 7.218s, episode steps: 1196, steps per second: 166, episode reward: 345.000, mean reward:  0.288 [ 0.000, 30.000], mean action: 2.143 [0.000, 5.000],  loss: 14.131099, mean_q: 217.102662, mean_eps: 0.138493\n",
            " 959435/1000000: episode: 1351, duration: 9.481s, episode steps: 1606, steps per second: 169, episode reward: 655.000, mean reward:  0.408 [ 0.000, 200.000], mean action: 1.701 [0.000, 5.000],  loss: 25.587821, mean_q: 197.099542, mean_eps: 0.137232\n",
            " 960476/1000000: episode: 1352, duration: 6.200s, episode steps: 1041, steps per second: 168, episode reward: 230.000, mean reward:  0.221 [ 0.000, 30.000], mean action: 2.233 [0.000, 5.000],  loss: 19.563707, mean_q: 176.832236, mean_eps: 0.136040\n",
            " 961293/1000000: episode: 1353, duration: 4.910s, episode steps: 817, steps per second: 166, episode reward: 195.000, mean reward:  0.239 [ 0.000, 30.000], mean action: 2.946 [0.000, 5.000],  loss: 14.403110, mean_q: 216.916296, mean_eps: 0.135204\n",
            " 961934/1000000: episode: 1354, duration: 3.864s, episode steps: 641, steps per second: 166, episode reward: 105.000, mean reward:  0.164 [ 0.000, 30.000], mean action: 3.036 [0.000, 5.000],  loss: 14.715935, mean_q: 209.109235, mean_eps: 0.134548\n",
            " 962744/1000000: episode: 1355, duration: 4.867s, episode steps: 810, steps per second: 166, episode reward: 145.000, mean reward:  0.179 [ 0.000, 30.000], mean action: 2.949 [0.000, 5.000],  loss: 14.069045, mean_q: 216.994309, mean_eps: 0.133895\n",
            " 963811/1000000: episode: 1356, duration: 6.441s, episode steps: 1067, steps per second: 166, episode reward: 230.000, mean reward:  0.216 [ 0.000, 30.000], mean action: 2.021 [0.000, 5.000],  loss: 18.351921, mean_q: 228.350512, mean_eps: 0.133051\n",
            " 964431/1000000: episode: 1357, duration: 3.752s, episode steps: 620, steps per second: 165, episode reward: 75.000, mean reward:  0.121 [ 0.000, 20.000], mean action: 1.877 [0.000, 5.000],  loss: 30.113656, mean_q: 231.177044, mean_eps: 0.132292\n",
            " 965202/1000000: episode: 1358, duration: 4.572s, episode steps: 771, steps per second: 169, episode reward: 275.000, mean reward:  0.357 [ 0.000, 30.000], mean action: 1.820 [0.000, 5.000],  loss: 19.850193, mean_q: 238.365727, mean_eps: 0.131666\n",
            " 965742/1000000: episode: 1359, duration: 3.204s, episode steps: 540, steps per second: 169, episode reward: 65.000, mean reward:  0.120 [ 0.000, 25.000], mean action: 2.700 [0.000, 5.000],  loss: 16.923542, mean_q: 226.845575, mean_eps: 0.131076\n",
            " 966759/1000000: episode: 1360, duration: 6.047s, episode steps: 1017, steps per second: 168, episode reward: 595.000, mean reward:  0.585 [ 0.000, 200.000], mean action: 2.355 [0.000, 5.000],  loss: 31.162139, mean_q: 221.657775, mean_eps: 0.130375\n",
            " 967075/1000000: episode: 1361, duration: 1.888s, episode steps: 316, steps per second: 167, episode reward: 40.000, mean reward:  0.127 [ 0.000, 25.000], mean action: 3.478 [0.000, 5.000],  loss: 32.929010, mean_q: 194.324735, mean_eps: 0.129775\n",
            " 967535/1000000: episode: 1362, duration: 2.759s, episode steps: 460, steps per second: 167, episode reward: 20.000, mean reward:  0.043 [ 0.000, 10.000], mean action: 2.959 [0.000, 5.000],  loss: 36.860242, mean_q: 198.114632, mean_eps: 0.129426\n",
            " 968259/1000000: episode: 1363, duration: 4.283s, episode steps: 724, steps per second: 169, episode reward: 165.000, mean reward:  0.228 [ 0.000, 30.000], mean action: 2.610 [0.000, 5.000],  loss: 34.519225, mean_q: 229.729863, mean_eps: 0.128893\n",
            " 969087/1000000: episode: 1364, duration: 4.946s, episode steps: 828, steps per second: 167, episode reward: 210.000, mean reward:  0.254 [ 0.000, 30.000], mean action: 1.187 [0.000, 5.000],  loss: 22.766467, mean_q: 227.162672, mean_eps: 0.128195\n",
            " 969442/1000000: episode: 1365, duration: 2.121s, episode steps: 355, steps per second: 167, episode reward: 65.000, mean reward:  0.183 [ 0.000, 20.000], mean action: 3.203 [0.000, 5.000],  loss: 24.914008, mean_q: 202.304585, mean_eps: 0.127662\n",
            " 970281/1000000: episode: 1366, duration: 4.983s, episode steps: 839, steps per second: 168, episode reward: 260.000, mean reward:  0.310 [ 0.000, 30.000], mean action: 2.702 [0.000, 5.000],  loss: 33.879086, mean_q: 213.132752, mean_eps: 0.127125\n",
            " 971112/1000000: episode: 1367, duration: 4.957s, episode steps: 831, steps per second: 168, episode reward: 240.000, mean reward:  0.289 [ 0.000, 30.000], mean action: 2.562 [0.000, 5.000],  loss: 15.167653, mean_q: 212.553871, mean_eps: 0.126374\n",
            " 972202/1000000: episode: 1368, duration: 6.509s, episode steps: 1090, steps per second: 167, episode reward: 225.000, mean reward:  0.206 [ 0.000, 30.000], mean action: 2.919 [0.000, 5.000],  loss: 10.135758, mean_q: 205.508026, mean_eps: 0.125509\n",
            " 973160/1000000: episode: 1369, duration: 5.777s, episode steps: 958, steps per second: 166, episode reward: 195.000, mean reward:  0.204 [ 0.000, 30.000], mean action: 2.745 [0.000, 5.000],  loss: 11.434870, mean_q: 209.688811, mean_eps: 0.124588\n",
            " 973670/1000000: episode: 1370, duration: 3.151s, episode steps: 510, steps per second: 162, episode reward: 105.000, mean reward:  0.206 [ 0.000, 25.000], mean action: 2.914 [0.000, 5.000],  loss: 6.746047, mean_q: 222.761852, mean_eps: 0.123927\n",
            " 974531/1000000: episode: 1371, duration: 5.204s, episode steps: 861, steps per second: 165, episode reward: 210.000, mean reward:  0.244 [ 0.000, 30.000], mean action: 2.955 [0.000, 5.000],  loss: 20.017768, mean_q: 214.507687, mean_eps: 0.123310\n",
            " 975254/1000000: episode: 1372, duration: 4.353s, episode steps: 723, steps per second: 166, episode reward: 120.000, mean reward:  0.166 [ 0.000, 30.000], mean action: 2.645 [0.000, 5.000],  loss: 16.236815, mean_q: 194.932353, mean_eps: 0.122597\n",
            " 975933/1000000: episode: 1373, duration: 4.055s, episode steps: 679, steps per second: 167, episode reward: 180.000, mean reward:  0.265 [ 0.000, 30.000], mean action: 2.677 [0.000, 5.000],  loss: 16.231727, mean_q: 206.090647, mean_eps: 0.121966\n",
            " 976531/1000000: episode: 1374, duration: 3.578s, episode steps: 598, steps per second: 167, episode reward: 45.000, mean reward:  0.075 [ 0.000, 10.000], mean action: 3.288 [0.000, 5.000],  loss: 17.963603, mean_q: 209.508675, mean_eps: 0.121392\n",
            " 977384/1000000: episode: 1375, duration: 5.181s, episode steps: 853, steps per second: 165, episode reward: 245.000, mean reward:  0.287 [ 0.000, 30.000], mean action: 3.317 [0.000, 5.000],  loss: 24.036312, mean_q: 231.532952, mean_eps: 0.120739\n",
            " 978060/1000000: episode: 1376, duration: 4.077s, episode steps: 676, steps per second: 166, episode reward: 135.000, mean reward:  0.200 [ 0.000, 30.000], mean action: 3.022 [0.000, 5.000],  loss: 22.089697, mean_q: 206.021927, mean_eps: 0.120051\n",
            " 978598/1000000: episode: 1377, duration: 3.208s, episode steps: 538, steps per second: 168, episode reward: 115.000, mean reward:  0.214 [ 0.000, 20.000], mean action: 2.751 [0.000, 5.000],  loss: 16.897288, mean_q: 200.525827, mean_eps: 0.119504\n",
            " 979547/1000000: episode: 1378, duration: 5.764s, episode steps: 949, steps per second: 165, episode reward: 370.000, mean reward:  0.390 [ 0.000, 200.000], mean action: 2.203 [0.000, 5.000],  loss: 38.022783, mean_q: 224.292401, mean_eps: 0.118835\n",
            " 980195/1000000: episode: 1379, duration: 3.887s, episode steps: 648, steps per second: 167, episode reward: 125.000, mean reward:  0.193 [ 0.000, 25.000], mean action: 2.599 [0.000, 5.000],  loss: 18.226799, mean_q: 216.656203, mean_eps: 0.118117\n",
            " 980848/1000000: episode: 1380, duration: 3.929s, episode steps: 653, steps per second: 166, episode reward: 140.000, mean reward:  0.214 [ 0.000, 25.000], mean action: 2.069 [0.000, 5.000],  loss: 22.368444, mean_q: 222.258147, mean_eps: 0.117531\n",
            " 982516/1000000: episode: 1381, duration: 9.844s, episode steps: 1668, steps per second: 169, episode reward: 745.000, mean reward:  0.447 [ 0.000, 200.000], mean action: 2.819 [0.000, 5.000],  loss: 33.269252, mean_q: 216.471177, mean_eps: 0.116487\n",
            " 983504/1000000: episode: 1382, duration: 5.913s, episode steps: 988, steps per second: 167, episode reward: 290.000, mean reward:  0.294 [ 0.000, 30.000], mean action: 2.494 [0.000, 5.000],  loss: 15.914302, mean_q: 176.117585, mean_eps: 0.115291\n",
            " 984332/1000000: episode: 1383, duration: 4.953s, episode steps: 828, steps per second: 167, episode reward: 105.000, mean reward:  0.127 [ 0.000, 25.000], mean action: 2.385 [0.000, 5.000],  loss: 12.365222, mean_q: 216.127687, mean_eps: 0.114474\n",
            " 984974/1000000: episode: 1384, duration: 3.825s, episode steps: 642, steps per second: 168, episode reward: 105.000, mean reward:  0.164 [ 0.000, 30.000], mean action: 2.298 [0.000, 5.000],  loss: 20.016397, mean_q: 226.858371, mean_eps: 0.113813\n",
            " 985379/1000000: episode: 1385, duration: 2.421s, episode steps: 405, steps per second: 167, episode reward: 20.000, mean reward:  0.049 [ 0.000, 10.000], mean action: 2.373 [0.000, 5.000],  loss: 20.233426, mean_q: 224.620429, mean_eps: 0.113342\n",
            " 986495/1000000: episode: 1386, duration: 6.628s, episode steps: 1116, steps per second: 168, episode reward: 295.000, mean reward:  0.264 [ 0.000, 30.000], mean action: 1.827 [0.000, 5.000],  loss: 24.115024, mean_q: 224.217926, mean_eps: 0.112657\n",
            " 987191/1000000: episode: 1387, duration: 4.149s, episode steps: 696, steps per second: 168, episode reward: 125.000, mean reward:  0.180 [ 0.000, 20.000], mean action: 1.841 [0.000, 5.000],  loss: 12.850084, mean_q: 216.269950, mean_eps: 0.111842\n",
            " 988125/1000000: episode: 1388, duration: 5.634s, episode steps: 934, steps per second: 166, episode reward: 240.000, mean reward:  0.257 [ 0.000, 30.000], mean action: 1.997 [0.000, 5.000],  loss: 22.861491, mean_q: 221.989926, mean_eps: 0.111108\n",
            " 989073/1000000: episode: 1389, duration: 5.678s, episode steps: 948, steps per second: 167, episode reward: 510.000, mean reward:  0.538 [ 0.000, 200.000], mean action: 2.497 [0.000, 5.000],  loss: 24.541761, mean_q: 219.082745, mean_eps: 0.110261\n",
            " 989455/1000000: episode: 1390, duration: 2.312s, episode steps: 382, steps per second: 165, episode reward: 60.000, mean reward:  0.157 [ 0.000, 20.000], mean action: 2.510 [0.000, 5.000],  loss: 30.170705, mean_q: 205.282512, mean_eps: 0.109663\n",
            " 989978/1000000: episode: 1391, duration: 3.170s, episode steps: 523, steps per second: 165, episode reward: 125.000, mean reward:  0.239 [ 0.000, 30.000], mean action: 2.629 [0.000, 5.000],  loss: 37.428570, mean_q: 208.864927, mean_eps: 0.109256\n",
            " 990477/1000000: episode: 1392, duration: 3.018s, episode steps: 499, steps per second: 165, episode reward: 95.000, mean reward:  0.190 [ 0.000, 20.000], mean action: 2.000 [0.000, 5.000],  loss: 38.580101, mean_q: 218.519881, mean_eps: 0.108796\n",
            " 991174/1000000: episode: 1393, duration: 4.190s, episode steps: 697, steps per second: 166, episode reward: 190.000, mean reward:  0.273 [ 0.000, 25.000], mean action: 2.108 [0.000, 5.000],  loss: 24.963218, mean_q: 219.381612, mean_eps: 0.108257\n",
            " 992302/1000000: episode: 1394, duration: 6.704s, episode steps: 1128, steps per second: 168, episode reward: 300.000, mean reward:  0.266 [ 0.000, 30.000], mean action: 2.338 [0.000, 5.000],  loss: 16.037765, mean_q: 221.321216, mean_eps: 0.107436\n",
            " 992945/1000000: episode: 1395, duration: 3.851s, episode steps: 643, steps per second: 167, episode reward: 125.000, mean reward:  0.194 [ 0.000, 30.000], mean action: 2.610 [0.000, 5.000],  loss: 11.645455, mean_q: 201.392389, mean_eps: 0.106639\n",
            " 993678/1000000: episode: 1396, duration: 4.342s, episode steps: 733, steps per second: 169, episode reward: 145.000, mean reward:  0.198 [ 0.000, 25.000], mean action: 2.048 [0.000, 5.000],  loss: 14.104827, mean_q: 210.538108, mean_eps: 0.106020\n",
            " 994314/1000000: episode: 1397, duration: 3.830s, episode steps: 636, steps per second: 166, episode reward: 155.000, mean reward:  0.244 [ 0.000, 30.000], mean action: 1.574 [0.000, 5.000],  loss: 20.779489, mean_q: 224.418049, mean_eps: 0.105404\n",
            " 995593/1000000: episode: 1398, duration: 7.549s, episode steps: 1279, steps per second: 169, episode reward: 370.000, mean reward:  0.289 [ 0.000, 30.000], mean action: 1.978 [0.000, 5.000],  loss: 16.795953, mean_q: 217.452602, mean_eps: 0.104542\n",
            " 996746/1000000: episode: 1399, duration: 6.891s, episode steps: 1153, steps per second: 167, episode reward: 270.000, mean reward:  0.234 [ 0.000, 30.000], mean action: 2.948 [0.000, 5.000],  loss: 17.403896, mean_q: 208.444494, mean_eps: 0.103448\n",
            " 997659/1000000: episode: 1400, duration: 5.568s, episode steps: 913, steps per second: 164, episode reward: 355.000, mean reward:  0.389 [ 0.000, 30.000], mean action: 1.294 [0.000, 5.000],  loss: 29.101534, mean_q: 222.613889, mean_eps: 0.102518\n",
            " 998943/1000000: episode: 1401, duration: 7.640s, episode steps: 1284, steps per second: 168, episode reward: 320.000, mean reward:  0.249 [ 0.000, 30.000], mean action: 2.023 [0.000, 5.000],  loss: 16.330116, mean_q: 198.911380, mean_eps: 0.101530\n",
            " 999759/1000000: episode: 1402, duration: 4.928s, episode steps: 816, steps per second: 166, episode reward: 195.000, mean reward:  0.239 [ 0.000, 30.000], mean action: 1.498 [0.000, 5.000],  loss: 12.693838, mean_q: 180.723767, mean_eps: 0.100585\n",
            "done, took 5979.275 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mPW4pcjAl9lt",
        "outputId": "0bdae8bf-c794-4be5-c5ff-ba48c7808f94"
      },
      "source": [
        "scores = dqn.test(env, nb_episodes=50, visualize=False)\n",
        "print(np.mean(scores.history['episode_reward']))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing for 50 episodes ...\n",
            "Episode 1: reward: 100.000, steps: 360\n",
            "Episode 2: reward: 105.000, steps: 662\n",
            "Episode 3: reward: 435.000, steps: 1721\n",
            "Episode 4: reward: 75.000, steps: 403\n",
            "Episode 5: reward: 210.000, steps: 836\n",
            "Episode 6: reward: 545.000, steps: 1325\n",
            "Episode 7: reward: 270.000, steps: 983\n",
            "Episode 8: reward: 15.000, steps: 387\n",
            "Episode 9: reward: 180.000, steps: 826\n",
            "Episode 10: reward: 110.000, steps: 687\n",
            "Episode 11: reward: 460.000, steps: 961\n",
            "Episode 12: reward: 315.000, steps: 1102\n",
            "Episode 13: reward: 210.000, steps: 853\n",
            "Episode 14: reward: 120.000, steps: 809\n",
            "Episode 15: reward: 105.000, steps: 668\n",
            "Episode 16: reward: 180.000, steps: 858\n",
            "Episode 17: reward: 75.000, steps: 401\n",
            "Episode 18: reward: 285.000, steps: 908\n",
            "Episode 19: reward: 120.000, steps: 733\n",
            "Episode 20: reward: 135.000, steps: 633\n",
            "Episode 21: reward: 155.000, steps: 821\n",
            "Episode 22: reward: 510.000, steps: 1070\n",
            "Episode 23: reward: 215.000, steps: 942\n",
            "Episode 24: reward: 260.000, steps: 995\n",
            "Episode 25: reward: 155.000, steps: 792\n",
            "Episode 26: reward: 105.000, steps: 637\n",
            "Episode 27: reward: 105.000, steps: 648\n",
            "Episode 28: reward: 105.000, steps: 614\n",
            "Episode 29: reward: 300.000, steps: 1091\n",
            "Episode 30: reward: 185.000, steps: 805\n",
            "Episode 31: reward: 75.000, steps: 387\n",
            "Episode 32: reward: 60.000, steps: 385\n",
            "Episode 33: reward: 180.000, steps: 807\n",
            "Episode 34: reward: 105.000, steps: 670\n",
            "Episode 35: reward: 50.000, steps: 399\n",
            "Episode 36: reward: 75.000, steps: 619\n",
            "Episode 37: reward: 210.000, steps: 832\n",
            "Episode 38: reward: 210.000, steps: 846\n",
            "Episode 39: reward: 355.000, steps: 780\n",
            "Episode 40: reward: 105.000, steps: 431\n",
            "Episode 41: reward: 210.000, steps: 816\n",
            "Episode 42: reward: 575.000, steps: 1492\n",
            "Episode 43: reward: 180.000, steps: 880\n",
            "Episode 44: reward: 565.000, steps: 1447\n",
            "Episode 45: reward: 415.000, steps: 951\n",
            "Episode 46: reward: 110.000, steps: 799\n",
            "Episode 47: reward: 260.000, steps: 949\n",
            "Episode 48: reward: 475.000, steps: 1010\n",
            "Episode 49: reward: 210.000, steps: 835\n",
            "Episode 50: reward: 460.000, steps: 1007\n",
            "220.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6biC3fCl9lt"
      },
      "source": [
        "# 4. Reloading Agent from Memory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HcoyLRF4l9lu",
        "outputId": "52a98c13-e8a9-4c9f-9c7f-70007dbf82eb"
      },
      "source": [
        " dqn.save_weights('SavedWeights/10k-Fast/dqn2dense005_weights.h5f')"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[WARNING] SavedWeights/10k-Fast/dqn2dense005_weights.h5f.index already exists - overwrite? [y/n]y\n",
            "[TIP] Next time specify overwrite=True!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-vnDNGAl9lu"
      },
      "source": [
        " #del model, dqn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crGVm6VSl9lu"
      },
      "source": [
        "dqn.load_weights('SavedWeights/10k-Fast/dqn2dense_weights.h5f') #2 dense layers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Y0ysBFgs9hL"
      },
      "source": [
        "dqn.load_weights('SavedWeights/10k-Fast/dqn2_weights.h5f') #default values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_r3WOh49l9lu"
      },
      "source": [
        "dqn.load_weights('SavedWeights/10k-Fast/dqn2dense005_weights.h5f') #with value_test 0.05"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}